{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7LfkL2r2Q1tr"
      },
      "source": [
        "# Getting Started: Exploring Nemo Fundamentals\n",
        "\n",
        "NeMo is a toolkit for creating [Conversational AI](https://developer.nvidia.com/conversational-ai#started) applications.\n",
        "\n",
        "NeMo toolkit makes it possible for researchers to easily compose complex neural network architectures for conversational AI using reusable components - Neural Modules. Neural Modules are conceptual blocks of neural networks that take typed inputs and produce typed outputs. Such modules typically represent data layers, encoders, decoders, language models, loss functions, or methods of combining activations.\n",
        "\n",
        "The toolkit comes with extendable collections of pre-built modules and ready-to-use models for automatic speech recognition (ASR), natural language processing (NLP) and text synthesis (TTS). Built for speed, NeMo can utilize NVIDIA's Tensor Cores and scale out training to multiple GPUs and multiple nodes.\n",
        "\n",
        "For more information, please visit https://docs.nvidia.com/deeplearning/nemo/user-guide/docs/en/main/#"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "zLSy94NEQi-e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4ea4af67-f604-49cf-f9cc-c38094ecfe7a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: wget in /usr/local/lib/python3.11/dist-packages (3.2)\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "libsndfile1 is already the newest version (1.0.31-2ubuntu0.2).\n",
            "ffmpeg is already the newest version (7:4.4.2-0ubuntu0.22.04.1).\n",
            "sox is already the newest version (14.4.2+git20190427-2+deb11u2ubuntu0.22.04.1).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 30 not upgraded.\n",
            "Requirement already satisfied: text-unidecode in /usr/local/lib/python3.11/dist-packages (1.3)\n",
            "\u001b[33mDEPRECATION: git+https://github.com/NVIDIA/NeMo.git@r2.3.0#egg=nemo_toolkit[all] contains an egg fragment with a non-PEP 508 name pip 25.0 will enforce this behaviour change. A possible replacement is to use the req @ url syntax, and remove the egg fragment. Discussion can be found at https://github.com/pypa/pip/issues/11617\u001b[0m\u001b[33m\n",
            "\u001b[0mCollecting nemo_toolkit (from nemo_toolkit[all])\n",
            "  Cloning https://github.com/NVIDIA/NeMo.git (to revision r2.3.0) to /tmp/pip-install-2329h2o0/nemo-toolkit_f590421b5b034229bb5f4ff457b9c5ae\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/NVIDIA/NeMo.git /tmp/pip-install-2329h2o0/nemo-toolkit_f590421b5b034229bb5f4ff457b9c5ae\n",
            "  Running command git checkout -b r2.3.0 --track origin/r2.3.0\n",
            "  Switched to a new branch 'r2.3.0'\n",
            "  Branch 'r2.3.0' set up to track remote branch 'r2.3.0' from 'origin'.\n",
            "  Resolved https://github.com/NVIDIA/NeMo.git to commit 89bb3932a05e92b8903c6074a19ea14c1191726f\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: huggingface_hub>=0.24 in /usr/local/lib/python3.11/dist-packages (from nemo_toolkit->nemo_toolkit[all]) (0.30.1)\n",
            "Requirement already satisfied: numba==0.61.0 in /usr/local/lib/python3.11/dist-packages (from nemo_toolkit->nemo_toolkit[all]) (0.61.0)\n",
            "Requirement already satisfied: numpy<2.0.0,>=1.22 in /usr/local/lib/python3.11/dist-packages (from nemo_toolkit->nemo_toolkit[all]) (1.26.4)\n",
            "Requirement already satisfied: onnx>=1.7.0 in /usr/local/lib/python3.11/dist-packages (from nemo_toolkit->nemo_toolkit[all]) (1.17.0)\n",
            "Requirement already satisfied: protobuf==4.24.4 in /usr/local/lib/python3.11/dist-packages (from nemo_toolkit->nemo_toolkit[all]) (4.24.4)\n",
            "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.11/dist-packages (from nemo_toolkit->nemo_toolkit[all]) (2.8.2)\n",
            "Requirement already satisfied: ruamel.yaml in /usr/local/lib/python3.11/dist-packages (from nemo_toolkit->nemo_toolkit[all]) (0.18.10)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (from nemo_toolkit->nemo_toolkit[all]) (1.6.1)\n",
            "Requirement already satisfied: setuptools>=70.0.0 in /usr/local/lib/python3.11/dist-packages (from nemo_toolkit->nemo_toolkit[all]) (75.2.0)\n",
            "Requirement already satisfied: tensorboard in /usr/local/lib/python3.11/dist-packages (from nemo_toolkit->nemo_toolkit[all]) (2.18.0)\n",
            "Requirement already satisfied: text-unidecode in /usr/local/lib/python3.11/dist-packages (from nemo_toolkit->nemo_toolkit[all]) (1.3)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (from nemo_toolkit->nemo_toolkit[all]) (2.6.0+cu124)\n",
            "Requirement already satisfied: tqdm>=4.41.0 in /usr/local/lib/python3.11/dist-packages (from nemo_toolkit->nemo_toolkit[all]) (4.67.1)\n",
            "Requirement already satisfied: wget in /usr/local/lib/python3.11/dist-packages (from nemo_toolkit->nemo_toolkit[all]) (3.2)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.11/dist-packages (from nemo_toolkit->nemo_toolkit[all]) (1.17.2)\n",
            "Requirement already satisfied: llvmlite<0.45,>=0.44.0dev0 in /usr/local/lib/python3.11/dist-packages (from numba==0.61.0->nemo_toolkit->nemo_toolkit[all]) (0.44.0)\n",
            "Collecting black~=24.3 (from nemo_toolkit->nemo_toolkit[all])\n",
            "  Using cached black-24.10.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.manylinux_2_28_x86_64.whl.metadata (79 kB)\n",
            "Requirement already satisfied: click>=8.1 in /usr/local/lib/python3.11/dist-packages (from nemo_toolkit->nemo_toolkit[all]) (8.1.8)\n",
            "Collecting coverage (from nemo_toolkit->nemo_toolkit[all])\n",
            "  Downloading coverage-7.8.0-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (8.5 kB)\n",
            "Collecting isort<6.0.0,>5.1.0 (from nemo_toolkit->nemo_toolkit[all])\n",
            "  Using cached isort-5.13.2-py3-none-any.whl.metadata (12 kB)\n",
            "Collecting parameterized (from nemo_toolkit->nemo_toolkit[all])\n",
            "  Using cached parameterized-0.9.0-py2.py3-none-any.whl.metadata (18 kB)\n",
            "Requirement already satisfied: pytest in /usr/local/lib/python3.11/dist-packages (from nemo_toolkit->nemo_toolkit[all]) (8.3.5)\n",
            "Collecting pytest-coverage (from nemo_toolkit->nemo_toolkit[all])\n",
            "  Downloading pytest_coverage-0.0-py2.py3-none-any.whl.metadata (380 bytes)\n",
            "Collecting pytest-mock (from nemo_toolkit->nemo_toolkit[all])\n",
            "  Using cached pytest_mock-3.14.0-py3-none-any.whl.metadata (3.8 kB)\n",
            "Collecting pytest-runner (from nemo_toolkit->nemo_toolkit[all])\n",
            "  Using cached pytest_runner-6.0.1-py3-none-any.whl.metadata (7.3 kB)\n",
            "Requirement already satisfied: sphinx in /usr/local/lib/python3.11/dist-packages (from nemo_toolkit->nemo_toolkit[all]) (8.2.3)\n",
            "Collecting sphinxcontrib-bibtex (from nemo_toolkit->nemo_toolkit[all])\n",
            "  Using cached sphinxcontrib_bibtex-2.6.3-py3-none-any.whl.metadata (6.3 kB)\n",
            "Requirement already satisfied: wandb in /usr/local/lib/python3.11/dist-packages (from nemo_toolkit->nemo_toolkit[all]) (0.19.9)\n",
            "Requirement already satisfied: cloudpickle in /usr/local/lib/python3.11/dist-packages (from nemo_toolkit->nemo_toolkit[all]) (3.1.1)\n",
            "Collecting fiddle (from nemo_toolkit->nemo_toolkit[all])\n",
            "  Using cached fiddle-0.3.0-py3-none-any.whl.metadata (2.3 kB)\n",
            "Requirement already satisfied: hydra-core<=1.3.2,>1.3 in /usr/local/lib/python3.11/dist-packages (from nemo_toolkit->nemo_toolkit[all]) (1.3.2)\n",
            "Collecting lightning<=2.4.0,>2.2.1 (from nemo_toolkit->nemo_toolkit[all])\n",
            "  Downloading lightning-2.4.0-py3-none-any.whl.metadata (38 kB)\n",
            "Requirement already satisfied: omegaconf<=2.3 in /usr/local/lib/python3.11/dist-packages (from nemo_toolkit->nemo_toolkit[all]) (2.3.0)\n",
            "Requirement already satisfied: peft in /usr/local/lib/python3.11/dist-packages (from nemo_toolkit->nemo_toolkit[all]) (0.14.0)\n",
            "Collecting torchmetrics>=0.11.0 (from nemo_toolkit->nemo_toolkit[all])\n",
            "  Using cached torchmetrics-1.7.1-py3-none-any.whl.metadata (21 kB)\n",
            "Collecting transformers<=4.48.3,>=4.48.0 (from nemo_toolkit->nemo_toolkit[all])\n",
            "  Downloading transformers-4.48.3-py3-none-any.whl.metadata (44 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.4/44.4 kB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting webdataset>=0.2.86 (from nemo_toolkit->nemo_toolkit[all])\n",
            "  Using cached webdataset-0.2.111-py3-none-any.whl.metadata (15 kB)\n",
            "Collecting bitsandbytes==0.45.3 (from nemo_toolkit->nemo_toolkit[all])\n",
            "  Downloading bitsandbytes-0.45.3-py3-none-manylinux_2_24_x86_64.whl.metadata (5.0 kB)\n",
            "Collecting datasets (from nemo_toolkit->nemo_toolkit[all])\n",
            "  Using cached datasets-3.5.0-py3-none-any.whl.metadata (19 kB)\n",
            "Requirement already satisfied: einops in /usr/local/lib/python3.11/dist-packages (from nemo_toolkit->nemo_toolkit[all]) (0.8.1)\n",
            "Requirement already satisfied: inflect in /usr/local/lib/python3.11/dist-packages (from nemo_toolkit->nemo_toolkit[all]) (7.5.0)\n",
            "Collecting mediapy==1.1.6 (from nemo_toolkit->nemo_toolkit[all])\n",
            "  Downloading mediapy-1.1.6-py3-none-any.whl.metadata (4.8 kB)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from nemo_toolkit->nemo_toolkit[all]) (2.2.2)\n",
            "Collecting sacremoses>=0.0.43 (from nemo_toolkit->nemo_toolkit[all])\n",
            "  Using cached sacremoses-0.1.1-py3-none-any.whl.metadata (8.3 kB)\n",
            "Requirement already satisfied: sentencepiece<1.0.0 in /usr/local/lib/python3.11/dist-packages (from nemo_toolkit->nemo_toolkit[all]) (0.2.0)\n",
            "Collecting braceexpand (from nemo_toolkit->nemo_toolkit[all])\n",
            "  Using cached braceexpand-0.1.7-py2.py3-none-any.whl.metadata (3.0 kB)\n",
            "Requirement already satisfied: editdistance in /usr/local/lib/python3.11/dist-packages (from nemo_toolkit->nemo_toolkit[all]) (0.8.1)\n",
            "Collecting g2p_en (from nemo_toolkit->nemo_toolkit[all])\n",
            "  Using cached g2p_en-2.1.0-py3-none-any.whl.metadata (4.5 kB)\n",
            "Collecting jiwer (from nemo_toolkit->nemo_toolkit[all])\n",
            "  Using cached jiwer-3.1.0-py3-none-any.whl.metadata (2.6 kB)\n",
            "Collecting kaldi-python-io (from nemo_toolkit->nemo_toolkit[all])\n",
            "  Using cached kaldi_python_io-1.2.2-py3-none-any.whl\n",
            "Collecting kaldiio (from nemo_toolkit->nemo_toolkit[all])\n",
            "  Using cached kaldiio-2.18.1-py3-none-any.whl.metadata (13 kB)\n",
            "Collecting lhotse>=1.26.0 (from nemo_toolkit->nemo_toolkit[all])\n",
            "  Using cached lhotse-1.31.0-py3-none-any.whl.metadata (18 kB)\n",
            "Requirement already satisfied: librosa>=0.10.1 in /usr/local/lib/python3.11/dist-packages (from nemo_toolkit->nemo_toolkit[all]) (0.11.0)\n",
            "Collecting marshmallow (from nemo_toolkit->nemo_toolkit[all])\n",
            "  Using cached marshmallow-3.26.1-py3-none-any.whl.metadata (7.3 kB)\n",
            "Collecting optuna (from nemo_toolkit->nemo_toolkit[all])\n",
            "  Downloading optuna-4.2.1-py3-none-any.whl.metadata (17 kB)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from nemo_toolkit->nemo_toolkit[all]) (24.2)\n",
            "Collecting pyannote.core (from nemo_toolkit->nemo_toolkit[all])\n",
            "  Using cached pyannote.core-5.0.0-py3-none-any.whl.metadata (1.4 kB)\n",
            "Collecting pyannote.metrics (from nemo_toolkit->nemo_toolkit[all])\n",
            "  Using cached pyannote.metrics-3.2.1-py3-none-any.whl.metadata (1.3 kB)\n",
            "Collecting pydub (from nemo_toolkit->nemo_toolkit[all])\n",
            "  Using cached pydub-0.25.1-py2.py3-none-any.whl.metadata (1.4 kB)\n",
            "Collecting pyloudnorm (from nemo_toolkit->nemo_toolkit[all])\n",
            "  Using cached pyloudnorm-0.1.1-py3-none-any.whl.metadata (5.6 kB)\n",
            "Collecting resampy (from nemo_toolkit->nemo_toolkit[all])\n",
            "  Using cached resampy-0.4.3-py3-none-any.whl.metadata (3.0 kB)\n",
            "Requirement already satisfied: scipy>=0.14 in /usr/local/lib/python3.11/dist-packages (from nemo_toolkit->nemo_toolkit[all]) (1.14.1)\n",
            "Requirement already satisfied: soundfile in /usr/local/lib/python3.11/dist-packages (from nemo_toolkit->nemo_toolkit[all]) (0.13.1)\n",
            "Collecting texterrors<1.0.0 (from nemo_toolkit->nemo_toolkit[all])\n",
            "  Downloading texterrors-0.5.1.tar.gz (23 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting num2words (from nemo_toolkit->nemo_toolkit[all])\n",
            "  Downloading num2words-0.5.14-py3-none-any.whl.metadata (13 kB)\n",
            "Collecting accelerated-scan (from nemo_toolkit->nemo_toolkit[all])\n",
            "  Using cached accelerated_scan-0.2.0-py3-none-any.whl.metadata (5.3 kB)\n",
            "Collecting boto3 (from nemo_toolkit->nemo_toolkit[all])\n",
            "  Using cached boto3-1.37.29-py3-none-any.whl.metadata (6.7 kB)\n",
            "Collecting faiss-cpu (from nemo_toolkit->nemo_toolkit[all])\n",
            "  Using cached faiss_cpu-1.10.0-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (4.4 kB)\n",
            "Collecting flask_restful (from nemo_toolkit->nemo_toolkit[all])\n",
            "  Using cached Flask_RESTful-0.3.10-py2.py3-none-any.whl.metadata (1.0 kB)\n",
            "Collecting ftfy (from nemo_toolkit->nemo_toolkit[all])\n",
            "  Using cached ftfy-6.3.1-py3-none-any.whl.metadata (7.3 kB)\n",
            "Requirement already satisfied: gdown in /usr/local/lib/python3.11/dist-packages (from nemo_toolkit->nemo_toolkit[all]) (5.2.0)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.11/dist-packages (from nemo_toolkit->nemo_toolkit[all]) (3.13.0)\n",
            "Collecting ijson (from nemo_toolkit->nemo_toolkit[all])\n",
            "  Using cached ijson-3.3.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (21 kB)\n",
            "Requirement already satisfied: jieba in /usr/local/lib/python3.11/dist-packages (from nemo_toolkit->nemo_toolkit[all]) (0.42.1)\n",
            "Collecting markdown2 (from nemo_toolkit->nemo_toolkit[all])\n",
            "  Using cached markdown2-2.5.3-py3-none-any.whl.metadata (2.1 kB)\n",
            "Requirement already satisfied: matplotlib>=3.3.2 in /usr/local/lib/python3.11/dist-packages (from nemo_toolkit->nemo_toolkit[all]) (3.10.0)\n",
            "Collecting megatron_core (from nemo_toolkit->nemo_toolkit[all])\n",
            "  Downloading megatron_core-0.11.0-cp311-cp311-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl.metadata (74 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m74.2/74.2 kB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: nltk>=3.6.5 in /usr/local/lib/python3.11/dist-packages (from nemo_toolkit->nemo_toolkit[all]) (3.9.1)\n",
            "Collecting nvidia-modelopt<=0.27.0,>=0.23.2 (from nvidia-modelopt[torch]<=0.27.0,>=0.23.2; platform_system != \"Darwin\" and extra == \"all\"->nemo_toolkit->nemo_toolkit[all])\n",
            "  Downloading nvidia_modelopt-0.27.0-py3-none-manylinux_2_28_x86_64.whl.metadata (9.7 kB)\n",
            "Collecting nvidia-resiliency-ext (from nemo_toolkit->nemo_toolkit[all])\n",
            "  Downloading nvidia_resiliency_ext-0.3.0-cp311-cp311-manylinux_2_31_x86_64.whl.metadata (3.0 kB)\n",
            "Collecting opencc (from nemo_toolkit->nemo_toolkit[all])\n",
            "  Downloading OpenCC-1.1.9-cp311-cp311-manylinux2014_x86_64.whl.metadata (13 kB)\n",
            "Collecting pangu (from nemo_toolkit->nemo_toolkit[all])\n",
            "  Using cached pangu-4.0.6.1-py3-none-any.whl.metadata (5.3 kB)\n",
            "Requirement already satisfied: prettytable in /usr/local/lib/python3.11/dist-packages (from nemo_toolkit->nemo_toolkit[all]) (3.16.0)\n",
            "Collecting rapidfuzz (from nemo_toolkit->nemo_toolkit[all])\n",
            "  Using cached rapidfuzz-3.13.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
            "Collecting rouge_score (from nemo_toolkit->nemo_toolkit[all])\n",
            "  Using cached rouge_score-0.1.2-py3-none-any.whl\n",
            "Collecting sacrebleu (from nemo_toolkit->nemo_toolkit[all])\n",
            "  Using cached sacrebleu-2.5.1-py3-none-any.whl.metadata (51 kB)\n",
            "Requirement already satisfied: sentence_transformers in /usr/local/lib/python3.11/dist-packages (from nemo_toolkit->nemo_toolkit[all]) (3.4.1)\n",
            "Collecting tensorstore<0.1.72 (from nemo_toolkit->nemo_toolkit[all])\n",
            "  Downloading tensorstore-0.1.71-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (21 kB)\n",
            "Collecting tiktoken==0.7.0 (from nemo_toolkit->nemo_toolkit[all])\n",
            "  Downloading tiktoken-0.7.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.6 kB)\n",
            "Collecting unstructured==0.14.9 (from nemo_toolkit->nemo_toolkit[all])\n",
            "  Downloading unstructured-0.14.9-py3-none-any.whl.metadata (28 kB)\n",
            "Collecting zarr<3.0.0,>=2.18.2 (from nemo_toolkit->nemo_toolkit[all])\n",
            "  Downloading zarr-2.18.5-py3-none-any.whl.metadata (5.8 kB)\n",
            "Collecting attrdict (from nemo_toolkit->nemo_toolkit[all])\n",
            "  Using cached attrdict-2.0.1-py2.py3-none-any.whl.metadata (6.7 kB)\n",
            "Collecting cdifflib==1.2.6 (from nemo_toolkit->nemo_toolkit[all])\n",
            "  Downloading cdifflib-1.2.6.tar.gz (11 kB)\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting janome (from nemo_toolkit->nemo_toolkit[all])\n",
            "  Downloading Janome-0.5.0-py2.py3-none-any.whl.metadata (2.6 kB)\n",
            "Collecting kornia (from nemo_toolkit->nemo_toolkit[all])\n",
            "  Using cached kornia-0.8.0-py2.py3-none-any.whl.metadata (17 kB)\n",
            "Collecting nemo_text_processing (from nemo_toolkit->nemo_toolkit[all])\n",
            "  Using cached nemo_text_processing-1.1.0-py3-none-any.whl.metadata (7.3 kB)\n",
            "Collecting pypinyin (from nemo_toolkit->nemo_toolkit[all])\n",
            "  Using cached pypinyin-0.54.0-py2.py3-none-any.whl.metadata (12 kB)\n",
            "Collecting pypinyin-dict (from nemo_toolkit->nemo_toolkit[all])\n",
            "  Using cached pypinyin_dict-0.9.0-py2.py3-none-any.whl.metadata (3.8 kB)\n",
            "Requirement already satisfied: seaborn in /usr/local/lib/python3.11/dist-packages (from nemo_toolkit->nemo_toolkit[all]) (0.13.2)\n",
            "Collecting progress>=1.5 (from nemo_toolkit->nemo_toolkit[all])\n",
            "  Using cached progress-1.6-py3-none-any.whl\n",
            "Requirement already satisfied: tabulate>=0.8.7 in /usr/local/lib/python3.11/dist-packages (from nemo_toolkit->nemo_toolkit[all]) (0.9.0)\n",
            "Collecting textdistance>=4.1.5 (from nemo_toolkit->nemo_toolkit[all])\n",
            "  Using cached textdistance-4.6.3-py3-none-any.whl.metadata (18 kB)\n",
            "Collecting addict (from nemo_toolkit->nemo_toolkit[all])\n",
            "  Using cached addict-2.4.0-py3-none-any.whl.metadata (1.0 kB)\n",
            "Collecting clip (from nemo_toolkit->nemo_toolkit[all])\n",
            "  Using cached clip-0.2.0-py3-none-any.whl\n",
            "Collecting decord (from nemo_toolkit->nemo_toolkit[all])\n",
            "  Using cached decord-0.6.0-py3-none-manylinux2010_x86_64.whl.metadata (422 bytes)\n",
            "Requirement already satisfied: diffusers>=0.19.3 in /usr/local/lib/python3.11/dist-packages (from nemo_toolkit->nemo_toolkit[all]) (0.32.2)\n",
            "Collecting einops_exts (from nemo_toolkit->nemo_toolkit[all])\n",
            "  Using cached einops_exts-0.0.4-py3-none-any.whl.metadata (621 bytes)\n",
            "Requirement already satisfied: imageio in /usr/local/lib/python3.11/dist-packages (from nemo_toolkit->nemo_toolkit[all]) (2.37.0)\n",
            "Collecting megatron-energon==5.2.0 (from nemo_toolkit->nemo_toolkit[all])\n",
            "  Downloading megatron_energon-5.2.0-py3-none-any.whl.metadata (5.1 kB)\n",
            "Collecting nerfacc>=0.5.3 (from nemo_toolkit->nemo_toolkit[all])\n",
            "  Using cached nerfacc-0.5.3-py3-none-any.whl.metadata (915 bytes)\n",
            "Collecting open_clip_torch==2.24.0 (from nemo_toolkit->nemo_toolkit[all])\n",
            "  Downloading open_clip_torch-2.24.0-py3-none-any.whl.metadata (30 kB)\n",
            "Collecting qwen_vl_utils (from nemo_toolkit->nemo_toolkit[all])\n",
            "  Downloading qwen_vl_utils-0.0.10-py3-none-any.whl.metadata (6.3 kB)\n",
            "Collecting taming-transformers (from nemo_toolkit->nemo_toolkit[all])\n",
            "  Using cached taming_transformers-0.0.1-py3-none-any.whl.metadata (499 bytes)\n",
            "Collecting torchdiffeq (from nemo_toolkit->nemo_toolkit[all])\n",
            "  Using cached torchdiffeq-0.2.5-py3-none-any.whl.metadata (440 bytes)\n",
            "Collecting torchsde (from nemo_toolkit->nemo_toolkit[all])\n",
            "  Using cached torchsde-0.2.6-py3-none-any.whl.metadata (5.3 kB)\n",
            "Collecting trimesh (from nemo_toolkit->nemo_toolkit[all])\n",
            "  Using cached trimesh-4.6.6-py3-none-any.whl.metadata (18 kB)\n",
            "Collecting pesq (from nemo_toolkit->nemo_toolkit[all])\n",
            "  Downloading pesq-0.0.4.tar.gz (38 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting pystoi (from nemo_toolkit->nemo_toolkit[all])\n",
            "  Downloading pystoi-0.4.1-py2.py3-none-any.whl.metadata (4.0 kB)\n",
            "Requirement already satisfied: accelerate in /usr/local/lib/python3.11/dist-packages (from nemo_toolkit->nemo_toolkit[all]) (1.5.2)\n",
            "Collecting fastapi (from nemo_toolkit->nemo_toolkit[all])\n",
            "  Downloading fastapi-0.115.12-py3-none-any.whl.metadata (27 kB)\n",
            "Collecting nvidia-pytriton (from nemo_toolkit->nemo_toolkit[all])\n",
            "  Downloading nvidia_pytriton-0.5.14-py3-none-manylinux_2_35_x86_64.whl.metadata (13 kB)\n",
            "Collecting pydantic-settings (from nemo_toolkit->nemo_toolkit[all])\n",
            "  Downloading pydantic_settings-2.8.1-py3-none-any.whl.metadata (3.5 kB)\n",
            "Collecting uvicorn (from nemo_toolkit->nemo_toolkit[all])\n",
            "  Downloading uvicorn-0.34.0-py3-none-any.whl.metadata (6.5 kB)\n",
            "Collecting nvidia-lm-eval (from nemo_toolkit->nemo_toolkit[all])\n",
            "  Downloading nvidia_lm_eval-25.3.1-py3-none-any.whl.metadata (11 kB)\n",
            "Requirement already satisfied: ipython in /usr/local/lib/python3.11/dist-packages (from mediapy==1.1.6->nemo_toolkit->nemo_toolkit[all]) (7.34.0)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.11/dist-packages (from mediapy==1.1.6->nemo_toolkit->nemo_toolkit[all]) (11.1.0)\n",
            "Collecting multi-storage-client>=0.13.0 (from megatron-energon==5.2.0->nemo_toolkit->nemo_toolkit[all])\n",
            "  Downloading multi_storage_client-0.18.0-py3-none-any.whl.metadata (2.9 kB)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.11/dist-packages (from megatron-energon==5.2.0->nemo_toolkit->nemo_toolkit[all]) (6.0.2)\n",
            "Collecting s3fs (from megatron-energon==5.2.0->nemo_toolkit->nemo_toolkit[all])\n",
            "  Downloading s3fs-2025.3.2-py3-none-any.whl.metadata (1.9 kB)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.11/dist-packages (from open_clip_torch==2.24.0->nemo_toolkit->nemo_toolkit[all]) (0.21.0+cu124)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.11/dist-packages (from open_clip_torch==2.24.0->nemo_toolkit->nemo_toolkit[all]) (2024.11.6)\n",
            "Requirement already satisfied: timm in /usr/local/lib/python3.11/dist-packages (from open_clip_torch==2.24.0->nemo_toolkit->nemo_toolkit[all]) (1.0.15)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.11/dist-packages (from tiktoken==0.7.0->nemo_toolkit->nemo_toolkit[all]) (2.32.3)\n",
            "Requirement already satisfied: chardet in /usr/local/lib/python3.11/dist-packages (from unstructured==0.14.9->nemo_toolkit->nemo_toolkit[all]) (5.2.0)\n",
            "Collecting filetype (from unstructured==0.14.9->nemo_toolkit->nemo_toolkit[all])\n",
            "  Downloading filetype-1.2.0-py2.py3-none-any.whl.metadata (6.5 kB)\n",
            "Collecting python-magic (from unstructured==0.14.9->nemo_toolkit->nemo_toolkit[all])\n",
            "  Downloading python_magic-0.4.27-py2.py3-none-any.whl.metadata (5.8 kB)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.11/dist-packages (from unstructured==0.14.9->nemo_toolkit->nemo_toolkit[all]) (5.3.1)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.11/dist-packages (from unstructured==0.14.9->nemo_toolkit->nemo_toolkit[all]) (4.13.3)\n",
            "Collecting emoji (from unstructured==0.14.9->nemo_toolkit->nemo_toolkit[all])\n",
            "  Downloading emoji-2.14.1-py3-none-any.whl.metadata (5.7 kB)\n",
            "Collecting dataclasses-json (from unstructured==0.14.9->nemo_toolkit->nemo_toolkit[all])\n",
            "  Downloading dataclasses_json-0.6.7-py3-none-any.whl.metadata (25 kB)\n",
            "Collecting python-iso639 (from unstructured==0.14.9->nemo_toolkit->nemo_toolkit[all])\n",
            "  Downloading python_iso639-2025.2.18-py3-none-any.whl.metadata (14 kB)\n",
            "Collecting langdetect (from unstructured==0.14.9->nemo_toolkit->nemo_toolkit[all])\n",
            "  Downloading langdetect-1.0.9.tar.gz (981 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m981.5/981.5 kB\u001b[0m \u001b[31m31.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting backoff (from unstructured==0.14.9->nemo_toolkit->nemo_toolkit[all])\n",
            "  Downloading backoff-2.2.1-py3-none-any.whl.metadata (14 kB)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.11/dist-packages (from unstructured==0.14.9->nemo_toolkit->nemo_toolkit[all]) (4.13.1)\n",
            "Collecting unstructured-client (from unstructured==0.14.9->nemo_toolkit->nemo_toolkit[all])\n",
            "  Downloading unstructured_client-0.32.3-py3-none-any.whl.metadata (23 kB)\n",
            "Collecting mypy-extensions>=0.4.3 (from black~=24.3->nemo_toolkit->nemo_toolkit[all])\n",
            "  Using cached mypy_extensions-1.0.0-py3-none-any.whl.metadata (1.1 kB)\n",
            "Collecting pathspec>=0.9.0 (from black~=24.3->nemo_toolkit->nemo_toolkit[all])\n",
            "  Using cached pathspec-0.12.1-py3-none-any.whl.metadata (21 kB)\n",
            "Requirement already satisfied: platformdirs>=2 in /usr/local/lib/python3.11/dist-packages (from black~=24.3->nemo_toolkit->nemo_toolkit[all]) (4.3.7)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.11/dist-packages (from diffusers>=0.19.3->nemo_toolkit->nemo_toolkit[all]) (8.6.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from diffusers>=0.19.3->nemo_toolkit->nemo_toolkit[all]) (3.18.0)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.11/dist-packages (from diffusers>=0.19.3->nemo_toolkit->nemo_toolkit[all]) (0.5.3)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub>=0.24->nemo_toolkit->nemo_toolkit[all]) (2025.3.2)\n",
            "Requirement already satisfied: antlr4-python3-runtime==4.9.* in /usr/local/lib/python3.11/dist-packages (from hydra-core<=1.3.2,>1.3->nemo_toolkit->nemo_toolkit[all]) (4.9.3)\n",
            "Requirement already satisfied: audioread>=2.1.9 in /usr/local/lib/python3.11/dist-packages (from lhotse>=1.26.0->nemo_toolkit->nemo_toolkit[all]) (3.0.1)\n",
            "Collecting cytoolz>=0.10.1 (from lhotse>=1.26.0->nemo_toolkit->nemo_toolkit[all])\n",
            "  Using cached cytoolz-1.0.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.6 kB)\n",
            "Collecting intervaltree>=3.1.0 (from lhotse>=1.26.0->nemo_toolkit->nemo_toolkit[all])\n",
            "  Using cached intervaltree-3.1.0-py2.py3-none-any.whl\n",
            "Collecting lilcom>=1.1.0 (from lhotse>=1.26.0->nemo_toolkit->nemo_toolkit[all])\n",
            "  Using cached lilcom-1.8.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.2 kB)\n",
            "Requirement already satisfied: joblib>=1.0 in /usr/local/lib/python3.11/dist-packages (from librosa>=0.10.1->nemo_toolkit->nemo_toolkit[all]) (1.4.2)\n",
            "Requirement already satisfied: decorator>=4.3.0 in /usr/local/lib/python3.11/dist-packages (from librosa>=0.10.1->nemo_toolkit->nemo_toolkit[all]) (4.4.2)\n",
            "Requirement already satisfied: pooch>=1.1 in /usr/local/lib/python3.11/dist-packages (from librosa>=0.10.1->nemo_toolkit->nemo_toolkit[all]) (1.8.2)\n",
            "Requirement already satisfied: soxr>=0.3.2 in /usr/local/lib/python3.11/dist-packages (from librosa>=0.10.1->nemo_toolkit->nemo_toolkit[all]) (0.5.0.post1)\n",
            "Requirement already satisfied: lazy_loader>=0.1 in /usr/local/lib/python3.11/dist-packages (from librosa>=0.10.1->nemo_toolkit->nemo_toolkit[all]) (0.4)\n",
            "Requirement already satisfied: msgpack>=1.0 in /usr/local/lib/python3.11/dist-packages (from librosa>=0.10.1->nemo_toolkit->nemo_toolkit[all]) (1.1.0)\n",
            "Collecting lightning-utilities<2.0,>=0.10.0 (from lightning<=2.4.0,>2.2.1->nemo_toolkit->nemo_toolkit[all])\n",
            "  Using cached lightning_utilities-0.14.3-py3-none-any.whl.metadata (5.6 kB)\n",
            "Collecting pytorch-lightning (from lightning<=2.4.0,>2.2.1->nemo_toolkit->nemo_toolkit[all])\n",
            "  Using cached pytorch_lightning-2.5.1-py3-none-any.whl.metadata (20 kB)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.3.2->nemo_toolkit->nemo_toolkit[all]) (1.3.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.3.2->nemo_toolkit->nemo_toolkit[all]) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.3.2->nemo_toolkit->nemo_toolkit[all]) (4.57.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.3.2->nemo_toolkit->nemo_toolkit[all]) (1.4.8)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.3.2->nemo_toolkit->nemo_toolkit[all]) (3.2.3)\n",
            "Requirement already satisfied: rich>=12 in /usr/local/lib/python3.11/dist-packages (from nerfacc>=0.5.3->nemo_toolkit->nemo_toolkit[all]) (13.9.4)\n",
            "Collecting nvidia-modelopt-core==0.27.0 (from nvidia-modelopt<=0.27.0,>=0.23.2->nvidia-modelopt[torch]<=0.27.0,>=0.23.2; platform_system != \"Darwin\" and extra == \"all\"->nemo_toolkit->nemo_toolkit[all])\n",
            "  Downloading nvidia_modelopt_core-0.27.0-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (847 bytes)\n",
            "Collecting ninja (from nvidia-modelopt<=0.27.0,>=0.23.2->nvidia-modelopt[torch]<=0.27.0,>=0.23.2; platform_system != \"Darwin\" and extra == \"all\"->nemo_toolkit->nemo_toolkit[all])\n",
            "  Using cached ninja-1.11.1.4-py3-none-manylinux_2_12_x86_64.manylinux2010_x86_64.whl.metadata (5.0 kB)\n",
            "Requirement already satisfied: pydantic>=2.0 in /usr/local/lib/python3.11/dist-packages (from nvidia-modelopt<=0.27.0,>=0.23.2->nvidia-modelopt[torch]<=0.27.0,>=0.23.2; platform_system != \"Darwin\" and extra == \"all\"->nemo_toolkit->nemo_toolkit[all]) (2.11.2)\n",
            "Collecting pulp (from nvidia-modelopt[torch]<=0.27.0,>=0.23.2; platform_system != \"Darwin\" and extra == \"all\"->nemo_toolkit->nemo_toolkit[all])\n",
            "  Downloading pulp-3.1.1-py3-none-any.whl.metadata (1.3 kB)\n",
            "Requirement already satisfied: pynvml>=11.5.0 in /usr/local/lib/python3.11/dist-packages (from nvidia-modelopt[torch]<=0.27.0,>=0.23.2; platform_system != \"Darwin\" and extra == \"all\"->nemo_toolkit->nemo_toolkit[all]) (12.0.0)\n",
            "Collecting torchprofile>=0.0.4 (from nvidia-modelopt[torch]<=0.27.0,>=0.23.2; platform_system != \"Darwin\" and extra == \"all\"->nemo_toolkit->nemo_toolkit[all])\n",
            "  Downloading torchprofile-0.0.4-py3-none-any.whl.metadata (303 bytes)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil->nemo_toolkit->nemo_toolkit[all]) (1.17.0)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->nemo_toolkit->nemo_toolkit[all]) (3.6.0)\n",
            "Requirement already satisfied: cffi>=1.0 in /usr/local/lib/python3.11/dist-packages (from soundfile->nemo_toolkit->nemo_toolkit[all]) (1.17.1)\n",
            "Requirement already satisfied: ml_dtypes>=0.3.1 in /usr/local/lib/python3.11/dist-packages (from tensorstore<0.1.72->nemo_toolkit->nemo_toolkit[all]) (0.4.1)\n",
            "Collecting pybind11 (from texterrors<1.0.0->nemo_toolkit->nemo_toolkit[all])\n",
            "  Using cached pybind11-2.13.6-py3-none-any.whl.metadata (9.5 kB)\n",
            "Collecting plac (from texterrors<1.0.0->nemo_toolkit->nemo_toolkit[all])\n",
            "  Using cached plac-1.4.5-py2.py3-none-any.whl.metadata (5.9 kB)\n",
            "Collecting loguru (from texterrors<1.0.0->nemo_toolkit->nemo_toolkit[all])\n",
            "  Using cached loguru-0.7.3-py3-none-any.whl.metadata (22 kB)\n",
            "Requirement already satisfied: termcolor in /usr/local/lib/python3.11/dist-packages (from texterrors<1.0.0->nemo_toolkit->nemo_toolkit[all]) (3.0.1)\n",
            "Collecting Levenshtein (from texterrors<1.0.0->nemo_toolkit->nemo_toolkit[all])\n",
            "  Using cached levenshtein-0.27.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.6 kB)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch->nemo_toolkit->nemo_toolkit[all]) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch->nemo_toolkit->nemo_toolkit[all]) (3.1.6)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->nemo_toolkit->nemo_toolkit[all]) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->nemo_toolkit->nemo_toolkit[all]) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->nemo_toolkit->nemo_toolkit[all]) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch->nemo_toolkit->nemo_toolkit[all]) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch->nemo_toolkit->nemo_toolkit[all]) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch->nemo_toolkit->nemo_toolkit[all]) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch->nemo_toolkit->nemo_toolkit[all]) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch->nemo_toolkit->nemo_toolkit[all]) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch->nemo_toolkit->nemo_toolkit[all]) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch->nemo_toolkit->nemo_toolkit[all]) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch->nemo_toolkit->nemo_toolkit[all]) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->nemo_toolkit->nemo_toolkit[all]) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->nemo_toolkit->nemo_toolkit[all]) (12.4.127)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch->nemo_toolkit->nemo_toolkit[all]) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch->nemo_toolkit->nemo_toolkit[all]) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch->nemo_toolkit->nemo_toolkit[all]) (1.3.0)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers<=4.48.3,>=4.48.0->nemo_toolkit->nemo_toolkit[all]) (0.21.1)\n",
            "Collecting asciitree (from zarr<3.0.0,>=2.18.2->nemo_toolkit->nemo_toolkit[all])\n",
            "  Downloading asciitree-0.3.3.tar.gz (4.0 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting fasteners (from zarr<3.0.0,>=2.18.2->nemo_toolkit->nemo_toolkit[all])\n",
            "  Downloading fasteners-0.19-py3-none-any.whl.metadata (4.9 kB)\n",
            "Collecting numcodecs!=0.14.0,!=0.14.1,>=0.10.0 (from zarr<3.0.0,>=2.18.2->nemo_toolkit->nemo_toolkit[all])\n",
            "  Using cached numcodecs-0.16.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.0 kB)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from accelerate->nemo_toolkit->nemo_toolkit[all]) (5.9.5)\n",
            "Collecting botocore<1.38.0,>=1.37.29 (from boto3->nemo_toolkit->nemo_toolkit[all])\n",
            "  Using cached botocore-1.37.29-py3-none-any.whl.metadata (5.7 kB)\n",
            "Collecting jmespath<2.0.0,>=0.7.1 (from boto3->nemo_toolkit->nemo_toolkit[all])\n",
            "  Using cached jmespath-1.0.1-py3-none-any.whl.metadata (7.6 kB)\n",
            "Collecting s3transfer<0.12.0,>=0.11.0 (from boto3->nemo_toolkit->nemo_toolkit[all])\n",
            "  Using cached s3transfer-0.11.4-py3-none-any.whl.metadata (1.7 kB)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets->nemo_toolkit->nemo_toolkit[all]) (18.1.0)\n",
            "Collecting dill<0.3.9,>=0.3.0 (from datasets->nemo_toolkit->nemo_toolkit[all])\n",
            "  Using cached dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n",
            "Collecting xxhash (from datasets->nemo_toolkit->nemo_toolkit[all])\n",
            "  Using cached xxhash-3.5.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
            "Collecting multiprocess<0.70.17 (from datasets->nemo_toolkit->nemo_toolkit[all])\n",
            "  Using cached multiprocess-0.70.16-py311-none-any.whl.metadata (7.2 kB)\n",
            "Collecting fsspec>=2023.5.0 (from huggingface_hub>=0.24->nemo_toolkit->nemo_toolkit[all])\n",
            "  Using cached fsspec-2024.12.0-py3-none-any.whl.metadata (11 kB)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from datasets->nemo_toolkit->nemo_toolkit[all]) (3.11.15)\n",
            "Collecting starlette<0.47.0,>=0.40.0 (from fastapi->nemo_toolkit->nemo_toolkit[all])\n",
            "  Downloading starlette-0.46.1-py3-none-any.whl.metadata (6.2 kB)\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.11/dist-packages (from fiddle->nemo_toolkit->nemo_toolkit[all]) (1.4.0)\n",
            "Requirement already satisfied: graphviz in /usr/local/lib/python3.11/dist-packages (from fiddle->nemo_toolkit->nemo_toolkit[all]) (0.20.3)\n",
            "Collecting libcst (from fiddle->nemo_toolkit->nemo_toolkit[all])\n",
            "  Using cached libcst-1.7.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (17 kB)\n",
            "Collecting aniso8601>=0.82 (from flask_restful->nemo_toolkit->nemo_toolkit[all])\n",
            "  Using cached aniso8601-10.0.0-py2.py3-none-any.whl.metadata (23 kB)\n",
            "Requirement already satisfied: Flask>=0.8 in /usr/local/lib/python3.11/dist-packages (from flask_restful->nemo_toolkit->nemo_toolkit[all]) (3.1.0)\n",
            "Requirement already satisfied: pytz in /usr/local/lib/python3.11/dist-packages (from flask_restful->nemo_toolkit->nemo_toolkit[all]) (2025.2)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.11/dist-packages (from ftfy->nemo_toolkit->nemo_toolkit[all]) (0.2.13)\n",
            "Collecting distance>=0.1.3 (from g2p_en->nemo_toolkit->nemo_toolkit[all])\n",
            "  Using cached Distance-0.1.3-py3-none-any.whl\n",
            "Requirement already satisfied: more_itertools>=8.5.0 in /usr/local/lib/python3.11/dist-packages (from inflect->nemo_toolkit->nemo_toolkit[all]) (10.6.0)\n",
            "Requirement already satisfied: typeguard>=4.0.1 in /usr/local/lib/python3.11/dist-packages (from inflect->nemo_toolkit->nemo_toolkit[all]) (4.4.2)\n",
            "Collecting kornia_rs>=0.1.0 (from kornia->nemo_toolkit->nemo_toolkit[all])\n",
            "  Using cached kornia_rs-0.1.8-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (10 kB)\n",
            "Collecting pytest_asyncio (from megatron_core->nemo_toolkit->nemo_toolkit[all])\n",
            "  Downloading pytest_asyncio-0.26.0-py3-none-any.whl.metadata (4.0 kB)\n",
            "Collecting pytest-cov (from megatron_core->nemo_toolkit->nemo_toolkit[all])\n",
            "  Downloading pytest_cov-6.1.1-py3-none-any.whl.metadata (28 kB)\n",
            "Collecting pytest-random-order (from megatron_core->nemo_toolkit->nemo_toolkit[all])\n",
            "  Downloading pytest_random_order-1.1.1-py3-none-any.whl.metadata (11 kB)\n",
            "Collecting pynini==2.1.6.post1 (from nemo_text_processing->nemo_toolkit->nemo_toolkit[all])\n",
            "  Using cached pynini-2.1.6.post1-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (4.8 kB)\n",
            "Collecting docopt>=0.6.2 (from num2words->nemo_toolkit->nemo_toolkit[all])\n",
            "  Using cached docopt-0.6.2-py2.py3-none-any.whl\n",
            "Collecting evaluate (from nvidia-lm-eval->nemo_toolkit->nemo_toolkit[all])\n",
            "  Downloading evaluate-0.4.3-py3-none-any.whl.metadata (9.2 kB)\n",
            "Collecting jsonlines (from nvidia-lm-eval->nemo_toolkit->nemo_toolkit[all])\n",
            "  Downloading jsonlines-4.0.0-py3-none-any.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: numexpr in /usr/local/lib/python3.11/dist-packages (from nvidia-lm-eval->nemo_toolkit->nemo_toolkit[all]) (2.10.2)\n",
            "Collecting pytablewriter (from nvidia-lm-eval->nemo_toolkit->nemo_toolkit[all])\n",
            "  Downloading pytablewriter-1.2.1-py3-none-any.whl.metadata (38 kB)\n",
            "Collecting tqdm-multiprocess (from nvidia-lm-eval->nemo_toolkit->nemo_toolkit[all])\n",
            "  Downloading tqdm_multiprocess-0.0.11-py3-none-any.whl.metadata (5.7 kB)\n",
            "Requirement already satisfied: zstandard in /usr/local/lib/python3.11/dist-packages (from nvidia-lm-eval->nemo_toolkit->nemo_toolkit[all]) (0.23.0)\n",
            "Collecting word2number (from nvidia-lm-eval->nemo_toolkit->nemo_toolkit[all])\n",
            "  Downloading word2number-1.1.zip (9.7 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting httpx==0.27.0 (from nvidia-lm-eval->nemo_toolkit->nemo_toolkit[all])\n",
            "  Downloading httpx-0.27.0-py3-none-any.whl.metadata (7.2 kB)\n",
            "Collecting immutabledict==4.2.0 (from nvidia-lm-eval->nemo_toolkit->nemo_toolkit[all])\n",
            "  Downloading immutabledict-4.2.0-py3-none-any.whl.metadata (3.4 kB)\n",
            "Requirement already satisfied: tenacity in /usr/local/lib/python3.11/dist-packages (from nvidia-lm-eval->nemo_toolkit->nemo_toolkit[all]) (9.1.2)\n",
            "Collecting beautifulsoup4 (from unstructured==0.14.9->nemo_toolkit->nemo_toolkit[all])\n",
            "  Downloading beautifulsoup4-4.13.1-py3-none-any.whl.metadata (3.8 kB)\n",
            "Collecting openai==1.61.0 (from nvidia-lm-eval->nemo_toolkit->nemo_toolkit[all])\n",
            "  Downloading openai-1.61.0-py3-none-any.whl.metadata (27 kB)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4->unstructured==0.14.9->nemo_toolkit->nemo_toolkit[all]) (2.6)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx==0.27.0->nvidia-lm-eval->nemo_toolkit->nemo_toolkit[all]) (4.9.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx==0.27.0->nvidia-lm-eval->nemo_toolkit->nemo_toolkit[all]) (2025.1.31)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx==0.27.0->nvidia-lm-eval->nemo_toolkit->nemo_toolkit[all]) (1.0.7)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.11/dist-packages (from httpx==0.27.0->nvidia-lm-eval->nemo_toolkit->nemo_toolkit[all]) (3.10)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.11/dist-packages (from httpx==0.27.0->nvidia-lm-eval->nemo_toolkit->nemo_toolkit[all]) (1.3.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.11/dist-packages (from openai==1.61.0->nvidia-lm-eval->nemo_toolkit->nemo_toolkit[all]) (1.9.0)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from openai==1.61.0->nvidia-lm-eval->nemo_toolkit->nemo_toolkit[all]) (0.9.0)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx==0.27.0->nvidia-lm-eval->nemo_toolkit->nemo_toolkit[all]) (0.14.0)\n",
            "Collecting portalocker (from sacrebleu->nemo_toolkit->nemo_toolkit[all])\n",
            "  Using cached portalocker-3.1.1-py3-none-any.whl.metadata (8.6 kB)\n",
            "Collecting colorama (from sacrebleu->nemo_toolkit->nemo_toolkit[all])\n",
            "  Using cached colorama-0.4.6-py2.py3-none-any.whl.metadata (17 kB)\n",
            "Requirement already satisfied: pyzmq>=23.0 in /usr/local/lib/python3.11/dist-packages (from nvidia-pytriton->nemo_toolkit->nemo_toolkit[all]) (24.0.1)\n",
            "Collecting sh>=1.14 (from nvidia-pytriton->nemo_toolkit->nemo_toolkit[all])\n",
            "  Downloading sh-2.2.2-py3-none-any.whl.metadata (3.7 kB)\n",
            "Collecting tritonclient~=2.50 (from tritonclient[grpc,http]~=2.50->nvidia-pytriton->nemo_toolkit->nemo_toolkit[all])\n",
            "  Downloading tritonclient-2.56.0-py3-none-manylinux1_x86_64.whl.metadata (2.8 kB)\n",
            "Requirement already satisfied: grpcio>=1.64.3 in /usr/local/lib/python3.11/dist-packages (from nvidia-pytriton->nemo_toolkit->nemo_toolkit[all]) (1.71.0)\n",
            "Collecting typing_inspect>=0.6.0 (from nvidia-pytriton->nemo_toolkit->nemo_toolkit[all])\n",
            "  Downloading typing_inspect-0.9.0-py3-none-any.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: typer>=0.9.0 in /usr/local/lib/python3.11/dist-packages (from nvidia-pytriton->nemo_toolkit->nemo_toolkit[all]) (0.15.2)\n",
            "Requirement already satisfied: defusedxml in /usr/local/lib/python3.11/dist-packages (from nvidia-resiliency-ext->nemo_toolkit->nemo_toolkit[all]) (0.7.1)\n",
            "Requirement already satisfied: nvidia-ml-py>=12.570.86 in /usr/local/lib/python3.11/dist-packages (from nvidia-resiliency-ext->nemo_toolkit->nemo_toolkit[all]) (12.570.86)\n",
            "Collecting psutil (from accelerate->nemo_toolkit->nemo_toolkit[all])\n",
            "  Downloading psutil-7.0.0-cp36-abi3-manylinux_2_12_x86_64.manylinux2010_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (22 kB)\n",
            "Collecting alembic>=1.5.0 (from optuna->nemo_toolkit->nemo_toolkit[all])\n",
            "  Downloading alembic-1.15.2-py3-none-any.whl.metadata (7.3 kB)\n",
            "Collecting colorlog (from optuna->nemo_toolkit->nemo_toolkit[all])\n",
            "  Downloading colorlog-6.9.0-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: sqlalchemy>=1.4.2 in /usr/local/lib/python3.11/dist-packages (from optuna->nemo_toolkit->nemo_toolkit[all]) (2.0.40)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->nemo_toolkit->nemo_toolkit[all]) (2025.2)\n",
            "Requirement already satisfied: sortedcontainers>=2.0.4 in /usr/local/lib/python3.11/dist-packages (from pyannote.core->nemo_toolkit->nemo_toolkit[all]) (2.4.0)\n",
            "Collecting pyannote.database>=4.0.1 (from pyannote.metrics->nemo_toolkit->nemo_toolkit[all])\n",
            "  Using cached pyannote.database-5.1.3-py3-none-any.whl.metadata (1.1 kB)\n",
            "Collecting python-dotenv>=0.21.0 (from pydantic-settings->nemo_toolkit->nemo_toolkit[all])\n",
            "  Downloading python_dotenv-1.1.0-py3-none-any.whl.metadata (24 kB)\n",
            "Requirement already satisfied: future>=0.16.0 in /usr/local/lib/python3.11/dist-packages (from pyloudnorm->nemo_toolkit->nemo_toolkit[all]) (1.0.0)\n",
            "Requirement already satisfied: iniconfig in /usr/local/lib/python3.11/dist-packages (from pytest->nemo_toolkit->nemo_toolkit[all]) (2.1.0)\n",
            "Requirement already satisfied: pluggy<2,>=1.5 in /usr/local/lib/python3.11/dist-packages (from pytest->nemo_toolkit->nemo_toolkit[all]) (1.5.0)\n",
            "Collecting pytest-cover (from pytest-coverage->nemo_toolkit->nemo_toolkit[all])\n",
            "  Downloading pytest_cover-3.0.0-py2.py3-none-any.whl.metadata (2.7 kB)\n",
            "Collecting av (from qwen_vl_utils->nemo_toolkit->nemo_toolkit[all])\n",
            "  Downloading av-14.3.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.7 kB)\n",
            "Requirement already satisfied: ruamel.yaml.clib>=0.2.7 in /usr/local/lib/python3.11/dist-packages (from ruamel.yaml->nemo_toolkit->nemo_toolkit[all]) (0.2.12)\n",
            "Requirement already satisfied: sphinxcontrib-applehelp>=1.0.7 in /usr/local/lib/python3.11/dist-packages (from sphinx->nemo_toolkit->nemo_toolkit[all]) (2.0.0)\n",
            "Requirement already satisfied: sphinxcontrib-devhelp>=1.0.6 in /usr/local/lib/python3.11/dist-packages (from sphinx->nemo_toolkit->nemo_toolkit[all]) (2.0.0)\n",
            "Requirement already satisfied: sphinxcontrib-htmlhelp>=2.0.6 in /usr/local/lib/python3.11/dist-packages (from sphinx->nemo_toolkit->nemo_toolkit[all]) (2.1.0)\n",
            "Requirement already satisfied: sphinxcontrib-jsmath>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from sphinx->nemo_toolkit->nemo_toolkit[all]) (1.0.1)\n",
            "Requirement already satisfied: sphinxcontrib-qthelp>=1.0.6 in /usr/local/lib/python3.11/dist-packages (from sphinx->nemo_toolkit->nemo_toolkit[all]) (2.0.0)\n",
            "Requirement already satisfied: sphinxcontrib-serializinghtml>=1.1.9 in /usr/local/lib/python3.11/dist-packages (from sphinx->nemo_toolkit->nemo_toolkit[all]) (2.0.0)\n",
            "Requirement already satisfied: Pygments>=2.17 in /usr/local/lib/python3.11/dist-packages (from sphinx->nemo_toolkit->nemo_toolkit[all]) (2.18.0)\n",
            "Requirement already satisfied: docutils<0.22,>=0.20 in /usr/local/lib/python3.11/dist-packages (from sphinx->nemo_toolkit->nemo_toolkit[all]) (0.21.2)\n",
            "Requirement already satisfied: snowballstemmer>=2.2 in /usr/local/lib/python3.11/dist-packages (from sphinx->nemo_toolkit->nemo_toolkit[all]) (2.2.0)\n",
            "Requirement already satisfied: babel>=2.13 in /usr/local/lib/python3.11/dist-packages (from sphinx->nemo_toolkit->nemo_toolkit[all]) (2.17.0)\n",
            "Requirement already satisfied: alabaster>=0.7.14 in /usr/local/lib/python3.11/dist-packages (from sphinx->nemo_toolkit->nemo_toolkit[all]) (1.0.0)\n",
            "Requirement already satisfied: imagesize>=1.3 in /usr/local/lib/python3.11/dist-packages (from sphinx->nemo_toolkit->nemo_toolkit[all]) (1.4.1)\n",
            "Requirement already satisfied: roman-numerals-py>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from sphinx->nemo_toolkit->nemo_toolkit[all]) (3.1.0)\n",
            "Collecting pybtex>=0.24 (from sphinxcontrib-bibtex->nemo_toolkit->nemo_toolkit[all])\n",
            "  Using cached pybtex-0.24.0-py2.py3-none-any.whl.metadata (2.0 kB)\n",
            "Collecting pybtex-docutils>=1.0.0 (from sphinxcontrib-bibtex->nemo_toolkit->nemo_toolkit[all])\n",
            "  Using cached pybtex_docutils-1.0.3-py3-none-any.whl.metadata (4.3 kB)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.11/dist-packages (from tensorboard->nemo_toolkit->nemo_toolkit[all]) (3.7)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from tensorboard->nemo_toolkit->nemo_toolkit[all]) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from tensorboard->nemo_toolkit->nemo_toolkit[all]) (3.1.3)\n",
            "Collecting trampoline>=0.1.2 (from torchsde->nemo_toolkit->nemo_toolkit[all])\n",
            "  Using cached trampoline-0.1.2-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: docker-pycreds>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from wandb->nemo_toolkit->nemo_toolkit[all]) (0.4.0)\n",
            "Requirement already satisfied: gitpython!=3.1.29,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from wandb->nemo_toolkit->nemo_toolkit[all]) (3.1.44)\n",
            "Requirement already satisfied: sentry-sdk>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from wandb->nemo_toolkit->nemo_toolkit[all]) (2.25.1)\n",
            "Requirement already satisfied: setproctitle in /usr/local/lib/python3.11/dist-packages (from wandb->nemo_toolkit->nemo_toolkit[all]) (1.3.5)\n",
            "Requirement already satisfied: Mako in /usr/lib/python3/dist-packages (from alembic>=1.5.0->optuna->nemo_toolkit->nemo_toolkit[all]) (1.1.3)\n",
            "Requirement already satisfied: urllib3!=2.2.0,<3,>=1.25.4 in /usr/local/lib/python3.11/dist-packages (from botocore<1.38.0,>=1.37.29->boto3->nemo_toolkit->nemo_toolkit[all]) (2.3.0)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.11/dist-packages (from cffi>=1.0->soundfile->nemo_toolkit->nemo_toolkit[all]) (2.22)\n",
            "Requirement already satisfied: toolz>=0.8.0 in /usr/local/lib/python3.11/dist-packages (from cytoolz>=0.10.1->lhotse>=1.26.0->nemo_toolkit->nemo_toolkit[all]) (0.12.1)\n",
            "Requirement already satisfied: itsdangerous>=2.2 in /usr/local/lib/python3.11/dist-packages (from Flask>=0.8->flask_restful->nemo_toolkit->nemo_toolkit[all]) (2.2.0)\n",
            "Requirement already satisfied: blinker>=1.9 in /usr/local/lib/python3.11/dist-packages (from Flask>=0.8->flask_restful->nemo_toolkit->nemo_toolkit[all]) (1.9.0)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets->nemo_toolkit->nemo_toolkit[all]) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets->nemo_toolkit->nemo_toolkit[all]) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets->nemo_toolkit->nemo_toolkit[all]) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets->nemo_toolkit->nemo_toolkit[all]) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets->nemo_toolkit->nemo_toolkit[all]) (6.2.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets->nemo_toolkit->nemo_toolkit[all]) (0.3.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets->nemo_toolkit->nemo_toolkit[all]) (1.18.3)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.11/dist-packages (from gitpython!=3.1.29,>=1.0.0->wandb->nemo_toolkit->nemo_toolkit[all]) (4.0.12)\n",
            "Requirement already satisfied: zipp>=3.20 in /usr/local/lib/python3.11/dist-packages (from importlib-metadata->diffusers>=0.19.3->nemo_toolkit->nemo_toolkit[all]) (3.21.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch->nemo_toolkit->nemo_toolkit[all]) (3.0.2)\n",
            "Requirement already satisfied: jsonschema<5.0,>=4.0 in /usr/local/lib/python3.11/dist-packages (from multi-storage-client>=0.13.0->megatron-energon==5.2.0->nemo_toolkit->nemo_toolkit[all]) (4.23.0)\n",
            "Requirement already satisfied: opentelemetry-api<2.0,>=1.24 in /usr/local/lib/python3.11/dist-packages (from multi-storage-client>=0.13.0->megatron-energon==5.2.0->nemo_toolkit->nemo_toolkit[all]) (1.31.1)\n",
            "Collecting latexcodec>=1.0.4 (from pybtex>=0.24->sphinxcontrib-bibtex->nemo_toolkit->nemo_toolkit[all])\n",
            "  Using cached latexcodec-3.0.0-py3-none-any.whl.metadata (4.9 kB)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.0->nvidia-modelopt<=0.27.0,>=0.23.2->nvidia-modelopt[torch]<=0.27.0,>=0.23.2; platform_system != \"Darwin\" and extra == \"all\"->nemo_toolkit->nemo_toolkit[all]) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.1 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.0->nvidia-modelopt<=0.27.0,>=0.23.2->nvidia-modelopt[torch]<=0.27.0,>=0.23.2; platform_system != \"Darwin\" and extra == \"all\"->nemo_toolkit->nemo_toolkit[all]) (2.33.1)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.0->nvidia-modelopt<=0.27.0,>=0.23.2->nvidia-modelopt[torch]<=0.27.0,>=0.23.2; platform_system != \"Darwin\" and extra == \"all\"->nemo_toolkit->nemo_toolkit[all]) (0.4.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken==0.7.0->nemo_toolkit->nemo_toolkit[all]) (3.4.1)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich>=12->nerfacc>=0.5.3->nemo_toolkit->nemo_toolkit[all]) (3.0.0)\n",
            "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.11/dist-packages (from sqlalchemy>=1.4.2->optuna->nemo_toolkit->nemo_toolkit[all]) (3.1.1)\n",
            "Collecting python-rapidjson>=0.9.1 (from tritonclient~=2.50->tritonclient[grpc,http]~=2.50->nvidia-pytriton->nemo_toolkit->nemo_toolkit[all])\n",
            "  Downloading python_rapidjson-1.20-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (22 kB)\n",
            "Collecting grpcio>=1.64.3 (from nvidia-pytriton->nemo_toolkit->nemo_toolkit[all])\n",
            "  Downloading grpcio-1.67.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.9 kB)\n",
            "INFO: pip is looking at multiple versions of tritonclient[grpc,http] to determine which version is compatible with other requirements. This could take a while.\n",
            "Collecting tritonclient[grpc,http]~=2.50 (from nvidia-pytriton->nemo_toolkit->nemo_toolkit[all])\n",
            "  Downloading tritonclient-2.55.0-py3-none-manylinux1_x86_64.whl.metadata (2.8 kB)\n",
            "  Downloading tritonclient-2.54.0-py3-none-manylinux1_x86_64.whl.metadata (2.8 kB)\n",
            "  Downloading tritonclient-2.53.0-py3-none-manylinux1_x86_64.whl.metadata (2.8 kB)\n",
            "  Downloading tritonclient-2.52.0-py3-none-manylinux1_x86_64.whl.metadata (2.8 kB)\n",
            "  Downloading tritonclient-2.51.0-py3-none-manylinux1_x86_64.whl.metadata (2.8 kB)\n",
            "Collecting geventhttpclient<=2.0.2,>=1.4.4 (from tritonclient[grpc,http]~=2.50->nvidia-pytriton->nemo_toolkit->nemo_toolkit[all])\n",
            "  Downloading geventhttpclient-2.0.2-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.3 kB)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from typer>=0.9.0->nvidia-pytriton->nemo_toolkit->nemo_toolkit[all]) (1.5.4)\n",
            "Collecting jedi>=0.16 (from ipython->mediapy==1.1.6->nemo_toolkit->nemo_toolkit[all])\n",
            "  Using cached jedi-0.19.2-py2.py3-none-any.whl.metadata (22 kB)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.11/dist-packages (from ipython->mediapy==1.1.6->nemo_toolkit->nemo_toolkit[all]) (0.7.5)\n",
            "Requirement already satisfied: traitlets>=4.2 in /usr/local/lib/python3.11/dist-packages (from ipython->mediapy==1.1.6->nemo_toolkit->nemo_toolkit[all]) (5.7.1)\n",
            "Requirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from ipython->mediapy==1.1.6->nemo_toolkit->nemo_toolkit[all]) (3.0.50)\n",
            "Requirement already satisfied: backcall in /usr/local/lib/python3.11/dist-packages (from ipython->mediapy==1.1.6->nemo_toolkit->nemo_toolkit[all]) (0.2.0)\n",
            "Requirement already satisfied: matplotlib-inline in /usr/local/lib/python3.11/dist-packages (from ipython->mediapy==1.1.6->nemo_toolkit->nemo_toolkit[all]) (0.1.7)\n",
            "Requirement already satisfied: pexpect>4.3 in /usr/local/lib/python3.11/dist-packages (from ipython->mediapy==1.1.6->nemo_toolkit->nemo_toolkit[all]) (4.9.0)\n",
            "Collecting DataProperty<2,>=1.1.0 (from pytablewriter->nvidia-lm-eval->nemo_toolkit->nemo_toolkit[all])\n",
            "  Downloading DataProperty-1.1.0-py3-none-any.whl.metadata (11 kB)\n",
            "Collecting mbstrdecoder<2,>=1.0.0 (from pytablewriter->nvidia-lm-eval->nemo_toolkit->nemo_toolkit[all])\n",
            "  Downloading mbstrdecoder-1.1.4-py3-none-any.whl.metadata (4.3 kB)\n",
            "Collecting pathvalidate<4,>=2.3.0 (from pytablewriter->nvidia-lm-eval->nemo_toolkit->nemo_toolkit[all])\n",
            "  Downloading pathvalidate-3.2.3-py3-none-any.whl.metadata (12 kB)\n",
            "Collecting tabledata<2,>=1.3.1 (from pytablewriter->nvidia-lm-eval->nemo_toolkit->nemo_toolkit[all])\n",
            "  Downloading tabledata-1.3.4-py3-none-any.whl.metadata (3.7 kB)\n",
            "Collecting tcolorpy<1,>=0.0.5 (from pytablewriter->nvidia-lm-eval->nemo_toolkit->nemo_toolkit[all])\n",
            "  Downloading tcolorpy-0.1.7-py3-none-any.whl.metadata (6.3 kB)\n",
            "Collecting typepy<2,>=1.3.2 (from typepy[datetime]<2,>=1.3.2->pytablewriter->nvidia-lm-eval->nemo_toolkit->nemo_toolkit[all])\n",
            "  Downloading typepy-1.3.4-py3-none-any.whl.metadata (9.2 kB)\n",
            "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /usr/local/lib/python3.11/dist-packages (from requests[socks]->gdown->nemo_toolkit->nemo_toolkit[all]) (1.7.1)\n",
            "Collecting aiobotocore<3.0.0,>=2.5.4 (from s3fs->megatron-energon==5.2.0->nemo_toolkit->nemo_toolkit[all])\n",
            "  Downloading aiobotocore-2.21.1-py3-none-any.whl.metadata (24 kB)\n",
            "INFO: pip is looking at multiple versions of s3fs to determine which version is compatible with other requirements. This could take a while.\n",
            "Collecting s3fs (from megatron-energon==5.2.0->nemo_toolkit->nemo_toolkit[all])\n",
            "  Downloading s3fs-2025.3.1-py3-none-any.whl.metadata (1.9 kB)\n",
            "  Downloading s3fs-2025.3.0-py3-none-any.whl.metadata (1.9 kB)\n",
            "  Downloading s3fs-2025.2.0-py3-none-any.whl.metadata (1.9 kB)\n",
            "  Downloading s3fs-2024.12.0-py3-none-any.whl.metadata (1.6 kB)\n",
            "Collecting aiofiles>=24.1.0 (from unstructured-client->unstructured==0.14.9->nemo_toolkit->nemo_toolkit[all])\n",
            "  Downloading aiofiles-24.1.0-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: cryptography>=3.1 in /usr/local/lib/python3.11/dist-packages (from unstructured-client->unstructured==0.14.9->nemo_toolkit->nemo_toolkit[all]) (43.0.3)\n",
            "Collecting eval-type-backport>=0.2.0 (from unstructured-client->unstructured==0.14.9->nemo_toolkit->nemo_toolkit[all])\n",
            "  Downloading eval_type_backport-0.2.2-py3-none-any.whl.metadata (2.2 kB)\n",
            "Requirement already satisfied: nest-asyncio>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from unstructured-client->unstructured==0.14.9->nemo_toolkit->nemo_toolkit[all]) (1.6.0)\n",
            "Collecting pypdf>=4.0 (from unstructured-client->unstructured==0.14.9->nemo_toolkit->nemo_toolkit[all])\n",
            "  Downloading pypdf-5.4.0-py3-none-any.whl.metadata (7.3 kB)\n",
            "Requirement already satisfied: requests-toolbelt>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from unstructured-client->unstructured==0.14.9->nemo_toolkit->nemo_toolkit[all]) (1.0.0)\n",
            "Collecting aioitertools<1.0.0,>=0.5.1 (from aiobotocore<3.0.0,>=2.5.4->s3fs->megatron-energon==5.2.0->nemo_toolkit->nemo_toolkit[all])\n",
            "  Downloading aioitertools-0.12.0-py3-none-any.whl.metadata (3.8 kB)\n",
            "INFO: pip is looking at multiple versions of aiobotocore to determine which version is compatible with other requirements. This could take a while.\n",
            "Collecting aiobotocore<3.0.0,>=2.5.4 (from s3fs->megatron-energon==5.2.0->nemo_toolkit->nemo_toolkit[all])\n",
            "  Downloading aiobotocore-2.21.0-py3-none-any.whl.metadata (24 kB)\n",
            "  Downloading aiobotocore-2.20.0-py3-none-any.whl.metadata (23 kB)\n",
            "  Downloading aiobotocore-2.19.0-py3-none-any.whl.metadata (23 kB)\n",
            "  Downloading aiobotocore-2.18.0-py3-none-any.whl.metadata (23 kB)\n",
            "  Downloading aiobotocore-2.17.0-py3-none-any.whl.metadata (23 kB)\n",
            "  Downloading aiobotocore-2.16.1-py3-none-any.whl.metadata (23 kB)\n",
            "  Downloading aiobotocore-2.16.0-py3-none-any.whl.metadata (23 kB)\n",
            "INFO: pip is still looking at multiple versions of aiobotocore to determine which version is compatible with other requirements. This could take a while.\n",
            "  Downloading aiobotocore-2.15.2-py3-none-any.whl.metadata (23 kB)\n",
            "  Downloading aiobotocore-2.15.1-py3-none-any.whl.metadata (23 kB)\n",
            "  Downloading aiobotocore-2.15.0-py3-none-any.whl.metadata (23 kB)\n",
            "  Downloading aiobotocore-2.14.0-py3-none-any.whl.metadata (23 kB)\n",
            "  Downloading aiobotocore-2.13.3-py3-none-any.whl.metadata (22 kB)\n",
            "INFO: This is taking longer than usual. You might need to provide the dependency resolver with stricter constraints to reduce runtime. See https://pip.pypa.io/warnings/backtracking for guidance. If you want to abort this run, press Ctrl + C.\n",
            "  Downloading aiobotocore-2.13.2-py3-none-any.whl.metadata (22 kB)\n",
            "  Downloading aiobotocore-2.13.1-py3-none-any.whl.metadata (22 kB)\n",
            "  Downloading aiobotocore-2.13.0-py3-none-any.whl.metadata (21 kB)\n",
            "  Downloading aiobotocore-2.12.4-py3-none-any.whl.metadata (21 kB)\n",
            "  Downloading aiobotocore-2.12.3-py3-none-any.whl.metadata (21 kB)\n",
            "  Downloading aiobotocore-2.12.2-py3-none-any.whl.metadata (21 kB)\n",
            "  Downloading aiobotocore-2.12.1-py3-none-any.whl.metadata (21 kB)\n",
            "  Downloading aiobotocore-2.12.0-py3-none-any.whl.metadata (21 kB)\n",
            "  Downloading aiobotocore-2.11.2-py3-none-any.whl.metadata (21 kB)\n",
            "  Downloading aiobotocore-2.11.1-py3-none-any.whl.metadata (21 kB)\n",
            "  Downloading aiobotocore-2.11.0-py3-none-any.whl.metadata (21 kB)\n",
            "  Downloading aiobotocore-2.10.0-py3-none-any.whl.metadata (20 kB)\n",
            "  Downloading aiobotocore-2.9.1-py3-none-any.whl.metadata (20 kB)\n",
            "  Downloading aiobotocore-2.9.0-py3-none-any.whl.metadata (20 kB)\n",
            "  Downloading aiobotocore-2.8.0-py3-none-any.whl.metadata (20 kB)\n",
            "  Downloading aiobotocore-2.7.0-py3-none-any.whl.metadata (20 kB)\n",
            "  Downloading aiobotocore-2.6.0-py3-none-any.whl.metadata (19 kB)\n",
            "  Downloading aiobotocore-2.5.4-py3-none-any.whl.metadata (19 kB)\n",
            "INFO: pip is still looking at multiple versions of s3fs to determine which version is compatible with other requirements. This could take a while.\n",
            "Collecting s3fs (from megatron-energon==5.2.0->nemo_toolkit->nemo_toolkit[all])\n",
            "  Downloading s3fs-2024.10.0-py3-none-any.whl.metadata (1.7 kB)\n",
            "  Downloading s3fs-2024.9.0-py3-none-any.whl.metadata (1.6 kB)\n",
            "  Downloading s3fs-2024.6.1-py3-none-any.whl.metadata (1.6 kB)\n",
            "  Downloading s3fs-2024.6.0-py3-none-any.whl.metadata (1.6 kB)\n",
            "  Downloading s3fs-2024.5.0-py3-none-any.whl.metadata (1.6 kB)\n",
            "INFO: This is taking longer than usual. You might need to provide the dependency resolver with stricter constraints to reduce runtime. See https://pip.pypa.io/warnings/backtracking for guidance. If you want to abort this run, press Ctrl + C.\n",
            "  Downloading s3fs-2024.3.1-py3-none-any.whl.metadata (1.6 kB)\n",
            "  Downloading s3fs-2024.3.0-py3-none-any.whl.metadata (1.6 kB)\n",
            "  Downloading s3fs-2024.2.0-py3-none-any.whl.metadata (1.6 kB)\n",
            "  Downloading s3fs-2023.12.2-py3-none-any.whl.metadata (1.6 kB)\n",
            "  Downloading s3fs-2023.12.1-py3-none-any.whl.metadata (1.6 kB)\n",
            "  Downloading s3fs-2023.10.0-py3-none-any.whl.metadata (1.6 kB)\n",
            "  Downloading s3fs-2023.9.2-py3-none-any.whl.metadata (1.6 kB)\n",
            "  Downloading s3fs-2023.9.1-py3-none-any.whl.metadata (1.6 kB)\n",
            "  Downloading s3fs-2023.9.0-py3-none-any.whl.metadata (1.6 kB)\n",
            "  Downloading s3fs-2023.6.0-py3-none-any.whl.metadata (1.6 kB)\n",
            "  Downloading s3fs-2023.5.0-py3-none-any.whl.metadata (1.6 kB)\n",
            "  Downloading s3fs-2023.4.0-py3-none-any.whl.metadata (1.6 kB)\n",
            "  Downloading s3fs-2023.3.0-py3-none-any.whl.metadata (1.6 kB)\n",
            "Collecting aiobotocore~=2.4.2 (from s3fs->megatron-energon==5.2.0->nemo_toolkit->nemo_toolkit[all])\n",
            "  Downloading aiobotocore-2.4.2-py3-none-any.whl.metadata (19 kB)\n",
            "Collecting s3fs (from megatron-energon==5.2.0->nemo_toolkit->nemo_toolkit[all])\n",
            "  Downloading s3fs-2023.1.0-py3-none-any.whl.metadata (1.5 kB)\n",
            "  Downloading s3fs-2022.11.0-py3-none-any.whl.metadata (1.5 kB)\n",
            "  Downloading s3fs-2022.10.0-py3-none-any.whl.metadata (1.5 kB)\n",
            "  Downloading s3fs-2022.8.2-py3-none-any.whl.metadata (1.6 kB)\n",
            "  Downloading s3fs-2022.8.1-py3-none-any.whl.metadata (1.6 kB)\n",
            "  Downloading s3fs-2022.8.0-py3-none-any.whl.metadata (1.6 kB)\n",
            "  Downloading s3fs-2022.7.1-py3-none-any.whl.metadata (1.5 kB)\n",
            "Collecting aiobotocore~=2.3.4 (from s3fs->megatron-energon==5.2.0->nemo_toolkit->nemo_toolkit[all])\n",
            "  Downloading aiobotocore-2.3.4-py3-none-any.whl.metadata (19 kB)\n",
            "Collecting s3fs (from megatron-energon==5.2.0->nemo_toolkit->nemo_toolkit[all])\n",
            "  Downloading s3fs-2022.7.0-py3-none-any.whl.metadata (1.5 kB)\n",
            "  Downloading s3fs-2022.5.0-py3-none-any.whl.metadata (1.5 kB)\n",
            "  Downloading s3fs-2022.3.0-py3-none-any.whl.metadata (1.5 kB)\n",
            "Collecting aiobotocore~=2.2.0 (from s3fs->megatron-energon==5.2.0->nemo_toolkit->nemo_toolkit[all])\n",
            "  Downloading aiobotocore-2.2.0.tar.gz (59 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m59.7/59.7 kB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting s3fs (from megatron-energon==5.2.0->nemo_toolkit->nemo_toolkit[all])\n",
            "  Downloading s3fs-2022.2.0-py3-none-any.whl.metadata (1.5 kB)\n",
            "Collecting aiobotocore~=2.1.0 (from s3fs->megatron-energon==5.2.0->nemo_toolkit->nemo_toolkit[all])\n",
            "  Downloading aiobotocore-2.1.2-py3-none-any.whl.metadata (18 kB)\n",
            "Collecting s3fs (from megatron-energon==5.2.0->nemo_toolkit->nemo_toolkit[all])\n",
            "  Downloading s3fs-2022.1.0-py3-none-any.whl.metadata (1.6 kB)\n",
            "  Downloading s3fs-2021.11.1-py3-none-any.whl.metadata (1.6 kB)\n",
            "Collecting aiobotocore~=2.0.1 (from s3fs->megatron-energon==5.2.0->nemo_toolkit->nemo_toolkit[all])\n",
            "  Downloading aiobotocore-2.0.1.tar.gz (54 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.5/54.5 kB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting s3fs (from megatron-energon==5.2.0->nemo_toolkit->nemo_toolkit[all])\n",
            "  Downloading s3fs-2021.11.0-py3-none-any.whl.metadata (1.6 kB)\n",
            "Collecting aiobotocore~=1.4.1 (from s3fs->megatron-energon==5.2.0->nemo_toolkit->nemo_toolkit[all])\n",
            "  Downloading aiobotocore-1.4.2.tar.gz (52 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m52.5/52.5 kB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting s3fs (from megatron-energon==5.2.0->nemo_toolkit->nemo_toolkit[all])\n",
            "  Downloading s3fs-2021.10.1-py3-none-any.whl.metadata (1.5 kB)\n",
            "  Downloading s3fs-2021.10.0-py3-none-any.whl.metadata (1.5 kB)\n",
            "  Downloading s3fs-2021.9.0-py3-none-any.whl.metadata (1.5 kB)\n",
            "  Downloading s3fs-2021.8.1-py3-none-any.whl.metadata (1.5 kB)\n",
            "  Downloading s3fs-2021.8.0-py3-none-any.whl.metadata (1.5 kB)\n",
            "  Downloading s3fs-2021.7.0-py3-none-any.whl.metadata (1.5 kB)\n",
            "  Downloading s3fs-2021.6.1-py3-none-any.whl.metadata (1.5 kB)\n",
            "  Downloading s3fs-2021.6.0-py3-none-any.whl.metadata (1.5 kB)\n",
            "  Downloading s3fs-2021.5.0-py3-none-any.whl.metadata (1.5 kB)\n",
            "  Downloading s3fs-2021.4.0-py3-none-any.whl.metadata (1.5 kB)\n",
            "  Downloading s3fs-0.6.0-py3-none-any.whl.metadata (1.4 kB)\n",
            "Collecting aiobotocore>=1.0.1 (from s3fs->megatron-energon==5.2.0->nemo_toolkit->nemo_toolkit[all])\n",
            "  Downloading aiobotocore-2.5.3-py3-none-any.whl.metadata (19 kB)\n",
            "  Downloading aiobotocore-2.5.2-py3-none-any.whl.metadata (19 kB)\n",
            "  Downloading aiobotocore-2.5.1-py3-none-any.whl.metadata (19 kB)\n",
            "  Downloading aiobotocore-2.5.0-py3-none-any.whl.metadata (19 kB)\n",
            "  Downloading aiobotocore-2.4.1-py3-none-any.whl.metadata (19 kB)\n",
            "  Downloading aiobotocore-2.4.0-py3-none-any.whl.metadata (19 kB)\n",
            "  Downloading aiobotocore-2.3.3.tar.gz (65 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m65.7/65.7 kB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Downloading aiobotocore-2.3.2.tar.gz (104 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m104.8/104.8 kB\u001b[0m \u001b[31m10.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Downloading aiobotocore-2.3.1.tar.gz (65 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m65.3/65.3 kB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Downloading aiobotocore-2.3.0.tar.gz (65 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m65.1/65.1 kB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Downloading aiobotocore-2.1.1.tar.gz (57 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.5/57.5 kB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Downloading aiobotocore-2.1.0.tar.gz (54 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.6/54.6 kB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Downloading aiobotocore-2.0.0.tar.gz (52 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.0/53.0 kB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Downloading aiobotocore-1.4.1.tar.gz (52 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m52.3/52.3 kB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Downloading aiobotocore-1.4.0.tar.gz (51 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m51.6/51.6 kB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Downloading aiobotocore-1.3.3.tar.gz (50 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.6/50.6 kB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Downloading aiobotocore-1.3.2.tar.gz (49 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.1/49.1 kB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Downloading aiobotocore-1.3.1.tar.gz (48 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m48.8/48.8 kB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Downloading aiobotocore-1.3.0.tar.gz (48 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m48.2/48.2 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Downloading aiobotocore-1.2.2.tar.gz (48 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m48.1/48.1 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Downloading aiobotocore-1.2.1.tar.gz (48 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m48.0/48.0 kB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Downloading aiobotocore-1.2.0.tar.gz (47 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m47.3/47.3 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Downloading aiobotocore-1.1.2-py3-none-any.whl.metadata (13 kB)\n",
            "  Downloading aiobotocore-1.1.1-py3-none-any.whl.metadata (13 kB)\n",
            "  Downloading aiobotocore-1.1.0-py3-none-any.whl.metadata (13 kB)\n",
            "  Downloading aiobotocore-1.0.7-py3-none-any.whl.metadata (13 kB)\n",
            "  Downloading aiobotocore-1.0.6-py3-none-any.whl.metadata (13 kB)\n",
            "  Downloading aiobotocore-1.0.5-py3-none-any.whl.metadata (13 kB)\n",
            "  Downloading aiobotocore-1.0.4-py3-none-any.whl.metadata (12 kB)\n",
            "  Downloading aiobotocore-1.0.3-py3-none-any.whl.metadata (12 kB)\n",
            "  Downloading aiobotocore-1.0.2-py3-none-any.whl.metadata (11 kB)\n",
            "  Downloading aiobotocore-1.0.1-py3-none-any.whl.metadata (11 kB)\n",
            "Collecting s3fs (from megatron-energon==5.2.0->nemo_toolkit->nemo_toolkit[all])\n",
            "  Downloading s3fs-0.5.2-py3-none-any.whl.metadata (1.2 kB)\n",
            "  Downloading s3fs-0.5.1-py3-none-any.whl.metadata (1.3 kB)\n",
            "  Downloading s3fs-0.5.0-py3-none-any.whl.metadata (1.3 kB)\n",
            "  Downloading s3fs-0.4.2-py3-none-any.whl.metadata (1.3 kB)\n",
            "Collecting gevent>=0.13 (from geventhttpclient<=2.0.2,>=1.4.4->tritonclient[grpc,http]~=2.50->nvidia-pytriton->nemo_toolkit->nemo_toolkit[all])\n",
            "  Downloading gevent-24.11.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (13 kB)\n",
            "Collecting brotli (from geventhttpclient<=2.0.2,>=1.4.4->tritonclient[grpc,http]~=2.50->nvidia-pytriton->nemo_toolkit->nemo_toolkit[all])\n",
            "  Downloading Brotli-1.1.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.5 kB)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.11/dist-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb->nemo_toolkit->nemo_toolkit[all]) (5.0.2)\n",
            "Requirement already satisfied: parso<0.9.0,>=0.8.4 in /usr/local/lib/python3.11/dist-packages (from jedi>=0.16->ipython->mediapy==1.1.6->nemo_toolkit->nemo_toolkit[all]) (0.8.4)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.11/dist-packages (from jsonschema<5.0,>=4.0->multi-storage-client>=0.13.0->megatron-energon==5.2.0->nemo_toolkit->nemo_toolkit[all]) (2024.10.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.11/dist-packages (from jsonschema<5.0,>=4.0->multi-storage-client>=0.13.0->megatron-energon==5.2.0->nemo_toolkit->nemo_toolkit[all]) (0.36.2)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.11/dist-packages (from jsonschema<5.0,>=4.0->multi-storage-client>=0.13.0->megatron-energon==5.2.0->nemo_toolkit->nemo_toolkit[all]) (0.24.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich>=12->nerfacc>=0.5.3->nemo_toolkit->nemo_toolkit[all]) (0.1.2)\n",
            "Requirement already satisfied: deprecated>=1.2.6 in /usr/local/lib/python3.11/dist-packages (from opentelemetry-api<2.0,>=1.24->multi-storage-client>=0.13.0->megatron-energon==5.2.0->nemo_toolkit->nemo_toolkit[all]) (1.2.18)\n",
            "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.11/dist-packages (from pexpect>4.3->ipython->mediapy==1.1.6->nemo_toolkit->nemo_toolkit[all]) (0.7.0)\n",
            "Collecting zope.event (from gevent>=0.13->geventhttpclient<=2.0.2,>=1.4.4->tritonclient[grpc,http]~=2.50->nvidia-pytriton->nemo_toolkit->nemo_toolkit[all])\n",
            "  Downloading zope.event-5.0-py3-none-any.whl.metadata (4.4 kB)\n",
            "Collecting zope.interface (from gevent>=0.13->geventhttpclient<=2.0.2,>=1.4.4->tritonclient[grpc,http]~=2.50->nvidia-pytriton->nemo_toolkit->nemo_toolkit[all])\n",
            "  Downloading zope.interface-7.2-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (44 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.4/44.4 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading bitsandbytes-0.45.3-py3-none-manylinux_2_24_x86_64.whl (76.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.1/76.1 MB\u001b[0m \u001b[31m9.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading mediapy-1.1.6-py3-none-any.whl (24 kB)\n",
            "Downloading megatron_energon-5.2.0-py3-none-any.whl (175 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m175.9/175.9 kB\u001b[0m \u001b[31m18.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading open_clip_torch-2.24.0-py3-none-any.whl (1.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m68.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tiktoken-0.7.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m63.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading unstructured-0.14.9-py3-none-any.whl (2.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m20.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hUsing cached black-24.10.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.manylinux_2_28_x86_64.whl (1.7 MB)\n",
            "Using cached isort-5.13.2-py3-none-any.whl (92 kB)\n",
            "Downloading jiwer-3.1.0-py3-none-any.whl (22 kB)\n",
            "Using cached lhotse-1.31.0-py3-none-any.whl (845 kB)\n",
            "Downloading lightning-2.4.0-py3-none-any.whl (810 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m811.0/811.0 kB\u001b[0m \u001b[31m50.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hUsing cached nerfacc-0.5.3-py3-none-any.whl (54 kB)\n",
            "Downloading nvidia_modelopt-0.27.0-py3-none-manylinux_2_28_x86_64.whl (659 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m659.5/659.5 kB\u001b[0m \u001b[31m46.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_modelopt_core-0.27.0-cp311-cp311-manylinux_2_28_x86_64.whl (1.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.4/1.4 MB\u001b[0m \u001b[31m73.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading rapidfuzz-3.13.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m92.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hUsing cached sacremoses-0.1.1-py3-none-any.whl (897 kB)\n",
            "Downloading tensorstore-0.1.71-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (17.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17.8/17.8 MB\u001b[0m \u001b[31m59.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hUsing cached textdistance-4.6.3-py3-none-any.whl (31 kB)\n",
            "Using cached torchmetrics-1.7.1-py3-none-any.whl (961 kB)\n",
            "Downloading transformers-4.48.3-py3-none-any.whl (9.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.7/9.7 MB\u001b[0m \u001b[31m109.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hUsing cached webdataset-0.2.111-py3-none-any.whl (85 kB)\n",
            "Downloading zarr-2.18.5-py3-none-any.whl (211 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.3/211.3 kB\u001b[0m \u001b[31m21.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hUsing cached accelerated_scan-0.2.0-py3-none-any.whl (11 kB)\n",
            "Using cached addict-2.4.0-py3-none-any.whl (3.8 kB)\n",
            "Using cached attrdict-2.0.1-py2.py3-none-any.whl (9.9 kB)\n",
            "Using cached boto3-1.37.29-py3-none-any.whl (139 kB)\n",
            "Using cached braceexpand-0.1.7-py2.py3-none-any.whl (5.9 kB)\n",
            "Downloading coverage-7.8.0-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (244 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m244.0/244.0 kB\u001b[0m \u001b[31m23.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hUsing cached datasets-3.5.0-py3-none-any.whl (491 kB)\n",
            "Using cached decord-0.6.0-py3-none-manylinux2010_x86_64.whl (13.6 MB)\n",
            "Using cached einops_exts-0.0.4-py3-none-any.whl (3.9 kB)\n",
            "Using cached faiss_cpu-1.10.0-cp311-cp311-manylinux_2_28_x86_64.whl (30.7 MB)\n",
            "Downloading fastapi-0.115.12-py3-none-any.whl (95 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m95.2/95.2 kB\u001b[0m \u001b[31m8.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hUsing cached fiddle-0.3.0-py3-none-any.whl (419 kB)\n",
            "Using cached Flask_RESTful-0.3.10-py2.py3-none-any.whl (26 kB)\n",
            "Using cached ftfy-6.3.1-py3-none-any.whl (44 kB)\n",
            "Using cached g2p_en-2.1.0-py3-none-any.whl (3.1 MB)\n",
            "Using cached ijson-3.3.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (119 kB)\n",
            "Downloading Janome-0.5.0-py2.py3-none-any.whl (19.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m19.7/19.7 MB\u001b[0m \u001b[31m15.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hUsing cached kaldiio-2.18.1-py3-none-any.whl (29 kB)\n",
            "Using cached kornia-0.8.0-py2.py3-none-any.whl (1.1 MB)\n",
            "Using cached markdown2-2.5.3-py3-none-any.whl (48 kB)\n",
            "Using cached marshmallow-3.26.1-py3-none-any.whl (50 kB)\n",
            "Downloading megatron_core-0.11.0-cp311-cp311-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl (1.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m72.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hUsing cached nemo_text_processing-1.1.0-py3-none-any.whl (2.7 MB)\n",
            "Using cached pynini-2.1.6.post1-cp311-cp311-manylinux_2_28_x86_64.whl (154.8 MB)\n",
            "Downloading num2words-0.5.14-py3-none-any.whl (163 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m163.5/163.5 kB\u001b[0m \u001b[31m260.3 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_lm_eval-25.3.1-py3-none-any.whl (4.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.2/4.2 MB\u001b[0m \u001b[31m20.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading beautifulsoup4-4.13.1-py3-none-any.whl (185 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m185.1/185.1 kB\u001b[0m \u001b[31m18.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading httpx-0.27.0-py3-none-any.whl (75 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.6/75.6 kB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading immutabledict-4.2.0-py3-none-any.whl (4.7 kB)\n",
            "Downloading openai-1.61.0-py3-none-any.whl (460 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m460.6/460.6 kB\u001b[0m \u001b[31m37.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hUsing cached sacrebleu-2.5.1-py3-none-any.whl (104 kB)\n",
            "Downloading nvidia_pytriton-0.5.14-py3-none-manylinux_2_35_x86_64.whl (55.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m55.4/55.4 MB\u001b[0m \u001b[31m12.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_resiliency_ext-0.3.0-cp311-cp311-manylinux_2_31_x86_64.whl (430 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m430.6/430.6 kB\u001b[0m \u001b[31m36.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading OpenCC-1.1.9-cp311-cp311-manylinux2014_x86_64.whl (1.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m61.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading optuna-4.2.1-py3-none-any.whl (383 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m383.6/383.6 kB\u001b[0m \u001b[31m34.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hUsing cached pangu-4.0.6.1-py3-none-any.whl (6.4 kB)\n",
            "Using cached parameterized-0.9.0-py2.py3-none-any.whl (20 kB)\n",
            "Using cached pyannote.core-5.0.0-py3-none-any.whl (58 kB)\n",
            "Using cached pyannote.metrics-3.2.1-py3-none-any.whl (51 kB)\n",
            "Downloading pydantic_settings-2.8.1-py3-none-any.whl (30 kB)\n",
            "Using cached pydub-0.25.1-py2.py3-none-any.whl (32 kB)\n",
            "Using cached pyloudnorm-0.1.1-py3-none-any.whl (9.6 kB)\n",
            "Using cached pypinyin-0.54.0-py2.py3-none-any.whl (837 kB)\n",
            "Using cached pypinyin_dict-0.9.0-py2.py3-none-any.whl (9.5 MB)\n",
            "Downloading pystoi-0.4.1-py2.py3-none-any.whl (8.2 kB)\n",
            "Downloading pytest_coverage-0.0-py2.py3-none-any.whl (2.0 kB)\n",
            "Using cached pytest_mock-3.14.0-py3-none-any.whl (9.9 kB)\n",
            "Using cached pytest_runner-6.0.1-py3-none-any.whl (7.2 kB)\n",
            "Downloading qwen_vl_utils-0.0.10-py3-none-any.whl (6.7 kB)\n",
            "Using cached resampy-0.4.3-py3-none-any.whl (3.1 MB)\n",
            "Using cached sphinxcontrib_bibtex-2.6.3-py3-none-any.whl (40 kB)\n",
            "Using cached taming_transformers-0.0.1-py3-none-any.whl (45 kB)\n",
            "Using cached torchdiffeq-0.2.5-py3-none-any.whl (32 kB)\n",
            "Using cached torchsde-0.2.6-py3-none-any.whl (61 kB)\n",
            "Using cached trimesh-4.6.6-py3-none-any.whl (709 kB)\n",
            "Downloading uvicorn-0.34.0-py3-none-any.whl (62 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.3/62.3 kB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading alembic-1.15.2-py3-none-any.whl (231 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m231.9/231.9 kB\u001b[0m \u001b[31m21.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hUsing cached aniso8601-10.0.0-py2.py3-none-any.whl (52 kB)\n",
            "Using cached botocore-1.37.29-py3-none-any.whl (13.5 MB)\n",
            "Using cached cytoolz-1.0.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.1 MB)\n",
            "Using cached dill-0.3.8-py3-none-any.whl (116 kB)\n",
            "Downloading evaluate-0.4.3-py3-none-any.whl (84 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.0/84.0 kB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hUsing cached fsspec-2024.12.0-py3-none-any.whl (183 kB)\n",
            "Using cached jmespath-1.0.1-py3-none-any.whl (20 kB)\n",
            "Using cached kornia_rs-0.1.8-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.1 MB)\n",
            "Using cached lightning_utilities-0.14.3-py3-none-any.whl (28 kB)\n",
            "Using cached lilcom-1.8.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (94 kB)\n",
            "Downloading multi_storage_client-0.18.0-py3-none-any.whl (112 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m112.8/112.8 kB\u001b[0m \u001b[31m11.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hUsing cached multiprocess-0.70.16-py311-none-any.whl (143 kB)\n",
            "Using cached mypy_extensions-1.0.0-py3-none-any.whl (4.7 kB)\n",
            "Using cached numcodecs-0.16.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (8.8 MB)\n",
            "Using cached pathspec-0.12.1-py3-none-any.whl (31 kB)\n",
            "Downloading psutil-7.0.0-cp36-abi3-manylinux_2_12_x86_64.manylinux2010_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (277 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m278.0/278.0 kB\u001b[0m \u001b[31m24.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hUsing cached pyannote.database-5.1.3-py3-none-any.whl (48 kB)\n",
            "Using cached pybind11-2.13.6-py3-none-any.whl (243 kB)\n",
            "Using cached pybtex-0.24.0-py2.py3-none-any.whl (561 kB)\n",
            "Using cached pybtex_docutils-1.0.3-py3-none-any.whl (6.4 kB)\n",
            "Downloading python_dotenv-1.1.0-py3-none-any.whl (20 kB)\n",
            "Using cached pytorch_lightning-2.5.1-py3-none-any.whl (822 kB)\n",
            "Using cached s3transfer-0.11.4-py3-none-any.whl (84 kB)\n",
            "Downloading sh-2.2.2-py3-none-any.whl (38 kB)\n",
            "Downloading starlette-0.46.1-py3-none-any.whl (71 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m72.0/72.0 kB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading torchprofile-0.0.4-py3-none-any.whl (7.7 kB)\n",
            "Using cached trampoline-0.1.2-py3-none-any.whl (5.2 kB)\n",
            "Downloading tritonclient-2.51.0-py3-none-manylinux1_x86_64.whl (14.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.0/14.0 MB\u001b[0m \u001b[31m66.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
            "Downloading av-14.3.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (35.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m35.2/35.2 MB\u001b[0m \u001b[31m18.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading backoff-2.2.1-py3-none-any.whl (15 kB)\n",
            "Using cached colorama-0.4.6-py2.py3-none-any.whl (25 kB)\n",
            "Downloading colorlog-6.9.0-py3-none-any.whl (11 kB)\n",
            "Downloading dataclasses_json-0.6.7-py3-none-any.whl (28 kB)\n",
            "Downloading emoji-2.14.1-py3-none-any.whl (590 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m590.6/590.6 kB\u001b[0m \u001b[31m38.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading fasteners-0.19-py3-none-any.whl (18 kB)\n",
            "Downloading filetype-1.2.0-py2.py3-none-any.whl (19 kB)\n",
            "Downloading jsonlines-4.0.0-py3-none-any.whl (8.7 kB)\n",
            "Downloading levenshtein-0.27.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (161 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m161.7/161.7 kB\u001b[0m \u001b[31m16.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hUsing cached libcst-1.7.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.3 MB)\n",
            "Using cached loguru-0.7.3-py3-none-any.whl (61 kB)\n",
            "Using cached ninja-1.11.1.4-py3-none-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (422 kB)\n",
            "Using cached plac-1.4.5-py2.py3-none-any.whl (22 kB)\n",
            "Using cached portalocker-3.1.1-py3-none-any.whl (19 kB)\n",
            "Downloading pulp-3.1.1-py3-none-any.whl (16.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.4/16.4 MB\u001b[0m \u001b[31m69.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pytablewriter-1.2.1-py3-none-any.whl (91 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m91.1/91.1 kB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pytest_asyncio-0.26.0-py3-none-any.whl (19 kB)\n",
            "Downloading pytest_cov-6.1.1-py3-none-any.whl (23 kB)\n",
            "Downloading pytest_cover-3.0.0-py2.py3-none-any.whl (3.8 kB)\n",
            "Downloading pytest_random_order-1.1.1-py3-none-any.whl (11 kB)\n",
            "Downloading python_iso639-2025.2.18-py3-none-any.whl (167 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m167.6/167.6 kB\u001b[0m \u001b[31m14.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading python_magic-0.4.27-py2.py3-none-any.whl (13 kB)\n",
            "Downloading s3fs-0.4.2-py3-none-any.whl (19 kB)\n",
            "Downloading tqdm_multiprocess-0.0.11-py3-none-any.whl (9.8 kB)\n",
            "Downloading unstructured_client-0.32.3-py3-none-any.whl (180 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m180.6/180.6 kB\u001b[0m \u001b[31m17.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hUsing cached xxhash-3.5.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
            "Downloading aiofiles-24.1.0-py3-none-any.whl (15 kB)\n",
            "Downloading DataProperty-1.1.0-py3-none-any.whl (27 kB)\n",
            "Downloading eval_type_backport-0.2.2-py3-none-any.whl (5.8 kB)\n",
            "Downloading geventhttpclient-2.0.2-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (100 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m100.4/100.4 kB\u001b[0m \u001b[31m10.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hUsing cached jedi-0.19.2-py2.py3-none-any.whl (1.6 MB)\n",
            "Using cached latexcodec-3.0.0-py3-none-any.whl (18 kB)\n",
            "Downloading mbstrdecoder-1.1.4-py3-none-any.whl (7.9 kB)\n",
            "Downloading pathvalidate-3.2.3-py3-none-any.whl (24 kB)\n",
            "Downloading pypdf-5.4.0-py3-none-any.whl (302 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m302.3/302.3 kB\u001b[0m \u001b[31m29.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading python_rapidjson-1.20-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m76.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tabledata-1.3.4-py3-none-any.whl (11 kB)\n",
            "Downloading tcolorpy-0.1.7-py3-none-any.whl (8.1 kB)\n",
            "Downloading typepy-1.3.4-py3-none-any.whl (31 kB)\n",
            "Downloading gevent-24.11.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (6.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.8/6.8 MB\u001b[0m \u001b[31m89.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading Brotli-1.1.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.9/2.9 MB\u001b[0m \u001b[31m100.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading zope.event-5.0-py3-none-any.whl (6.8 kB)\n",
            "Downloading zope.interface-7.2-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (259 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m259.8/259.8 kB\u001b[0m \u001b[31m24.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: nemo_toolkit, cdifflib, texterrors, langdetect, pesq, asciitree, word2number\n",
            "  Building wheel for nemo_toolkit (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for nemo_toolkit: filename=nemo_toolkit-2.3.0rc3-py3-none-any.whl size=5847011 sha256=cdc256aa0d0a43bde38fd05ca4a6ece757f15097546faeae346f90bd17886907\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-_7d4h7go/wheels/b1/67/04/77a9caaadbd1c67aae74d7fe694957b44422c23a0050c6968a\n",
            "  Building wheel for cdifflib (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for cdifflib: filename=cdifflib-1.2.6-cp311-cp311-linux_x86_64.whl size=28737 sha256=e8398384a18785aea4392051cb6f3acca353843636d991be054df0928dfdc3f8\n",
            "  Stored in directory: /root/.cache/pip/wheels/61/25/f2/4ee06fe9d0bcb43991be6302633001497862ff216060e9361f\n",
            "  Building wheel for texterrors (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for texterrors: filename=texterrors-0.5.1-cp311-cp311-linux_x86_64.whl size=1077993 sha256=98851818363a93379261921de43157c4d807b9bdd05a50123569a1f26c327302\n",
            "  Stored in directory: /root/.cache/pip/wheels/6f/94/c8/7edaa578fc800d26e3fda18fba557a4218ab553d078ee51b46\n",
            "  Building wheel for langdetect (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for langdetect: filename=langdetect-1.0.9-py3-none-any.whl size=993223 sha256=a43937490444b3c0580f504ecd034b6bd87b0df9647370e6cdb97830426f025a\n",
            "  Stored in directory: /root/.cache/pip/wheels/0a/f2/b2/e5ca405801e05eb7c8ed5b3b4bcf1fcabcd6272c167640072e\n",
            "  Building wheel for pesq (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pesq: filename=pesq-0.0.4-cp311-cp311-linux_x86_64.whl size=275947 sha256=efcba495ba662750b460c2cdcd0b0ad80861dabff8bdf6d42b67075411df89d0\n",
            "  Stored in directory: /root/.cache/pip/wheels/ae/f1/23/2698d0bf31eec2b2aa50623b5d93b6206c49c7155d0e31345d\n",
            "  Building wheel for asciitree (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for asciitree: filename=asciitree-0.3.3-py3-none-any.whl size=5031 sha256=befb2ff7b5a10ab4e5dc636623247e3531679dfd144369ed1ca65a087ca6c669\n",
            "  Stored in directory: /root/.cache/pip/wheels/71/c1/da/23077eb3b87d24d6f3852ed1ed1a1ac2d3c885ad6ebd2b4a07\n",
            "  Building wheel for word2number (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for word2number: filename=word2number-1.1-py3-none-any.whl size=5568 sha256=34119b0bf7a2fc009085eba07a3d72849ceaab3ef0f2c71db365f89db716a909\n",
            "  Stored in directory: /root/.cache/pip/wheels/cd/ef/ae/073b491b14d25e2efafcffca9e16b2ee6d114ec5c643ba4f06\n",
            "Successfully built nemo_toolkit cdifflib texterrors langdetect pesq asciitree word2number\n",
            "Installing collected packages: word2number, trampoline, pydub, progress, plac, pesq, pangu, opencc, janome, ijson, filetype, docopt, distance, clip, brotli, braceexpand, asciitree, aniso8601, addict, zope.interface, zope.event, xxhash, webdataset, uvicorn, trimesh, textdistance, tcolorpy, sh, sacremoses, rapidfuzz, python-rapidjson, python-magic, python-iso639, python-dotenv, pytest-runner, pypinyin, pypdf, pynini, pybind11, pulp, psutil, portalocker, pathvalidate, pathspec, parameterized, nvidia-modelopt-core, numcodecs, num2words, ninja, mypy-extensions, mbstrdecoder, marshmallow, markdown2, loguru, lilcom, lightning-utilities, libcst, latexcodec, langdetect, kornia_rs, kaldiio, kaldi-python-io, jsonlines, jmespath, jedi, isort, intervaltree, immutabledict, ftfy, fsspec, fasteners, faiss-cpu, eval-type-backport, emoji, einops_exts, dill, decord, cytoolz, coverage, colorlog, colorama, cdifflib, beautifulsoup4, backoff, av, attrdict, aiofiles, zarr, typing_inspect, typepy, tritonclient, tqdm-multiprocess, tiktoken, tensorstore, starlette, sacrebleu, rouge_score, resampy, qwen_vl_utils, pytest-random-order, pytest-mock, pytest_asyncio, pystoi, pypinyin-dict, pyloudnorm, pybtex, pyannote.core, multiprocess, Levenshtein, jiwer, httpx, gevent, fiddle, botocore, black, alembic, unstructured-client, texterrors, s3transfer, s3fs, pytest-cov, pydantic-settings, pybtex-docutils, optuna, openai, nvidia-modelopt, mediapy, geventhttpclient, g2p_en, flask_restful, fastapi, dataclasses-json, unstructured, transformers, torchsde, torchmetrics, torchdiffeq, sphinxcontrib-bibtex, pytest-cover, pyannote.database, nvidia-resiliency-ext, nerfacc, nemo_toolkit, multi-storage-client, lhotse, kornia, datasets, DataProperty, boto3, bitsandbytes, accelerated-scan, torchprofile, tabledata, pytorch-lightning, pytest-coverage, pyannote.metrics, nvidia-pytriton, nemo_text_processing, megatron-energon, evaluate, taming-transformers, pytablewriter, open_clip_torch, lightning, nvidia-lm-eval, megatron_core\n",
            "  Attempting uninstall: psutil\n",
            "    Found existing installation: psutil 5.9.5\n",
            "    Uninstalling psutil-5.9.5:\n",
            "      Successfully uninstalled psutil-5.9.5\n",
            "  Attempting uninstall: immutabledict\n",
            "    Found existing installation: immutabledict 4.2.1\n",
            "    Uninstalling immutabledict-4.2.1:\n",
            "      Successfully uninstalled immutabledict-4.2.1\n",
            "  Attempting uninstall: fsspec\n",
            "    Found existing installation: fsspec 2025.3.2\n",
            "    Uninstalling fsspec-2025.3.2:\n",
            "      Successfully uninstalled fsspec-2025.3.2\n",
            "  Attempting uninstall: beautifulsoup4\n",
            "    Found existing installation: beautifulsoup4 4.13.3\n",
            "    Uninstalling beautifulsoup4-4.13.3:\n",
            "      Successfully uninstalled beautifulsoup4-4.13.3\n",
            "  Attempting uninstall: tensorstore\n",
            "    Found existing installation: tensorstore 0.1.73\n",
            "    Uninstalling tensorstore-0.1.73:\n",
            "      Successfully uninstalled tensorstore-0.1.73\n",
            "  Attempting uninstall: httpx\n",
            "    Found existing installation: httpx 0.28.1\n",
            "    Uninstalling httpx-0.28.1:\n",
            "      Successfully uninstalled httpx-0.28.1\n",
            "  Attempting uninstall: openai\n",
            "    Found existing installation: openai 1.70.0\n",
            "    Uninstalling openai-1.70.0:\n",
            "      Successfully uninstalled openai-1.70.0\n",
            "  Attempting uninstall: transformers\n",
            "    Found existing installation: transformers 4.50.3\n",
            "    Uninstalling transformers-4.50.3:\n",
            "      Successfully uninstalled transformers-4.50.3\n",
            "  Attempting uninstall: nemo_toolkit\n",
            "    Found existing installation: nemo-toolkit 2.4.0rc0\n",
            "    Uninstalling nemo-toolkit-2.4.0rc0:\n",
            "      Successfully uninstalled nemo-toolkit-2.4.0rc0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "google-genai 1.9.0 requires httpx<1.0.0,>=0.28.1, but you have httpx 0.27.0 which is incompatible.\n",
            "gcsfs 2025.3.2 requires fsspec==2025.3.2, but you have fsspec 2024.12.0 which is incompatible.\n",
            "cuml-cu12 25.2.1 requires numba<0.61.0a0,>=0.59.1, but you have numba 0.61.0 which is incompatible.\n",
            "distributed-ucxx-cu12 0.42.0 requires numba<0.61.0a0,>=0.59.1, but you have numba 0.61.0 which is incompatible.\n",
            "cudf-cu12 25.2.1 requires numba<0.61.0a0,>=0.59.1, but you have numba 0.61.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed DataProperty-1.1.0 Levenshtein-0.27.1 accelerated-scan-0.2.0 addict-2.4.0 aiofiles-24.1.0 alembic-1.15.2 aniso8601-10.0.0 asciitree-0.3.3 attrdict-2.0.1 av-14.3.0 backoff-2.2.1 beautifulsoup4-4.13.1 bitsandbytes-0.45.3 black-24.10.0 boto3-1.37.29 botocore-1.37.29 braceexpand-0.1.7 brotli-1.1.0 cdifflib-1.2.6 clip-0.2.0 colorama-0.4.6 colorlog-6.9.0 coverage-7.8.0 cytoolz-1.0.1 dataclasses-json-0.6.7 datasets-3.5.0 decord-0.6.0 dill-0.3.8 distance-0.1.3 docopt-0.6.2 einops_exts-0.0.4 emoji-2.14.1 eval-type-backport-0.2.2 evaluate-0.4.3 faiss-cpu-1.10.0 fastapi-0.115.12 fasteners-0.19 fiddle-0.3.0 filetype-1.2.0 flask_restful-0.3.10 fsspec-2024.12.0 ftfy-6.3.1 g2p_en-2.1.0 gevent-24.11.1 geventhttpclient-2.0.2 httpx-0.27.0 ijson-3.3.0 immutabledict-4.2.0 intervaltree-3.1.0 isort-5.13.2 janome-0.5.0 jedi-0.19.2 jiwer-3.1.0 jmespath-1.0.1 jsonlines-4.0.0 kaldi-python-io-1.2.2 kaldiio-2.18.1 kornia-0.8.0 kornia_rs-0.1.8 langdetect-1.0.9 latexcodec-3.0.0 lhotse-1.31.0 libcst-1.7.0 lightning-2.4.0 lightning-utilities-0.14.3 lilcom-1.8.1 loguru-0.7.3 markdown2-2.5.3 marshmallow-3.26.1 mbstrdecoder-1.1.4 mediapy-1.1.6 megatron-energon-5.2.0 megatron_core-0.11.0 multi-storage-client-0.18.0 multiprocess-0.70.16 mypy-extensions-1.0.0 nemo_text_processing-1.1.0 nemo_toolkit-2.3.0rc3 nerfacc-0.5.3 ninja-1.11.1.4 num2words-0.5.14 numcodecs-0.16.0 nvidia-lm-eval-25.3.1 nvidia-modelopt-0.27.0 nvidia-modelopt-core-0.27.0 nvidia-pytriton-0.5.14 nvidia-resiliency-ext-0.3.0 open_clip_torch-2.24.0 openai-1.61.0 opencc-1.1.9 optuna-4.2.1 pangu-4.0.6.1 parameterized-0.9.0 pathspec-0.12.1 pathvalidate-3.2.3 pesq-0.0.4 plac-1.4.5 portalocker-3.1.1 progress-1.6 psutil-7.0.0 pulp-3.1.1 pyannote.core-5.0.0 pyannote.database-5.1.3 pyannote.metrics-3.2.1 pybind11-2.13.6 pybtex-0.24.0 pybtex-docutils-1.0.3 pydantic-settings-2.8.1 pydub-0.25.1 pyloudnorm-0.1.1 pynini-2.1.6.post1 pypdf-5.4.0 pypinyin-0.54.0 pypinyin-dict-0.9.0 pystoi-0.4.1 pytablewriter-1.2.1 pytest-cov-6.1.1 pytest-cover-3.0.0 pytest-coverage-0.0 pytest-mock-3.14.0 pytest-random-order-1.1.1 pytest-runner-6.0.1 pytest_asyncio-0.26.0 python-dotenv-1.1.0 python-iso639-2025.2.18 python-magic-0.4.27 python-rapidjson-1.20 pytorch-lightning-2.5.1 qwen_vl_utils-0.0.10 rapidfuzz-3.13.0 resampy-0.4.3 rouge_score-0.1.2 s3fs-0.4.2 s3transfer-0.11.4 sacrebleu-2.5.1 sacremoses-0.1.1 sh-2.2.2 sphinxcontrib-bibtex-2.6.3 starlette-0.46.1 tabledata-1.3.4 taming-transformers-0.0.1 tcolorpy-0.1.7 tensorstore-0.1.71 textdistance-4.6.3 texterrors-0.5.1 tiktoken-0.7.0 torchdiffeq-0.2.5 torchmetrics-1.7.1 torchprofile-0.0.4 torchsde-0.2.6 tqdm-multiprocess-0.0.11 trampoline-0.1.2 transformers-4.48.3 trimesh-4.6.6 tritonclient-2.51.0 typepy-1.3.4 typing_inspect-0.9.0 unstructured-0.14.9 unstructured-client-0.32.3 uvicorn-0.34.0 webdataset-0.2.111 word2number-1.1 xxhash-3.5.0 zarr-2.18.5 zope.event-5.0 zope.interface-7.2\n",
            "mkdir: cannot create directory ‘configs’: File exists\n"
          ]
        }
      ],
      "source": [
        "\"\"\"\n",
        "You can run either this notebook locally (if you have all the dependencies and a GPU) or on Google Colab.\n",
        "\n",
        "Instructions for setting up Colab are as follows:\n",
        "1. Open a new Python 3 notebook.\n",
        "2. Import this notebook from GitHub (File -> Upload Notebook -> \"GITHUB\" tab -> copy/paste GitHub URL)\n",
        "3. Connect to an instance with a GPU (Runtime -> Change runtime type -> select \"GPU\" for hardware accelerator)\n",
        "4. Run this cell to set up dependencies.\n",
        "\"\"\"\n",
        "# If you're using Google Colab and not running locally, run this cell.\n",
        "\n",
        "## Install dependencies\n",
        "!pip install wget\n",
        "!apt-get install sox libsndfile1 ffmpeg\n",
        "!pip install text-unidecode\n",
        "\n",
        "# ## Install NeMo\n",
        "BRANCH = 'r2.3.0'\n",
        "!python -m pip install git+https://github.com/NVIDIA/NeMo.git@$BRANCH#egg=nemo_toolkit[all]\n",
        "\n",
        "## Install TorchAudio\n",
        "!pip install torchaudio>=0.10.0 -f https://download.pytorch.org/whl/torch_stable.html\n",
        "\n",
        "## Grab the config we'll use in this example\n",
        "!mkdir configs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6G2TZkaxcM0e"
      },
      "source": [
        "## Foundations of NeMo\n",
        "---------\n",
        "\n",
        "NeMo models leverage [PyTorch Lightning](https://github.com/PyTorchLightning/pytorch-lightning) Module, and are compatible with the entire PyTorch ecosystem. This means that users have the full flexibility of using the higher level APIs provided by PyTorch Lightning (via Trainer), or write their own training and evaluation loops in PyTorch directly (by simply calling the model and the individual components of the model).\n",
        "\n",
        "For NeMo developers, a \"Model\" is the neural network(s) as well as all the infrastructure supporting those network(s), wrapped into a singular, cohesive unit. As such, all NeMo models are constructed to contain the following out of the box (at the bare minimum, some models support additional functionality too!) -\n",
        "\n",
        " -  Neural Network architecture - all of the modules that are required for the model.\n",
        "\n",
        " -  Dataset + Data Loaders - all of the components that prepare the data for consumption during training or evaluation.\n",
        "\n",
        " -  Preprocessing + Postprocessing - all of the components that process the datasets so they can easily be consumed by the modules.\n",
        "\n",
        " -  Optimizer + Schedulers - basic defaults that work out of the box, and allow further experimentation with ease.\n",
        "\n",
        " - Any other supporting infrastructure - tokenizers, language model configuration, data augmentation etc.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "XxAwtqWBQrNk",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "a6ef853a-2ea1-4fa2-a365-f62d01705048"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'2.3.0rc3'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 3
        }
      ],
      "source": [
        "import nemo\n",
        "nemo.__version__"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H01SHfKQh-gV"
      },
      "source": [
        "## NeMo Collections\n",
        "\n",
        "NeMo is sub-divided into a few fundamental collections based on their domains - `asr`, `nlp`, `tts`. When you performed the `import nemo` statement above, none of the above collections were imported. This is because you might not need all of the collections at once, so NeMo allows partial imports of just one or more collection, as and when you require them.\n",
        "\n",
        "-------\n",
        "Let's import the above three collections -"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "J09NNa8fhth7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dd7d84a8-1f26-4d4c-c07a-b70a6d08e8be"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[NeMo W 2025-04-08 06:20:14 nemo_logging:405] Please use the EncDecSpeakerLabelModel instead of this model. EncDecClassificationModel model is kept for backward compatibility with older models.\n"
          ]
        }
      ],
      "source": [
        "import nemo.collections.asr as nemo_asr\n",
        "import nemo.collections.nlp as nemo_nlp\n",
        "import nemo.collections.tts as nemo_tts"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bSvYoeBrjPza"
      },
      "source": [
        "## NeMo Models in Collections\n",
        "\n",
        "NeMo contains several models for each of its collections, pertaining to certain common tasks involved in conversational AI. At a brief glance, let's look at all the Models that NeMo offers for the above 3 collections."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "9LbbC_92i41f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "921f39ac-d605-43d7-cc18-be79133005a5"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['ASRModel',\n",
              " 'EncDecCTCModel',\n",
              " 'EncDecClassificationModel',\n",
              " 'EncDecDenoiseMaskedTokenPredModel',\n",
              " 'EncDecDiarLabelModel',\n",
              " 'EncDecFrameClassificationModel',\n",
              " 'EncDecHybridRNNTCTCBPEModel',\n",
              " 'EncDecHybridRNNTCTCModel',\n",
              " 'EncDecK2RnntSeqModel',\n",
              " 'EncDecK2SeqModel',\n",
              " 'EncDecMaskedTokenPredModel',\n",
              " 'EncDecMultiTaskModel',\n",
              " 'EncDecRNNTBPEModel',\n",
              " 'EncDecRNNTModel',\n",
              " 'EncDecSpeakerLabelModel',\n",
              " 'SLUIntentSlotBPEModel',\n",
              " 'SortformerEncLabelModel',\n",
              " 'SpeechEncDecSelfSupervisedModel']"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ],
      "source": [
        "asr_models = [model for model in dir(nemo_asr.models) if model.endswith(\"Model\")]\n",
        "asr_models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "t5_ax9Z8j9FC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "02821ab7-f3d2-4f11-f66f-72b372bd1532"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['BERTLMModel',\n",
              " 'BertDPRModel',\n",
              " 'BertJointIRModel',\n",
              " 'DuplexDecoderModel',\n",
              " 'DuplexTaggerModel',\n",
              " 'DuplexTextNormalizationModel',\n",
              " 'EntityLinkingModel',\n",
              " 'GLUEModel',\n",
              " 'IntentSlotClassificationModel',\n",
              " 'MTEncDecModel',\n",
              " 'MegatronGPTPromptLearningModel',\n",
              " 'MultiLabelIntentSlotClassificationModel',\n",
              " 'PunctuationCapitalizationLexicalAudioModel',\n",
              " 'PunctuationCapitalizationModel',\n",
              " 'QAModel',\n",
              " 'SpellcheckingAsrCustomizationModel',\n",
              " 'Text2SparqlModel',\n",
              " 'TextClassificationModel',\n",
              " 'ThutmoseTaggerModel',\n",
              " 'TokenClassificationModel',\n",
              " 'TransformerLMModel',\n",
              " 'ZeroShotIntentModel']"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ],
      "source": [
        "nlp_models = [model for model in dir(nemo_nlp.models) if model.endswith(\"Model\")]\n",
        "nlp_models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bQdR6RJdkezq"
      },
      "outputs": [],
      "source": [
        "tts_models = [model for model in dir(nemo_tts.models) if model.endswith(\"Model\")]\n",
        "tts_models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iWKxKQnSkj9Z"
      },
      "source": [
        "## The NeMo Model\n",
        "\n",
        "Let's dive deeper into what a NeMo model really is. There are many ways we can create these models - we can use the constructor and pass in a config, we can instantiate the model from a pre-trained checkpoint, or simply pass a pre-trained model name and instantiate a model directly from the cloud !\n",
        "\n",
        "---------\n",
        "For now, let's try to work with an ASR model - [Citrinet](https://arxiv.org/abs/2104.01721)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "n-XOQaW1kh3v",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "528924d0-9da8-4eda-a7b2-db5c806d4a80"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[NeMo I 2025-04-08 06:20:58 nemo_logging:393] Downloading from: https://api.ngc.nvidia.com/v2/models/nvidia/nemo/stt_en_citrinet_512/versions/1.0.0rc1/files/stt_en_citrinet_512.nemo to /root/.cache/torch/NeMo/NeMo_2.3.0rc3/stt_en_citrinet_512/3262321355385bb7cf5a583146117d77/stt_en_citrinet_512.nemo\n",
            "[NeMo I 2025-04-08 06:21:01 nemo_logging:393] Instantiating model from pre-trained checkpoint\n",
            "[NeMo I 2025-04-08 06:21:03 nemo_logging:393] Tokenizer SentencePieceTokenizer initialized with 1024 tokens\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[NeMo W 2025-04-08 06:21:04 nemo_logging:405] If you intend to do training or fine-tuning, please call the ModelPT.setup_training_data() method and provide a valid configuration file to setup the train data loader.\n",
            "    Train config : \n",
            "    manifest_filepath: null\n",
            "    sample_rate: 16000\n",
            "    batch_size: 32\n",
            "    trim_silence: true\n",
            "    max_duration: 16.7\n",
            "    shuffle: true\n",
            "    is_tarred: false\n",
            "    tarred_audio_filepaths: null\n",
            "    \n",
            "[NeMo W 2025-04-08 06:21:04 nemo_logging:405] If you intend to do validation, please call the ModelPT.setup_validation_data() or ModelPT.setup_multiple_validation_data() method and provide a valid configuration file to setup the validation data loader(s). \n",
            "    Validation config : \n",
            "    manifest_filepath: null\n",
            "    sample_rate: 16000\n",
            "    batch_size: 32\n",
            "    shuffle: false\n",
            "    \n",
            "[NeMo W 2025-04-08 06:21:04 nemo_logging:405] Please call the ModelPT.setup_test_data() or ModelPT.setup_multiple_test_data() method and provide a valid configuration file to setup the test data loader(s).\n",
            "    Test config : \n",
            "    manifest_filepath:\n",
            "    - /home/smajumdar/PycharmProjects/nemo-eval/nemo_eval/librispeech/manifests/dev_other.json\n",
            "    sample_rate: 16000\n",
            "    batch_size: 32\n",
            "    shuffle: false\n",
            "    num_workers: 12\n",
            "    pin_memory: true\n",
            "    \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[NeMo I 2025-04-08 06:21:04 nemo_logging:393] PADDING: 16\n",
            "[NeMo I 2025-04-08 06:21:05 nemo_logging:393] Model EncDecCTCModelBPE was successfully restored from /root/.cache/torch/NeMo/NeMo_2.3.0rc3/stt_en_citrinet_512/3262321355385bb7cf5a583146117d77/stt_en_citrinet_512.nemo.\n"
          ]
        }
      ],
      "source": [
        "citrinet = nemo_asr.models.EncDecCTCModelBPE.from_pretrained('stt_en_citrinet_512')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "YP4X7KVPli6g",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2582930e-5055-4561-ab76-0c20736b4a7b"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "  | Name              | Type                              | Params | Mode \n",
              "--------------------------------------------------------------------------------\n",
              "0 | preprocessor      | AudioToMelSpectrogramPreprocessor | 0      | train\n",
              "1 | encoder           | ConvASREncoder                    | 36.3 M | train\n",
              "2 | decoder           | ConvASRDecoder                    | 657 K  | train\n",
              "3 | loss              | CTCLoss                           | 0      | train\n",
              "4 | spec_augmentation | SpectrogramAugmentation           | 0      | train\n",
              "5 | wer               | WER                               | 0      | train\n",
              "--------------------------------------------------------------------------------\n",
              "37.0 M    Trainable params\n",
              "0         Non-trainable params\n",
              "37.0 M    Total params\n",
              "147.977   Total estimated model params size (MB)\n",
              "943       Modules in train mode\n",
              "0         Modules in eval mode"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ],
      "source": [
        "citrinet.summarize()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MB91Swu0pIKr"
      },
      "source": [
        "## Model Configuration using OmegaConf\n",
        "--------\n",
        "\n",
        "So we could download, instantiate and analyse the high level structure of the `Citrinet` model in a few lines! Now let's delve deeper into the configuration file that makes the model work.\n",
        "\n",
        "First, we import [OmegaConf](https://omegaconf.readthedocs.io/en/latest/). OmegaConf is an excellent library that is used throughout NeMo in order to enable us to perform yaml configuration management more easily. Additionally, it plays well with another library, [Hydra](https://hydra.cc/docs/intro/), that is used by NeMo to perform on the fly config edits from the command line, dramatically boosting ease of use of our config files !"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "RkgrDJvumFER"
      },
      "outputs": [],
      "source": [
        "from omegaconf import OmegaConf"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CktakfBluA56"
      },
      "source": [
        "All NeMo models come packaged with their model configuration inside the `cfg` attribute. While technically it is meant to be config declaration of the model as it has been currently constructed, `cfg` is an essential tool to modify the behaviour of the Model after it has been constructed. It can be safely used to make it easier to perform many essential tasks inside Models.\n",
        "\n",
        "To be doubly sure, we generally work on a copy of the config until we are ready to edit it inside the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "ISd6z7sXt9Mm"
      },
      "outputs": [],
      "source": [
        "import copy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "N2_SiLHRve8A",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bd2c9b83-26a7-4218-b731-3ee0b6e4ebd3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "sample_rate: 16000\n",
            "train_ds:\n",
            "  manifest_filepath: null\n",
            "  sample_rate: 16000\n",
            "  batch_size: 32\n",
            "  trim_silence: true\n",
            "  max_duration: 16.7\n",
            "  shuffle: true\n",
            "  is_tarred: false\n",
            "  tarred_audio_filepaths: null\n",
            "validation_ds:\n",
            "  manifest_filepath: null\n",
            "  sample_rate: 16000\n",
            "  batch_size: 32\n",
            "  shuffle: false\n",
            "test_ds:\n",
            "  manifest_filepath:\n",
            "  - /home/smajumdar/PycharmProjects/nemo-eval/nemo_eval/librispeech/manifests/dev_other.json\n",
            "  sample_rate: 16000\n",
            "  batch_size: 32\n",
            "  shuffle: false\n",
            "  num_workers: 12\n",
            "  pin_memory: true\n",
            "model_defaults:\n",
            "  repeat: 5\n",
            "  dropout: 0.0\n",
            "  separable: true\n",
            "  se: true\n",
            "  se_context_size: -1\n",
            "tokenizer:\n",
            "  dir: /home/smajumdar/PycharmProjects/nemo-eval/nemo_beta_eval/asrset/manifests/asrset_1.4/tokenizers/no_appen/tokenizer_spe_unigram_v1024/\n",
            "  type: bpe\n",
            "preprocessor:\n",
            "  _target_: nemo.collections.asr.modules.AudioToMelSpectrogramPreprocessor\n",
            "  sample_rate: 16000\n",
            "  normalize: per_feature\n",
            "  window_size: 0.025\n",
            "  window_stride: 0.01\n",
            "  window: hann\n",
            "  features: 80\n",
            "  n_fft: 512\n",
            "  frame_splicing: 1\n",
            "  dither: 1.0e-05\n",
            "  pad_to: 16\n",
            "  stft_conv: false\n",
            "spec_augment:\n",
            "  _target_: nemo.collections.asr.modules.SpectrogramAugmentation\n",
            "  freq_masks: 2\n",
            "  time_masks: 10\n",
            "  freq_width: 27\n",
            "  time_width: 0.05\n",
            "encoder:\n",
            "  _target_: nemo.collections.asr.modules.ConvASREncoder\n",
            "  feat_in: 80\n",
            "  activation: relu\n",
            "  conv_mask: true\n",
            "  jasper:\n",
            "  - filters: 512\n",
            "    repeat: 1\n",
            "    kernel:\n",
            "    - 5\n",
            "    stride:\n",
            "    - 1\n",
            "    dilation:\n",
            "    - 1\n",
            "    dropout: 0.0\n",
            "    residual: false\n",
            "    separable: true\n",
            "    se: true\n",
            "    se_context_size: -1\n",
            "  - filters: 512\n",
            "    repeat: 5\n",
            "    kernel:\n",
            "    - 11\n",
            "    stride:\n",
            "    - 2\n",
            "    dilation:\n",
            "    - 1\n",
            "    dropout: 0.0\n",
            "    residual: true\n",
            "    separable: true\n",
            "    se: true\n",
            "    se_context_size: -1\n",
            "    stride_last: true\n",
            "    residual_mode: stride_add\n",
            "  - filters: 512\n",
            "    repeat: 5\n",
            "    kernel:\n",
            "    - 13\n",
            "    stride:\n",
            "    - 1\n",
            "    dilation:\n",
            "    - 1\n",
            "    dropout: 0.0\n",
            "    residual: true\n",
            "    separable: true\n",
            "    se: true\n",
            "    se_context_size: -1\n",
            "  - filters: 512\n",
            "    repeat: 5\n",
            "    kernel:\n",
            "    - 15\n",
            "    stride:\n",
            "    - 1\n",
            "    dilation:\n",
            "    - 1\n",
            "    dropout: 0.0\n",
            "    residual: true\n",
            "    separable: true\n",
            "    se: true\n",
            "    se_context_size: -1\n",
            "  - filters: 512\n",
            "    repeat: 5\n",
            "    kernel:\n",
            "    - 17\n",
            "    stride:\n",
            "    - 1\n",
            "    dilation:\n",
            "    - 1\n",
            "    dropout: 0.0\n",
            "    residual: true\n",
            "    separable: true\n",
            "    se: true\n",
            "    se_context_size: -1\n",
            "  - filters: 512\n",
            "    repeat: 5\n",
            "    kernel:\n",
            "    - 19\n",
            "    stride:\n",
            "    - 1\n",
            "    dilation:\n",
            "    - 1\n",
            "    dropout: 0.0\n",
            "    residual: true\n",
            "    separable: true\n",
            "    se: true\n",
            "    se_context_size: -1\n",
            "  - filters: 512\n",
            "    repeat: 5\n",
            "    kernel:\n",
            "    - 21\n",
            "    stride:\n",
            "    - 1\n",
            "    dilation:\n",
            "    - 1\n",
            "    dropout: 0.0\n",
            "    residual: true\n",
            "    separable: true\n",
            "    se: true\n",
            "    se_context_size: -1\n",
            "  - filters: 512\n",
            "    repeat: 5\n",
            "    kernel:\n",
            "    - 13\n",
            "    stride:\n",
            "    - 2\n",
            "    dilation:\n",
            "    - 1\n",
            "    dropout: 0.0\n",
            "    residual: true\n",
            "    separable: true\n",
            "    se: true\n",
            "    se_context_size: -1\n",
            "    stride_last: true\n",
            "    residual_mode: stride_add\n",
            "  - filters: 512\n",
            "    repeat: 5\n",
            "    kernel:\n",
            "    - 15\n",
            "    stride:\n",
            "    - 1\n",
            "    dilation:\n",
            "    - 1\n",
            "    dropout: 0.0\n",
            "    residual: true\n",
            "    separable: true\n",
            "    se: true\n",
            "    se_context_size: -1\n",
            "  - filters: 512\n",
            "    repeat: 5\n",
            "    kernel:\n",
            "    - 17\n",
            "    stride:\n",
            "    - 1\n",
            "    dilation:\n",
            "    - 1\n",
            "    dropout: 0.0\n",
            "    residual: true\n",
            "    separable: true\n",
            "    se: true\n",
            "    se_context_size: -1\n",
            "  - filters: 512\n",
            "    repeat: 5\n",
            "    kernel:\n",
            "    - 19\n",
            "    stride:\n",
            "    - 1\n",
            "    dilation:\n",
            "    - 1\n",
            "    dropout: 0.0\n",
            "    residual: true\n",
            "    separable: true\n",
            "    se: true\n",
            "    se_context_size: -1\n",
            "  - filters: 512\n",
            "    repeat: 5\n",
            "    kernel:\n",
            "    - 21\n",
            "    stride:\n",
            "    - 1\n",
            "    dilation:\n",
            "    - 1\n",
            "    dropout: 0.0\n",
            "    residual: true\n",
            "    separable: true\n",
            "    se: true\n",
            "    se_context_size: -1\n",
            "  - filters: 512\n",
            "    repeat: 5\n",
            "    kernel:\n",
            "    - 23\n",
            "    stride:\n",
            "    - 1\n",
            "    dilation:\n",
            "    - 1\n",
            "    dropout: 0.0\n",
            "    residual: true\n",
            "    separable: true\n",
            "    se: true\n",
            "    se_context_size: -1\n",
            "  - filters: 512\n",
            "    repeat: 5\n",
            "    kernel:\n",
            "    - 25\n",
            "    stride:\n",
            "    - 1\n",
            "    dilation:\n",
            "    - 1\n",
            "    dropout: 0.0\n",
            "    residual: true\n",
            "    separable: true\n",
            "    se: true\n",
            "    se_context_size: -1\n",
            "  - filters: 512\n",
            "    repeat: 5\n",
            "    kernel:\n",
            "    - 25\n",
            "    stride:\n",
            "    - 2\n",
            "    dilation:\n",
            "    - 1\n",
            "    dropout: 0.0\n",
            "    residual: true\n",
            "    separable: true\n",
            "    se: true\n",
            "    se_context_size: -1\n",
            "    stride_last: true\n",
            "    residual_mode: stride_add\n",
            "  - filters: 512\n",
            "    repeat: 5\n",
            "    kernel:\n",
            "    - 27\n",
            "    stride:\n",
            "    - 1\n",
            "    dilation:\n",
            "    - 1\n",
            "    dropout: 0.0\n",
            "    residual: true\n",
            "    separable: true\n",
            "    se: true\n",
            "    se_context_size: -1\n",
            "  - filters: 512\n",
            "    repeat: 5\n",
            "    kernel:\n",
            "    - 29\n",
            "    stride:\n",
            "    - 1\n",
            "    dilation:\n",
            "    - 1\n",
            "    dropout: 0.0\n",
            "    residual: true\n",
            "    separable: true\n",
            "    se: true\n",
            "    se_context_size: -1\n",
            "  - filters: 512\n",
            "    repeat: 5\n",
            "    kernel:\n",
            "    - 31\n",
            "    stride:\n",
            "    - 1\n",
            "    dilation:\n",
            "    - 1\n",
            "    dropout: 0.0\n",
            "    residual: true\n",
            "    separable: true\n",
            "    se: true\n",
            "    se_context_size: -1\n",
            "  - filters: 512\n",
            "    repeat: 5\n",
            "    kernel:\n",
            "    - 33\n",
            "    stride:\n",
            "    - 1\n",
            "    dilation:\n",
            "    - 1\n",
            "    dropout: 0.0\n",
            "    residual: true\n",
            "    separable: true\n",
            "    se: true\n",
            "    se_context_size: -1\n",
            "  - filters: 512\n",
            "    repeat: 5\n",
            "    kernel:\n",
            "    - 35\n",
            "    stride:\n",
            "    - 1\n",
            "    dilation:\n",
            "    - 1\n",
            "    dropout: 0.0\n",
            "    residual: true\n",
            "    separable: true\n",
            "    se: true\n",
            "    se_context_size: -1\n",
            "  - filters: 512\n",
            "    repeat: 5\n",
            "    kernel:\n",
            "    - 37\n",
            "    stride:\n",
            "    - 1\n",
            "    dilation:\n",
            "    - 1\n",
            "    dropout: 0.0\n",
            "    residual: true\n",
            "    separable: true\n",
            "    se: true\n",
            "    se_context_size: -1\n",
            "  - filters: 512\n",
            "    repeat: 5\n",
            "    kernel:\n",
            "    - 39\n",
            "    stride:\n",
            "    - 1\n",
            "    dilation:\n",
            "    - 1\n",
            "    dropout: 0.0\n",
            "    residual: true\n",
            "    separable: true\n",
            "    se: true\n",
            "    se_context_size: -1\n",
            "  - filters: 640\n",
            "    repeat: 1\n",
            "    kernel:\n",
            "    - 41\n",
            "    stride:\n",
            "    - 1\n",
            "    dilation:\n",
            "    - 1\n",
            "    dropout: 0.0\n",
            "    residual: false\n",
            "    separable: true\n",
            "    se: true\n",
            "    se_context_size: -1\n",
            "decoder:\n",
            "  _target_: nemo.collections.asr.modules.ConvASRDecoder\n",
            "  feat_in: 640\n",
            "  num_classes: 1024\n",
            "  vocabulary:\n",
            "  - <unk>\n",
            "  - s\n",
            "  - ▁the\n",
            "  - t\n",
            "  - ▁a\n",
            "  - ▁i\n",
            "  - ''''\n",
            "  - ▁and\n",
            "  - ▁to\n",
            "  - ed\n",
            "  - d\n",
            "  - ▁of\n",
            "  - e\n",
            "  - ▁in\n",
            "  - ing\n",
            "  - .\n",
            "  - ▁it\n",
            "  - ▁you\n",
            "  - 'n'\n",
            "  - ▁that\n",
            "  - m\n",
            "  - 'y'\n",
            "  - er\n",
            "  - ▁he\n",
            "  - re\n",
            "  - r\n",
            "  - ▁was\n",
            "  - ▁is\n",
            "  - ▁for\n",
            "  - ▁know\n",
            "  - a\n",
            "  - p\n",
            "  - c\n",
            "  - ','\n",
            "  - ▁be\n",
            "  - o\n",
            "  - ▁but\n",
            "  - ▁they\n",
            "  - g\n",
            "  - ▁so\n",
            "  - ly\n",
            "  - b\n",
            "  - ▁s\n",
            "  - ▁yeah\n",
            "  - ▁we\n",
            "  - ▁have\n",
            "  - ▁re\n",
            "  - ▁like\n",
            "  - l\n",
            "  - ▁on\n",
            "  - ll\n",
            "  - u\n",
            "  - ▁with\n",
            "  - ▁do\n",
            "  - al\n",
            "  - ▁not\n",
            "  - ▁are\n",
            "  - or\n",
            "  - ar\n",
            "  - le\n",
            "  - ▁this\n",
            "  - ▁as\n",
            "  - es\n",
            "  - ▁c\n",
            "  - ▁de\n",
            "  - f\n",
            "  - in\n",
            "  - i\n",
            "  - ve\n",
            "  - ▁uh\n",
            "  - ent\n",
            "  - ▁or\n",
            "  - ▁what\n",
            "  - ▁me\n",
            "  - ▁t\n",
            "  - ▁at\n",
            "  - ▁my\n",
            "  - ▁his\n",
            "  - ▁there\n",
            "  - w\n",
            "  - ▁all\n",
            "  - ▁just\n",
            "  - h\n",
            "  - ▁can\n",
            "  - ri\n",
            "  - il\n",
            "  - k\n",
            "  - ic\n",
            "  - ▁e\n",
            "  - ▁\n",
            "  - ▁um\n",
            "  - ▁don\n",
            "  - ▁b\n",
            "  - ▁had\n",
            "  - ch\n",
            "  - ation\n",
            "  - en\n",
            "  - th\n",
            "  - ▁no\n",
            "  - ▁she\n",
            "  - it\n",
            "  - ▁one\n",
            "  - ▁think\n",
            "  - ▁st\n",
            "  - ▁if\n",
            "  - ▁from\n",
            "  - ter\n",
            "  - ▁an\n",
            "  - an\n",
            "  - ur\n",
            "  - ▁out\n",
            "  - 'on'\n",
            "  - ▁go\n",
            "  - ck\n",
            "  - ▁would\n",
            "  - ▁were\n",
            "  - ▁w\n",
            "  - ▁will\n",
            "  - ▁about\n",
            "  - ▁right\n",
            "  - ment\n",
            "  - ▁her\n",
            "  - te\n",
            "  - ion\n",
            "  - ▁well\n",
            "  - ▁by\n",
            "  - ce\n",
            "  - ▁g\n",
            "  - ▁oh\n",
            "  - ▁up\n",
            "  - ro\n",
            "  - ra\n",
            "  - ▁when\n",
            "  - ▁some\n",
            "  - ▁also\n",
            "  - ▁their\n",
            "  - ers\n",
            "  - ow\n",
            "  - ▁more\n",
            "  - ▁time\n",
            "  - ate\n",
            "  - ▁has\n",
            "  - ▁people\n",
            "  - ▁see\n",
            "  - ▁pa\n",
            "  - el\n",
            "  - ▁get\n",
            "  - ▁ex\n",
            "  - ▁mean\n",
            "  - li\n",
            "  - ▁really\n",
            "  - v\n",
            "  - ▁ra\n",
            "  - ▁been\n",
            "  - ▁said\n",
            "  - '-'\n",
            "  - la\n",
            "  - ge\n",
            "  - ▁how\n",
            "  - ▁po\n",
            "  - ir\n",
            "  - ▁mo\n",
            "  - ▁who\n",
            "  - ▁because\n",
            "  - ▁co\n",
            "  - ▁other\n",
            "  - ▁f\n",
            "  - id\n",
            "  - ol\n",
            "  - ▁un\n",
            "  - ▁now\n",
            "  - ▁work\n",
            "  - ist\n",
            "  - us\n",
            "  - ▁your\n",
            "  - ▁them\n",
            "  - ver\n",
            "  - as\n",
            "  - ne\n",
            "  - ▁ca\n",
            "  - lo\n",
            "  - ▁fa\n",
            "  - ▁him\n",
            "  - ng\n",
            "  - ▁good\n",
            "  - ▁could\n",
            "  - ▁pro\n",
            "  - ive\n",
            "  - ▁con\n",
            "  - de\n",
            "  - un\n",
            "  - age\n",
            "  - ▁ma\n",
            "  - '?'\n",
            "  - at\n",
            "  - ▁ro\n",
            "  - ▁ba\n",
            "  - ▁then\n",
            "  - ▁com\n",
            "  - est\n",
            "  - vi\n",
            "  - ▁dis\n",
            "  - ies\n",
            "  - ance\n",
            "  - ▁su\n",
            "  - ▁even\n",
            "  - ▁any\n",
            "  - ut\n",
            "  - ad\n",
            "  - ul\n",
            "  - ▁se\n",
            "  - ▁two\n",
            "  - ▁bu\n",
            "  - ▁lo\n",
            "  - ▁say\n",
            "  - ▁la\n",
            "  - ▁fi\n",
            "  - is\n",
            "  - ▁li\n",
            "  - ▁over\n",
            "  - ▁new\n",
            "  - ▁man\n",
            "  - ▁sp\n",
            "  - ity\n",
            "  - ▁did\n",
            "  - ▁bo\n",
            "  - ▁very\n",
            "  - x\n",
            "  - end\n",
            "  - ▁which\n",
            "  - ▁our\n",
            "  - ▁after\n",
            "  - ▁o\n",
            "  - ke\n",
            "  - ▁p\n",
            "  - im\n",
            "  - ▁want\n",
            "  - ▁ha\n",
            "  - ▁v\n",
            "  - z\n",
            "  - ▁where\n",
            "  - ard\n",
            "  - um\n",
            "  - ▁into\n",
            "  - ru\n",
            "  - ▁di\n",
            "  - ▁lot\n",
            "  - ▁dr\n",
            "  - mp\n",
            "  - ▁day\n",
            "  - ated\n",
            "  - ci\n",
            "  - ▁these\n",
            "  - ▁than\n",
            "  - ▁take\n",
            "  - ▁kind\n",
            "  - ▁got\n",
            "  - ight\n",
            "  - ▁make\n",
            "  - ence\n",
            "  - ▁pre\n",
            "  - ▁going\n",
            "  - ish\n",
            "  - ▁k\n",
            "  - able\n",
            "  - ▁look\n",
            "  - ti\n",
            "  - per\n",
            "  - ▁here\n",
            "  - ▁en\n",
            "  - ▁ah\n",
            "  - ry\n",
            "  - ▁too\n",
            "  - ▁part\n",
            "  - ant\n",
            "  - one\n",
            "  - ▁ho\n",
            "  - ▁much\n",
            "  - ▁way\n",
            "  - ▁sa\n",
            "  - ▁something\n",
            "  - mo\n",
            "  - ▁us\n",
            "  - ▁th\n",
            "  - ▁mhm\n",
            "  - ▁mi\n",
            "  - ▁off\n",
            "  - pe\n",
            "  - ▁back\n",
            "  - les\n",
            "  - ▁cr\n",
            "  - ▁ri\n",
            "  - ▁fe\n",
            "  - und\n",
            "  - ▁fl\n",
            "  - port\n",
            "  - ▁school\n",
            "  - ▁ch\n",
            "  - ▁should\n",
            "  - ▁first\n",
            "  - ▁only\n",
            "  - ▁le\n",
            "  - ot\n",
            "  - tion\n",
            "  - ▁little\n",
            "  - ▁da\n",
            "  - ▁hu\n",
            "  - ▁d\n",
            "  - me\n",
            "  - ta\n",
            "  - ▁down\n",
            "  - ▁okay\n",
            "  - ▁come\n",
            "  - ain\n",
            "  - ff\n",
            "  - ▁car\n",
            "  - co\n",
            "  - ▁need\n",
            "  - ture\n",
            "  - ▁many\n",
            "  - ▁things\n",
            "  - ▁ta\n",
            "  - qu\n",
            "  - man\n",
            "  - ty\n",
            "  - iv\n",
            "  - ▁year\n",
            "  - he\n",
            "  - ▁thing\n",
            "  - ho\n",
            "  - ▁singapore\n",
            "  - po\n",
            "  - ▁vi\n",
            "  - ▁sc\n",
            "  - ▁still\n",
            "  - der\n",
            "  - ▁hi\n",
            "  - ▁never\n",
            "  - ▁qu\n",
            "  - ia\n",
            "  - ▁fr\n",
            "  - ▁min\n",
            "  - ▁most\n",
            "  - om\n",
            "  - ful\n",
            "  - ▁bi\n",
            "  - ▁long\n",
            "  - ig\n",
            "  - ▁years\n",
            "  - ous\n",
            "  - ▁three\n",
            "  - ▁play\n",
            "  - ▁before\n",
            "  - ▁pi\n",
            "  - ical\n",
            "  - ▁those\n",
            "  - ▁comp\n",
            "  - huh\n",
            "  - ▁live\n",
            "  - tor\n",
            "  - ise\n",
            "  - ▁old\n",
            "  - am\n",
            "  - rr\n",
            "  - ▁sta\n",
            "  - ▁n\n",
            "  - ick\n",
            "  - di\n",
            "  - ma\n",
            "  - ary\n",
            "  - ction\n",
            "  - ▁friend\n",
            "  - ition\n",
            "  - ▁gu\n",
            "  - ▁through\n",
            "  - pp\n",
            "  - for\n",
            "  - ie\n",
            "  - ious\n",
            "  - ▁sh\n",
            "  - ▁home\n",
            "  - lu\n",
            "  - ▁high\n",
            "  - ian\n",
            "  - cu\n",
            "  - ▁help\n",
            "  - ▁give\n",
            "  - ▁talk\n",
            "  - ▁sha\n",
            "  - ▁such\n",
            "  - ▁didn\n",
            "  - em\n",
            "  - ▁may\n",
            "  - ▁ga\n",
            "  - ▁'\n",
            "  - ▁gra\n",
            "  - ▁guess\n",
            "  - ▁every\n",
            "  - ▁app\n",
            "  - tic\n",
            "  - ▁tra\n",
            "  - ▁\"\n",
            "  - op\n",
            "  - ▁made\n",
            "  - '\"'\n",
            "  - ▁op\n",
            "  - ▁own\n",
            "  - ▁mar\n",
            "  - 'no'\n",
            "  - ▁ph\n",
            "  - ▁life\n",
            "  - ▁y\n",
            "  - ak\n",
            "  - ine\n",
            "  - ▁pu\n",
            "  - ▁place\n",
            "  - ▁always\n",
            "  - ▁start\n",
            "  - ▁jo\n",
            "  - ▁pe\n",
            "  - ▁let\n",
            "  - ▁name\n",
            "  - ni\n",
            "  - ▁same\n",
            "  - ▁last\n",
            "  - ▁cl\n",
            "  - ph\n",
            "  - ▁both\n",
            "  - ▁pri\n",
            "  - ities\n",
            "  - ▁another\n",
            "  - and\n",
            "  - ▁al\n",
            "  - ▁boy\n",
            "  - ving\n",
            "  - ▁actually\n",
            "  - ▁person\n",
            "  - ▁went\n",
            "  - ▁yes\n",
            "  - ca\n",
            "  - ally\n",
            "  - ▁h\n",
            "  - ▁great\n",
            "  - ▁thought\n",
            "  - ▁used\n",
            "  - act\n",
            "  - ▁feel\n",
            "  - ward\n",
            "  - ▁different\n",
            "  - ▁cons\n",
            "  - ▁show\n",
            "  - ▁watch\n",
            "  - ▁being\n",
            "  - ▁money\n",
            "  - ay\n",
            "  - ▁try\n",
            "  - ▁why\n",
            "  - ▁big\n",
            "  - ens\n",
            "  - ▁cha\n",
            "  - ▁find\n",
            "  - ▁hand\n",
            "  - ▁real\n",
            "  - ▁four\n",
            "  - ial\n",
            "  - ▁ne\n",
            "  - ▁che\n",
            "  - ▁read\n",
            "  - ▁five\n",
            "  - ▁family\n",
            "  - ag\n",
            "  - ▁change\n",
            "  - ▁add\n",
            "  - ha\n",
            "  - ▁put\n",
            "  - par\n",
            "  - lic\n",
            "  - side\n",
            "  - ▁came\n",
            "  - ▁under\n",
            "  - ness\n",
            "  - ▁per\n",
            "  - j\n",
            "  - ▁around\n",
            "  - ▁end\n",
            "  - ▁house\n",
            "  - if\n",
            "  - ▁while\n",
            "  - vo\n",
            "  - ▁act\n",
            "  - ▁happen\n",
            "  - ▁plan\n",
            "  - mit\n",
            "  - ▁far\n",
            "  - ▁tri\n",
            "  - ▁ten\n",
            "  - ▁du\n",
            "  - ▁win\n",
            "  - ▁tea\n",
            "  - ze\n",
            "  - ▁better\n",
            "  - ▁sure\n",
            "  - ▁mu\n",
            "  - ▁use\n",
            "  - ▁anything\n",
            "  - ▁love\n",
            "  - ▁world\n",
            "  - ▁hard\n",
            "  - ure\n",
            "  - ▁does\n",
            "  - ▁war\n",
            "  - ▁stuff\n",
            "  - ▁ja\n",
            "  - ▁must\n",
            "  - min\n",
            "  - gg\n",
            "  - ▁ru\n",
            "  - ▁care\n",
            "  - ▁tell\n",
            "  - ▁pl\n",
            "  - ▁doing\n",
            "  - ▁probably\n",
            "  - ▁found\n",
            "  - ative\n",
            "  - ▁point\n",
            "  - ach\n",
            "  - ▁ju\n",
            "  - ip\n",
            "  - ▁again\n",
            "  - ▁interest\n",
            "  - ▁state\n",
            "  - ▁week\n",
            "  - na\n",
            "  - ▁might\n",
            "  - ▁pretty\n",
            "  - ▁ki\n",
            "  - ▁fo\n",
            "  - ber\n",
            "  - ▁am\n",
            "  - line\n",
            "  - led\n",
            "  - ▁six\n",
            "  - ▁acc\n",
            "  - ▁bri\n",
            "  - ▁call\n",
            "  - ▁sw\n",
            "  - ▁each\n",
            "  - ▁business\n",
            "  - ▁keep\n",
            "  - ▁away\n",
            "  - cause\n",
            "  - ▁pass\n",
            "  - ▁va\n",
            "  - ▁children\n",
            "  - ▁pay\n",
            "  - ▁count\n",
            "  - ▁public\n",
            "  - ▁everything\n",
            "  - land\n",
            "  - ▁though\n",
            "  - ▁men\n",
            "  - bo\n",
            "  - ▁young\n",
            "  - ▁na\n",
            "  - ▁move\n",
            "  - ough\n",
            "  - ating\n",
            "  - com\n",
            "  - ▁month\n",
            "  - ton\n",
            "  - ▁close\n",
            "  - ▁few\n",
            "  - '!'\n",
            "  - ▁maybe\n",
            "  - ▁imp\n",
            "  - son\n",
            "  - ▁grow\n",
            "  - ▁u\n",
            "  - ▁turn\n",
            "  - ible\n",
            "  - ▁em\n",
            "  - ▁air\n",
            "  - ▁ever\n",
            "  - our\n",
            "  - ▁sea\n",
            "  - ▁fun\n",
            "  - ▁government\n",
            "  - ▁miss\n",
            "  - ▁done\n",
            "  - ▁next\n",
            "  - ▁kids\n",
            "  - ▁cor\n",
            "  - ▁set\n",
            "  - ▁run\n",
            "  - way\n",
            "  - ▁wa\n",
            "  - ▁getting\n",
            "  - ▁eight\n",
            "  - ▁open\n",
            "  - ▁job\n",
            "  - ▁problem\n",
            "  - ook\n",
            "  - ▁night\n",
            "  - ▁learn\n",
            "  - ▁book\n",
            "  - ual\n",
            "  - ▁ti\n",
            "  - ▁best\n",
            "  - cept\n",
            "  - ▁during\n",
            "  - ▁small\n",
            "  - ex\n",
            "  - ▁without\n",
            "  - ▁water\n",
            "  - ▁trans\n",
            "  - ▁course\n",
            "  - ▁once\n",
            "  - ▁sit\n",
            "  - ▁area\n",
            "  - ▁country\n",
            "  - ▁mister\n",
            "  - ▁nothing\n",
            "  - ▁whole\n",
            "  - ▁believe\n",
            "  - ▁service\n",
            "  - ▁took\n",
            "  - ▁face\n",
            "  - ▁bad\n",
            "  - ▁later\n",
            "  - ▁head\n",
            "  - ▁called\n",
            "  - ▁seven\n",
            "  - ▁art\n",
            "  - ▁since\n",
            "  - ▁er\n",
            "  - ▁fact\n",
            "  - ▁city\n",
            "  - ▁market\n",
            "  - ▁hour\n",
            "  - ▁continue\n",
            "  - ship\n",
            "  - ▁invest\n",
            "  - ▁exactly\n",
            "  - ▁large\n",
            "  - ▁true\n",
            "  - ▁nine\n",
            "  - ▁sub\n",
            "  - ▁having\n",
            "  - ▁game\n",
            "  - va\n",
            "  - ▁lu\n",
            "  - ▁conf\n",
            "  - ▁case\n",
            "  - ▁doesn\n",
            "  - ▁certain\n",
            "  - ▁wi\n",
            "  - ▁law\n",
            "  - ▁else\n",
            "  - fi\n",
            "  - ▁left\n",
            "  - ▁enough\n",
            "  - ▁second\n",
            "  - ▁gonna\n",
            "  - ▁food\n",
            "  - ▁hope\n",
            "  - ▁saw\n",
            "  - ▁between\n",
            "  - ▁je\n",
            "  - bi\n",
            "  - ▁girl\n",
            "  - ▁company\n",
            "  - ▁able\n",
            "  - ▁expect\n",
            "  - ▁told\n",
            "  - ▁stand\n",
            "  - ▁group\n",
            "  - ▁main\n",
            "  - ▁walk\n",
            "  - ▁cause\n",
            "  - ▁however\n",
            "  - ▁number\n",
            "  - ▁follow\n",
            "  - ▁near\n",
            "  - ▁yet\n",
            "  - ▁sometimes\n",
            "  - ▁train\n",
            "  - ▁lead\n",
            "  - ▁system\n",
            "  - ▁remain\n",
            "  - ▁develop\n",
            "  - gra\n",
            "  - ▁word\n",
            "  - ▁exc\n",
            "  - ▁together\n",
            "  - ▁consider\n",
            "  - ▁town\n",
            "  - ▁less\n",
            "  - ator\n",
            "  - ▁important\n",
            "  - ▁remember\n",
            "  - ▁free\n",
            "  - ▁quite\n",
            "  - ▁understand\n",
            "  - ▁bra\n",
            "  - ▁support\n",
            "  - ▁idea\n",
            "  - ▁stop\n",
            "  - ▁reason\n",
            "  - ▁nice\n",
            "  - ▁mm\n",
            "  - ▁agree\n",
            "  - ▁low\n",
            "  - ▁against\n",
            "  - ▁issue\n",
            "  - ▁become\n",
            "  - ▁today\n",
            "  - ▁side\n",
            "  - ▁student\n",
            "  - ▁matter\n",
            "  - ▁question\n",
            "  - ▁mother\n",
            "  - ▁father\n",
            "  - ▁hundred\n",
            "  - ▁sort\n",
            "  - ▁eat\n",
            "  - ▁already\n",
            "  - ▁rest\n",
            "  - ▁line\n",
            "  - ▁asked\n",
            "  - ▁include\n",
            "  - ▁upon\n",
            "  - ▁office\n",
            "  - ▁won\n",
            "  - ▁class\n",
            "  - ▁wait\n",
            "  - ▁twenty\n",
            "  - ▁half\n",
            "  - ▁light\n",
            "  - ▁price\n",
            "  - ▁almost\n",
            "  - ash\n",
            "  - ▁child\n",
            "  - ▁sign\n",
            "  - ▁least\n",
            "  - ▁several\n",
            "  - press\n",
            "  - ▁either\n",
            "  - ▁minute\n",
            "  - ▁himself\n",
            "  - ▁parents\n",
            "  - ▁room\n",
            "  - ▁whatever\n",
            "  - ▁general\n",
            "  - ▁cost\n",
            "  - ▁among\n",
            "  - ▁direct\n",
            "  - ▁computer\n",
            "  - ▁appear\n",
            "  - ▁meet\n",
            "  - ▁ski\n",
            "  - ▁return\n",
            "  - ▁couple\n",
            "  - ▁product\n",
            "  - ▁suppose\n",
            "  - ▁definitely\n",
            "  - ▁america\n",
            "  - ▁term\n",
            "  - ▁usually\n",
            "  - ▁strong\n",
            "  - ▁current\n",
            "  - ▁arm\n",
            "  - ▁speak\n",
            "  - ▁local\n",
            "  - ▁south\n",
            "  - ▁experience\n",
            "  - ▁full\n",
            "  - ▁north\n",
            "  - ▁elect\n",
            "  - ▁leave\n",
            "  - ▁provide\n",
            "  - qui\n",
            "  - ▁power\n",
            "  - ▁movie\n",
            "  - ▁everyone\n",
            "  - ▁making\n",
            "  - ▁member\n",
            "  - ▁woman\n",
            "  - ▁somebody\n",
            "  - ▁wonder\n",
            "  - ▁short\n",
            "  - ▁health\n",
            "  - ▁police\n",
            "  - ▁bank\n",
            "  - ▁until\n",
            "  - ▁companies\n",
            "  - ▁everybody\n",
            "  - ▁knew\n",
            "  - ▁program\n",
            "  - ▁music\n",
            "  - ▁york\n",
            "  - ▁land\n",
            "  - ▁doctor\n",
            "  - ▁answer\n",
            "  - ▁building\n",
            "  - ▁employ\n",
            "  - ▁travel\n",
            "  - ▁major\n",
            "  - ▁seems\n",
            "  - ▁safe\n",
            "  - gue\n",
            "  - ▁college\n",
            "  - ▁along\n",
            "  - ▁clear\n",
            "  - ▁especially\n",
            "  - ▁umhu\n",
            "  - ▁result\n",
            "  - ▁type\n",
            "  - ▁court\n",
            "  - ▁black\n",
            "  - ▁hold\n",
            "  - ▁myself\n",
            "  - ▁education\n",
            "  - ▁social\n",
            "  - ▁enjoy\n",
            "  - ▁became\n",
            "  - ▁whether\n",
            "  - ▁morning\n",
            "  - ▁difficult\n",
            "  - ▁shi\n",
            "  - ▁felt\n",
            "  - ▁husband\n",
            "  - ▁white\n",
            "  - ▁taking\n",
            "  - ▁million\n",
            "  - ▁require\n",
            "  - ▁early\n",
            "  - ency\n",
            "  - ▁visit\n",
            "  - ▁level\n",
            "  - ▁brother\n",
            "  - ▁married\n",
            "  - ▁further\n",
            "  - ▁affect\n",
            "  - ▁serve\n",
            "  - ▁present\n",
            "  - ▁park\n",
            "  - ▁effect\n",
            "  - ▁wife\n",
            "  - ▁teacher\n",
            "  - ▁cannot\n",
            "  - ▁community\n",
            "  - ▁street\n",
            "  - ▁period\n",
            "  - ▁national\n",
            "  - ▁view\n",
            "  - ▁future\n",
            "  - ▁daughter\n",
            "  - ▁situation\n",
            "  - ▁grand\n",
            "  - ▁success\n",
            "  - ▁perform\n",
            "  - ▁concern\n",
            "  - ▁complete\n",
            "  - ▁example\n",
            "  - ized\n",
            "  - ▁thousand\n",
            "  - ▁increase\n",
            "  - ▁began\n",
            "  - ▁final\n",
            "  - ▁east\n",
            "  - ▁sense\n",
            "  - ▁charge\n",
            "  - ▁record\n",
            "  - ▁born\n",
            "  - ▁instead\n",
            "  - ▁receive\n",
            "  - ▁women\n",
            "  - ▁across\n",
            "  - ▁information\n",
            "  - ▁although\n",
            "  - ▁process\n",
            "  - ▁condition\n",
            "  - ▁security\n",
            "  - ▁treat\n",
            "  - ▁funny\n",
            "  - ▁custom\n",
            "  - ▁cold\n",
            "  - ▁behind\n",
            "  - ified\n",
            "  - ▁ground\n",
            "  - cycl\n",
            "  - ▁depend\n",
            "  - ▁themselves\n",
            "  - ▁design\n",
            "  - ▁slow\n",
            "  - ▁third\n",
            "  - ▁smoke\n",
            "  - ▁wrong\n",
            "  - ▁project\n",
            "  - ▁space\n",
            "  - ▁drink\n",
            "  - ▁particular\n",
            "  - ▁listen\n",
            "  - ▁thirty\n",
            "  - ▁special\n",
            "  - ability\n",
            "  - ▁improve\n",
            "  - ▁attack\n",
            "  - ▁happy\n",
            "  - ▁strange\n",
            "  - ▁english\n",
            "  - ▁value\n",
            "  - ▁brought\n",
            "  - ▁private\n",
            "  - ▁account\n",
            "  - ▁china\n",
            "  - ▁spoke\n",
            "  - ▁foreign\n",
            "  - ▁possible\n",
            "  - ▁author\n",
            "  - ▁circ\n",
            "  - ▁voice\n",
            "  - ▁figure\n",
            "  - ▁control\n",
            "  - ▁according\n",
            "  - ▁green\n",
            "  - ▁university\n",
            "  - ▁language\n",
            "  - ▁please\n",
            "  - ▁animal\n",
            "  - ▁church\n",
            "  - ▁society\n",
            "  - ▁dream\n",
            "  - ’\n",
            "  - q\n",
            "  - ':'\n",
            "  - ;\n",
            "  - —\n",
            "  - ‘\n",
            "  - ”\n",
            "  - _\n",
            "  - '3'\n",
            "  - '8'\n",
            "  - <\n",
            "  - '>'\n",
            "  - '1'\n",
            "  - –\n",
            "  - '7'\n",
            "  - (\n",
            "  - )\n",
            "  - '0'\n",
            "  - '2'\n",
            "  - '4'\n",
            "  - +\n",
            "  - '&'\n",
            "  - '5'\n",
            "  - '9'\n",
            "  - ü\n",
            "  - é\n",
            "  - /\n",
            "  - á\n",
            "  - ó\n",
            "  - ō\n",
            "  - ú\n",
            "  - ']'\n",
            "  - â\n",
            "  - í\n",
            "  - ã\n",
            "  - ð\n",
            "  - ā\n",
            "  - ć\n",
            "  - č\n",
            "  - š\n",
            "  - è\n",
            "  - ë\n",
            "  - '`'\n",
            "  - ç\n",
            "  - ū\n",
            "  - ạ\n",
            "  - ø\n",
            "  - '='\n",
            "  - à\n",
            "  - ł\n",
            "  - α\n",
            "  - ô\n",
            "  - к\n",
            "  - '}'\n",
            "  - å\n",
            "  - ă\n",
            "  - и\n",
            "  - ī\n",
            "  - π\n",
            "  - œ\n",
            "  - \\\n",
            "  - '['\n",
            "  - ñ\n",
            "  - ß\n",
            "  - ö\n",
            "  - ä\n",
            "  - '6'\n",
            "  - з\n",
            "  - н\n",
            "  - û\n",
            "  - '%'\n",
            "  - '{'\n",
            "  - ¡\n",
            "  - æ\n",
            "  - ê\n",
            "  - þ\n",
            "  - ę\n",
            "  - ě\n",
            "  - ğ\n",
            "  - ń\n",
            "  - ő\n",
            "  - ř\n",
            "  - ž\n",
            "  - ʻ\n",
            "  - в\n",
            "  - е\n",
            "  - й\n",
            "  - л\n",
            "  - ь\n",
            "  - χ\n",
            "  - “\n",
            "optim:\n",
            "  name: novograd\n",
            "  lr: 0.05\n",
            "  betas:\n",
            "  - 0.8\n",
            "  - 0.25\n",
            "  weight_decay: 0.001\n",
            "  sched:\n",
            "    name: CosineAnnealing\n",
            "    warmup_steps: 1000\n",
            "    warmup_ratio: null\n",
            "    min_lr: 1.0e-09\n",
            "    last_epoch: -1\n",
            "target: nemo.collections.asr.models.ctc_bpe_models.EncDecCTCModelBPE\n",
            "nemo_version: 2.3.0rc3\n",
            "decoding:\n",
            "  strategy: greedy_batch\n",
            "  preserve_alignments: null\n",
            "  compute_timestamps: null\n",
            "  word_seperator: ' '\n",
            "  segment_seperators:\n",
            "  - .\n",
            "  - '!'\n",
            "  - '?'\n",
            "  segment_gap_threshold: null\n",
            "  ctc_timestamp_type: all\n",
            "  batch_dim_index: 0\n",
            "  greedy:\n",
            "    preserve_alignments: false\n",
            "    compute_timestamps: false\n",
            "    preserve_frame_confidence: false\n",
            "    confidence_method_cfg:\n",
            "      name: entropy\n",
            "      entropy_type: tsallis\n",
            "      alpha: 0.33\n",
            "      entropy_norm: exp\n",
            "      temperature: DEPRECATED\n",
            "  beam:\n",
            "    beam_size: 4\n",
            "    search_type: default\n",
            "    preserve_alignments: false\n",
            "    compute_timestamps: false\n",
            "    return_best_hypothesis: true\n",
            "    beam_alpha: 1.0\n",
            "    beam_beta: 0.0\n",
            "    kenlm_path: null\n",
            "    flashlight_cfg:\n",
            "      lexicon_path: null\n",
            "      boost_path: null\n",
            "      beam_size_token: 16\n",
            "      beam_threshold: 20.0\n",
            "      unk_weight: -.inf\n",
            "      sil_weight: 0.0\n",
            "    pyctcdecode_cfg:\n",
            "      beam_prune_logp: -10.0\n",
            "      token_min_logp: -5.0\n",
            "      prune_history: false\n",
            "      hotwords: null\n",
            "      hotword_weight: 10.0\n",
            "  wfst:\n",
            "    beam_size: 4\n",
            "    search_type: riva\n",
            "    return_best_hypothesis: true\n",
            "    preserve_alignments: false\n",
            "    compute_timestamps: false\n",
            "    decoding_mode: nbest\n",
            "    open_vocabulary_decoding: false\n",
            "    beam_width: 10.0\n",
            "    lm_weight: 1.0\n",
            "    device: cuda\n",
            "    arpa_lm_path: null\n",
            "    wfst_lm_path: null\n",
            "    riva_decoding_cfg: {}\n",
            "    k2_decoding_cfg:\n",
            "      search_beam: 20.0\n",
            "      output_beam: 10.0\n",
            "      min_active_states: 30\n",
            "      max_active_states: 10000\n",
            "  confidence_cfg:\n",
            "    preserve_frame_confidence: false\n",
            "    preserve_token_confidence: false\n",
            "    preserve_word_confidence: false\n",
            "    exclude_blank: true\n",
            "    aggregation: min\n",
            "    tdt_include_duration: false\n",
            "    method_cfg:\n",
            "      name: entropy\n",
            "      entropy_type: tsallis\n",
            "      alpha: 0.33\n",
            "      entropy_norm: exp\n",
            "      temperature: DEPRECATED\n",
            "  temperature: 1.0\n",
            "\n"
          ]
        }
      ],
      "source": [
        "cfg = copy.deepcopy(citrinet.cfg)\n",
        "print(OmegaConf.to_yaml(cfg))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W_V3e3W7vqOb"
      },
      "source": [
        "## Analysing the contents of the Model config\n",
        "----------\n",
        "\n",
        "Above we see a configuration for the Citrinet model. As discussed in the beginning, NeMo models contain the entire definition of the neural network(s) as well as most of the surrounding infrastructure to support that model within themselves. Here, we see a perfect example of this behaviour.\n",
        "\n",
        "Citrinet contains within its config -\n",
        "\n",
        "- `preprocessor` - MelSpectrogram preprocessing layer\n",
        "- `encoder` - The acoustic encoder model.\n",
        "- `decoder` - The CTC decoder layer.\n",
        "- `optim` (and potentially `sched`) - Optimizer configuration. Can optionally include Scheduler information.\n",
        "- `spec_augment` - Spectrogram Augmentation support.\n",
        "- `train_ds`, `validation_ds` and `test_ds` - Dataset and data loader construction information."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sIwhdXkwxn6R"
      },
      "source": [
        "## Modifying the contents of the Model config\n",
        "----------\n",
        "\n",
        "Say we want to experiment with a different preprocessor (we want MelSpectrogram, but with different configuration than was provided in the original configuration). Or say we want to add a scheduler to this model during training.\n",
        "\n",
        "OmegaConf makes this a very simple task for us!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "WlSZ8EA4yGKo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ef83a78f-6d4c-46b0-eca6-4630144f28e6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Old Config: \n",
            "name: novograd\n",
            "lr: 0.05\n",
            "betas:\n",
            "- 0.8\n",
            "- 0.25\n",
            "weight_decay: 0.001\n",
            "sched:\n",
            "  name: CosineAnnealing\n",
            "  warmup_steps: 1000\n",
            "  warmup_ratio: null\n",
            "  min_lr: 1.0e-09\n",
            "  last_epoch: -1\n",
            "\n",
            "New Config: \n",
            "name: novograd\n",
            "lr: 0.05\n",
            "betas:\n",
            "- 0.8\n",
            "- 0.25\n",
            "weight_decay: 0.001\n",
            "sched:\n",
            "  name: CosineAnnealing\n",
            "  warmup_steps: 1000\n",
            "  min_lr: 1.0e-06\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# OmegaConf won't allow you to add new config items, so we temporarily disable this safeguard.\n",
        "OmegaConf.set_struct(cfg, False)\n",
        "\n",
        "# Let's see the old optim config\n",
        "print(\"Old Config: \")\n",
        "print(OmegaConf.to_yaml(cfg.optim))\n",
        "\n",
        "sched = {'name': 'CosineAnnealing', 'warmup_steps': 1000, 'min_lr': 1e-6}\n",
        "sched = OmegaConf.create(sched)  # Convert it into a DictConfig\n",
        "\n",
        "# Assign it to cfg.optim.sched namespace\n",
        "cfg.optim.sched = sched\n",
        "\n",
        "# Let's see the new optim config\n",
        "print(\"New Config: \")\n",
        "print(OmegaConf.to_yaml(cfg.optim))\n",
        "\n",
        "# Here, we restore the safeguards so no more additions can be made to the config\n",
        "OmegaConf.set_struct(cfg, True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-nMDN66502kn"
      },
      "source": [
        "## Updating the model from config\n",
        "----------\n",
        "\n",
        "NeMo Models can be updated in a few ways, but we follow similar patterns within each collection so as to maintain consistency.\n",
        "\n",
        "Here, we will show the two most common ways to modify core components of the model - using the `from_config_dict` method, and updating a few special parts of the model.\n",
        "\n",
        "Remember, all NeMo models are PyTorch Lightning modules, which themselves are PyTorch modules, so we have a lot of flexibility here!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qrKzFYkZ20aa"
      },
      "source": [
        "### Update model using `from_config_dict`\n",
        "\n",
        "In certain config files, you will notice the following pattern :\n",
        "\n",
        "```yaml\n",
        "preprocessor:\n",
        "  _target_: nemo.collections.asr.modules.AudioToMelSpectrogramPreprocessor\n",
        "  normalize: per_feature\n",
        "  window_size: 0.02\n",
        "  sample_rate: 16000\n",
        "  window_stride: 0.01\n",
        "  window: hann\n",
        "  features: 64\n",
        "  n_fft: 512\n",
        "  frame_splicing: 1\n",
        "  dither: 1.0e-05\n",
        "  stft_conv: false\n",
        "```\n",
        "\n",
        "You might ask, why are we using `_target_`? Well, it is generally rare for the preprocessor, encoder, decoder and perhaps a few other details to be changed often from the command line when experimenting. In order to stabilize these settings, we enforce that our preprocessor will always be of type `AudioToMelSpectrogramPreprocessor` for this model by setting its `_target_` attribute in the config. In order to provide its parameters in the class constructor, we simply add them after `_target_`.\n",
        "\n",
        "---------\n",
        "Note, we can still change all of the parameters of this `AudioToMelSpectrogramPreprocessor` class from the command line using hydra, so we don't lose any flexibility once we decide what type of preprocessing class we want !\n",
        "\n",
        "This also gives us a flexible way to instantiate parts of the model from just the config object !"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "1Be08R4szkT3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6ba7fa20-d8ec-4bfd-c9b0-cd92f5b95d35"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[NeMo I 2025-04-08 06:21:36 nemo_logging:393] PADDING: 16\n",
            "AudioToMelSpectrogramPreprocessor(\n",
            "  (featurizer): FilterbankFeatures()\n",
            ")\n"
          ]
        }
      ],
      "source": [
        "new_preprocessor_config = copy.deepcopy(cfg.preprocessor)\n",
        "new_preprocessor = citrinet.from_config_dict(new_preprocessor_config)\n",
        "print(new_preprocessor)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UzJQ7Y8H4S_U"
      },
      "source": [
        "So how do we actually update our model's internal preprocessor with something new? Well, since NeMo Model's are just pytorch Modules, we can just replace their attribute !"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "WdtnPKX84OJ-"
      },
      "outputs": [],
      "source": [
        "citrinet.preprocessor = new_preprocessor"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "OMz2KR-24xTO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b0e588b0-bd85-4b79-d2c9-48de1c082691"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "  | Name              | Type                              | Params | Mode \n",
              "--------------------------------------------------------------------------------\n",
              "0 | preprocessor      | AudioToMelSpectrogramPreprocessor | 0      | train\n",
              "1 | encoder           | ConvASREncoder                    | 36.3 M | train\n",
              "2 | decoder           | ConvASRDecoder                    | 657 K  | train\n",
              "3 | loss              | CTCLoss                           | 0      | train\n",
              "4 | spec_augmentation | SpectrogramAugmentation           | 0      | train\n",
              "5 | wer               | WER                               | 0      | train\n",
              "--------------------------------------------------------------------------------\n",
              "37.0 M    Trainable params\n",
              "0         Non-trainable params\n",
              "37.0 M    Total params\n",
              "147.977   Total estimated model params size (MB)\n",
              "943       Modules in train mode\n",
              "0         Modules in eval mode"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ],
      "source": [
        "citrinet.summarize()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gPb_BdPN40Ro"
      },
      "source": [
        "--------\n",
        "This might look like nothing changed - because we didn't actually modify the config for the preprocessor at all ! But as we showed above, we can easily modify the config for the preprocessor, instantiate it from config, and then just set it to the model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IV8WKJkD5E_Q"
      },
      "source": [
        "-------\n",
        "**NOTE**: Preprocessors don't generally have weights, so this was easy, but say we want to replace a part of the model which actually has trained parameters?\n",
        "\n",
        "Well, the above approach will still work, just remember the fact that the new module you inserted into `citrinet.encoder` or `citrinet.decoder` actually won't have pretrained weights. You can easily rectify that by loading the state dict for the module *before* you set it to the Model though!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YplQcgfG6S1U"
      },
      "source": [
        "### Preserving the new config\n",
        "\n",
        "So we went ahead and updated the preprocessor of the model. We however also need to perform a crucial step - **preserving the updated config**!\n",
        "\n",
        "Why do we want to do this? NeMo has many ways of saving and restoring its models, which we will discuss a bit later. All of them depend on having an updated config that defines the model in its entirety, so if we modify anything, we should also update the corresponding part of the config to safely save and restore models."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "dsxQHBV86R4a"
      },
      "outputs": [],
      "source": [
        "# Update the config copy\n",
        "cfg.preprocessor = new_preprocessor_config\n",
        "# Update the model config\n",
        "citrinet.cfg = cfg"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eXRRBnJk5tCv"
      },
      "source": [
        "## Update a few special components of the Model\n",
        "---------\n",
        "\n",
        "While the above approach is good for most major components of the model, NeMo has special utilities for a few components.\n",
        "\n",
        "They are -\n",
        "\n",
        " - `setup_training_data`\n",
        " - `setup_validation_data` and `setup_multi_validation_data`\n",
        " - `setup_test_data` and `setup_multi_test_data`\n",
        " - `setup_optimization`\n",
        "\n",
        "These special utilities are meant to help you easily setup training, validation, testing once you restore a model from a checkpoint.\n",
        "\n",
        "------\n",
        "One of the major tasks of all conversational AI models is fine-tuning onto new datasets - new languages, new corpus of text, new voices etc. It is often insufficient to have just a pre-trained model. So these setup methods are provided to enable users to adapt models *after* they have been already trained or provided to you.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B7Y7wt2x9goJ"
      },
      "source": [
        "You might remember having seen a few warning messages the moment you tried to instantiate the pre-trained model. Those warnings are in fact reminders to call the appropriate setup methods for the task you want to perform.\n",
        "\n",
        "Those warnings are simply displaying the old config that was used to train that model, and are a basic template that you can easily modify. You have the ability to modify the `train_ds`, `validation_ds` and `test_ds` sub-configs in their entirety in order to evaluate, fine-tune or train from scratch the model, or any further purpose as you require it.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1hXXdaup-QmG"
      },
      "source": [
        "Let's discuss how to add the scheduler to the model below (which initially had just an optimizer in its config)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "cveKWvMZ4zBo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dad58903-cdd6-488d-d193-63eba3c16b24"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "name: novograd\n",
            "lr: 0.05\n",
            "betas:\n",
            "- 0.8\n",
            "- 0.25\n",
            "weight_decay: 0.001\n",
            "sched:\n",
            "  name: CosineAnnealing\n",
            "  warmup_steps: 1000\n",
            "  min_lr: 1.0e-06\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Let's print out the current optimizer\n",
        "print(OmegaConf.to_yaml(citrinet.cfg.optim))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "XVguw3k0-f6b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e5e2522d-2725-4616-cdb0-d84e81e6dcdf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[NeMo W 2025-04-08 06:21:44 nemo_logging:405] Trainer wasn't specified in model constructor. Make sure that you really wanted it.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[NeMo I 2025-04-08 06:21:44 nemo_logging:393] Optimizer config = Novograd (\n",
            "    Parameter Group 0\n",
            "        amsgrad: False\n",
            "        betas: [0.8, 0.25]\n",
            "        eps: 1e-08\n",
            "        grad_averaging: False\n",
            "        lr: 0.05\n",
            "        weight_decay: 0.001\n",
            "    )\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[NeMo W 2025-04-08 06:21:44 nemo_logging:405] Neither `max_steps` nor `iters_per_batch` were provided to `optim.sched`, cannot compute effective `max_steps` !\n",
            "    Scheduler will not be instantiated !\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(Novograd (\n",
              " Parameter Group 0\n",
              "     amsgrad: False\n",
              "     betas: [0.8, 0.25]\n",
              "     eps: 1e-08\n",
              "     grad_averaging: False\n",
              "     lr: 0.05\n",
              "     weight_decay: 0.001\n",
              " ),\n",
              " None)"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ],
      "source": [
        "# Now let's update the config\n",
        "citrinet.setup_optimization(cfg.optim)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1JZBCQeW-21X"
      },
      "source": [
        "-------\n",
        "We see a warning -\n",
        "\n",
        "```\n",
        "Neither `max_steps` nor `iters_per_batch` were provided to `optim.sched`, cannot compute effective `max_steps` !\n",
        "    Scheduler will not be instantiated !\n",
        "```\n",
        "\n",
        "We don't have a train dataset setup, nor do we have max_steps in the config. Most NeMo schedulers cannot be instantiated without computing how many train steps actually exist!\n",
        "\n",
        "Here, we can temporarily allow the scheduler construction by explicitly passing a max_steps value to be 100"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "mqC89hfE-tqf"
      },
      "outputs": [],
      "source": [
        "OmegaConf.set_struct(cfg.optim.sched, False)\n",
        "\n",
        "cfg.optim.sched.max_steps = 100\n",
        "\n",
        "OmegaConf.set_struct(cfg.optim.sched, True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "r22IqOBK_q6l",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "241f2121-8773-48eb-983f-aff0dd544041"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[NeMo W 2025-04-08 06:21:47 nemo_logging:405] Trainer wasn't specified in model constructor. Make sure that you really wanted it.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[NeMo I 2025-04-08 06:21:47 nemo_logging:393] Optimizer config = Novograd (\n",
            "    Parameter Group 0\n",
            "        amsgrad: False\n",
            "        betas: [0.8, 0.25]\n",
            "        eps: 1e-08\n",
            "        grad_averaging: False\n",
            "        lr: 0.05\n",
            "        weight_decay: 0.001\n",
            "    )\n",
            "[NeMo I 2025-04-08 06:21:47 nemo_logging:393] Scheduler \"<nemo.core.optim.lr_scheduler.CosineAnnealing object at 0x79f044b88f90>\" \n",
            "    will be used during training (effective maximum steps = 100) - \n",
            "    Parameters : \n",
            "    (warmup_steps: 1000\n",
            "    min_lr: 1.0e-06\n",
            "    max_steps: 100\n",
            "    )\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(Novograd (\n",
              " Parameter Group 0\n",
              "     amsgrad: False\n",
              "     betas: [0.8, 0.25]\n",
              "     eps: 1e-08\n",
              "     grad_averaging: False\n",
              "     initial_lr: 0.05\n",
              "     lr: 4.995004995004995e-05\n",
              "     weight_decay: 0.001\n",
              " ),\n",
              " {'scheduler': <nemo.core.optim.lr_scheduler.CosineAnnealing at 0x79f044b88f90>,\n",
              "  'interval': 'step',\n",
              "  'frequency': 1,\n",
              "  'monitor': 'loss',\n",
              "  'reduce_on_plateau': False})"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ],
      "source": [
        "# Now let's update the config and try again\n",
        "citrinet.setup_optimization(cfg.optim)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U7Eezf_sAVS0"
      },
      "source": [
        "You might wonder why we didn't explicitly set `citrinet.cfg.optim = cfg.optim`.\n",
        "\n",
        "This is because the `setup_optimization()` method does it for you! You can still update the config manually."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "THqhXy_lQ7i8"
      },
      "source": [
        "### Optimizer & Scheduler Config\n",
        "\n",
        "Optimizers and schedulers are common components of models, and are essential to train the model from scratch.\n",
        "\n",
        "They are grouped together under a unified `optim` namespace, as schedulers often operate on a given optimizer.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6HY51nuoSJs5"
      },
      "source": [
        "### Let's breakdown the general `optim` structure\n",
        "```yaml\n",
        "optim:\n",
        "    name: novograd\n",
        "    lr: 0.01\n",
        "\n",
        "    # optimizer arguments\n",
        "    betas: [0.8, 0.25]\n",
        "    weight_decay: 0.001\n",
        "\n",
        "    # scheduler setup\n",
        "    sched:\n",
        "      name: CosineAnnealing\n",
        "\n",
        "      # Optional arguments\n",
        "      max_steps: -1 # computed at runtime or explicitly set here\n",
        "      monitor: val_loss\n",
        "      reduce_on_plateau: false\n",
        "\n",
        "      # scheduler config override\n",
        "      warmup_steps: 1000\n",
        "      warmup_ratio: null\n",
        "      min_lr: 1e-9\n",
        "```\n",
        "\n",
        "Essential Optimizer components -\n",
        "\n",
        " - `name`: String name of the optimizer. Generally a lower case of the class name.\n",
        " - `lr`: Learning rate is a required argument to all optimizers.\n",
        "\n",
        "Optional Optimizer components - after the above two arguments are provided, any additional arguments added under `optim` will be passed to the constructor of that optimizer as keyword arguments\n",
        "\n",
        " - `betas`: List of beta values to pass to the optimizer\n",
        " - `weight_decay`: Optional weight decay passed to the optimizer.\n",
        "\n",
        "Optional Scheduler components - `sched` is an optional setup of the scheduler for the given optimizer.\n",
        "\n",
        "If `sched` is provided, only one essential argument needs to be provided :\n",
        "\n",
        " - `name`: The name of the scheduler. Generally, it is the full class name.\n",
        "\n",
        "Optional Scheduler components -\n",
        "\n",
        " - `max_steps`: Max steps as an override from the user. If one provides `trainer.max_steps` inside the trainer configuration, that value is used instead. If neither value is set, the scheduler will attempt to compute the `effective max_steps` using the size of the train data loader. If that too fails, then the scheduler will not be created at all.\n",
        "\n",
        " - `monitor`: Used if you are using an adaptive scheduler such as ReduceLROnPlateau. Otherwise ignored. Defaults to `loss` - indicating train loss as monitor.\n",
        "\n",
        " - `reduce_on_plateau`: Required to be set to true if using an adaptive scheduler.\n",
        "\n",
        "Any additional arguments under `sched` will be supplied as keyword arguments to the constructor of the scheduler.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V3pQM2aj_6WX"
      },
      "source": [
        "## Difference between the data loader setup methods\n",
        "----------\n",
        "\n",
        "You might notice, we have multiple setup methods for validation and test data sets. We also don't have an equivalent `setup_multi_train_data`.\n",
        "\n",
        "In general, the `multi` methods refer to multiple data sets / data loaders.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g33nMx9WCJdj"
      },
      "source": [
        "### Where's `setup_multi_train_data`?\n",
        "With the above in mind, let's tackle why we don't have `setup_multi_train_data`.\n",
        "\n",
        "NeMo is concerned with multiple domains - `asr`, `nlp` and `tts`. The way datasets are setup and used in these domains is dramatically different. It is often unclear what it means to have multiple train datasets - do we concatenate them? Do we randomly sample (with same or different probability) from each of them?\n",
        "\n",
        "Therefore we leave such support for multiple datasets up to the model itself. For example, in ASR, you can concatenate multiple train manifest files by using commas when providing the `manifest_filepath` value!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BjI2Q5LECJib"
      },
      "source": [
        "### What are multi methods?\n",
        "\n",
        "In many cases, especially true for ASR and NLP, we may have multiple validation and test datasets. The most common example for this in ASR is `Librispeech`, which has `dev_clean`, `dev_other`, `test_clean`, `test_other`.\n",
        "\n",
        "NeMo standardizes how to handle multiple data loaders for validation and testing, so that all of our collections have a similar look and feel, as well as ease development of our models. During evaluation, these datasets are treated independently and prepended with resolved names so that logs are separate!\n",
        "\n",
        "The `multi` methods are therefore generalizations of the single validation and single test data setup methods, with some additional functionality. If you provide multiple datasets, you still have to write code for just one dataset and NeMo will automatically attach the appropriate names to your logs so you can differentiate between them!\n",
        "\n",
        "Furthermore, they also automatically preserve the config the user passes to them when updating the validation or test data loaders.\n",
        "\n",
        "**In general, it is preferred to call the `setup_multi_validation_data` and `setup_multi_test_data` methods, even if you are only using single datasets, simply for the automated management they provide.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZKURHn0jH_52"
      },
      "source": [
        "## Creating Model from constructor vs restoring a model\n",
        "---------\n",
        "\n",
        "You might notice, we discuss all of the above setup methods in the context of model after it is restored. However, NeMo scripts do not call them inside any of the example train scripts themselves.\n",
        "\n",
        "This is because these methods are automatically called by the constructor when the Model is created for the first time, but these methods are skipped during restoration (either from a PyTorch Lightning checkpoint using `load_from_checkpoint`, or via `restore_from` method inside NeMo Models).\n",
        "\n",
        "This is done as most datasets are stored on a user's local directory, and the path to these datasets is set in the config (either set by default, or set by Hydra overrides). On the other hand, the models are meant to be portable. On another user's system, the data might not be placed at exactly the same location, or even on the same drive as specified in the model's config!\n",
        "\n",
        "Therefore we allow the constructor some brevity and automate such dataset setup, whereas restoration warns that data loaders were not set up and provides the user with ways to set up their own datasets.\n",
        "\n",
        "------\n",
        "\n",
        "Why are optimizers not restored automatically? Well, optimizers themselves don't face an issue, but as we saw before, schedulers depend on the number of train steps in order to calculate their schedule.\n",
        "\n",
        "However, if you don't wish to modify the optimizer and scheduler, and prefer to leave them to their default values, that's perfectly alright. The `setup_optimization()` method is automatically called by PyTorch Lightning for you when you begin training your model!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g91FE8mlMcnh"
      },
      "source": [
        "## Saving and restoring models\n",
        "----------\n",
        "\n",
        "NeMo provides a few ways to save and restore models. If you utilize the Experiment Manager that is part of all NeMo train scripts, PyTorch Lightning will automatically save checkpoints for you in the experiment directory.\n",
        "\n",
        "We can also use packaged files using the specialized `save_to` and `restore_from` methods."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NzMxga7QNYn8"
      },
      "source": [
        "### Saving and Restoring from PTL Checkpoints\n",
        "----------\n",
        "\n",
        "The PyTorch Lightning Trainer object will periodically save checkpoints when the experiment manager is being used during training.\n",
        "\n",
        "PyTorch Lightning checkpoints can then be loaded and evaluated / fine-tuned just as always using the class method `load_from_checkpoint`.\n",
        "\n",
        "For example, restore a Citrinet model from a checkpoint -\n",
        "\n",
        "```python\n",
        "citrinet = nemo_asr.models.EncDecCTCModelBPE.load_from_checkpoint(<path to checkpoint>)\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W4YzAG-KOBkZ"
      },
      "source": [
        "### Saving and Restoring from .nemo files\n",
        "----------\n",
        "\n",
        "There are a few models which might require external dependencies to be packaged with them in order to restore them properly.\n",
        "\n",
        "One such example is an ASR model with an external BPE tokenizer. It is preferred if the model includes all of the components required to restore it, but a binary file for a tokenizer cannot be serialized into a PyTorch Lightning checkpoint.\n",
        "\n",
        "In such cases, we can use the `save_to` and `restore_from` method to package the entire model + its components (here, the tokenizer file(s)) into a tarfile. This can then be easily imported by the user and used to restore the model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P6_vMSwXNJ74"
      },
      "outputs": [],
      "source": [
        "# Save the model\n",
        "citrinet.save_to('citrinet_512.nemo')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HrBhgaqyP4rU"
      },
      "outputs": [],
      "source": [
        "!ls -d -- *.nemo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Tyht1E0DQGb_"
      },
      "outputs": [],
      "source": [
        "# Restore the model\n",
        "temp_cn = nemo_asr.models.EncDecCTCModelBPE.restore_from('citrinet_512.nemo')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dqNpmYYJQS2H"
      },
      "outputs": [],
      "source": [
        "temp_cn.summarize()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A5e42EoiZYjf"
      },
      "outputs": [],
      "source": [
        "# Note that the preprocessor + optimizer config have been preserved after the changes we made !\n",
        "print(OmegaConf.to_yaml(temp_cn.cfg))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OI3RxwpcV-UF"
      },
      "source": [
        "Note, that .nemo file is a simple .tar.gz with checkpoint, configuration and, potentially, other artifacts such as tokenizer configs being used by the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jFBAGcaDWLiu"
      },
      "outputs": [],
      "source": [
        "!cp citrinet_512.nemo citrinet_512.tar.gz\n",
        "!tar -xvf citrinet_512.tar.gz"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mkau4Q9jZo1l"
      },
      "source": [
        "### Extracting PyTorch checkpoints from NeMo tarfiles (Model level)\n",
        "-----------\n",
        "\n",
        "While the .nemo tarfile is an excellent way to have a portable model, sometimes it is necessary for researchers to have access to the basic PyTorch save format. NeMo aims to be entirely compatible with PyTorch, and therefore offers a simple method to extract just the PyTorch checkpoint from the .nemo tarfile."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qccPANeycCoq"
      },
      "outputs": [],
      "source": [
        "import torch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A4zswOKHar9q"
      },
      "outputs": [],
      "source": [
        "state_dict = temp_cn.extract_state_dict_from('citrinet_512.nemo', save_dir='./pt_ckpt/')\n",
        "!ls ./pt_ckpt/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ACB-0dfnbFG3"
      },
      "source": [
        "As we can see below, there is now a single basic PyTorch checkpoint available inside the `pt_ckpt` directory, which we can use to load the weights of the entire model as below"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4ZAF_A0uc5bB"
      },
      "outputs": [],
      "source": [
        "temp_cn.load_state_dict(torch.load('./pt_ckpt/model_weights.ckpt'))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hkq6EM99cS6y"
      },
      "source": [
        "### Extracting PyTorch checkpoints from NeMo tarfiles (Module level)\n",
        "----------\n",
        "\n",
        "While the above method is exceptional when extracting the checkpoint of the entire model, sometimes there may be a necessity to load and save the individual modules that comprise the Model.\n",
        "\n",
        "The same extraction method offers a flag to extract the individual model level checkpoints into their individual files, so that users have access to per-module level checkpoints."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LW6wve2zbT9D"
      },
      "outputs": [],
      "source": [
        "state_dict = temp_cn.extract_state_dict_from('citrinet_512.nemo', save_dir='./pt_module_ckpt/', split_by_module=True)\n",
        "!ls ./pt_module_ckpt/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DtV5vpb5d1ni"
      },
      "source": [
        "Now, we can load and assign the weights of the individual modules of the above Citrinet Model !"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rVHylSKFdywn"
      },
      "outputs": [],
      "source": [
        "temp_cn.preprocessor.load_state_dict(torch.load('./pt_module_ckpt/preprocessor.ckpt'))\n",
        "temp_cn.encoder.load_state_dict(torch.load('./pt_module_ckpt/encoder.ckpt'))\n",
        "temp_cn.decoder.load_state_dict(torch.load('./pt_module_ckpt/decoder.ckpt'))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "88vOGV7VYcuu"
      },
      "source": [
        "# NeMo with Hydra\n",
        "\n",
        "[Hydra](https://hydra.cc/docs/intro/) is used throughout NeMo as a way to enable rapid prototyping using predefined config files. Hydra and OmegaConf offer great compatibility with each other, and below we show a few general helpful tips to improve productivity with Hydra when using NeMo."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DfY6Ha3qYcxG"
      },
      "source": [
        "## Hydra Help\n",
        "--------\n",
        "\n",
        "Since our scripts are written with hydra in mind, you might notice that using `python <script.py> --help` returns you a config rather than the usual help format from argparse.\n",
        "\n",
        "Using `--help` you can see the default config attached to the script - every NeMo script has at least one default config file attached to it. This gives you a guide on how you can modify values for an experiment.\n",
        "\n",
        "Hydra also has a special `--hydra-help` flag, which will offer you more help with respect to hydra itself as it is set up in the script.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gEsZlnfaYc3X"
      },
      "source": [
        "## Changing config paths and files\n",
        "---------\n",
        "\n",
        "While all NeMo models come with at least 1 default config file, one might want to switch configs without changing code. This is easily achieved by the following commands :\n",
        "\n",
        "- `--config-path`: Path to the directory which contains the config files\n",
        "- `--config-name`: Name of the config file we wish to load.\n",
        "\n",
        "Note that these two arguments need to be at the very beginning of your execution statement, before you provide any command line overrides to your config file."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZyNHlArpYc9A"
      },
      "source": [
        "## Overriding config from the command line\n",
        "----------\n",
        "\n",
        "Hydra allows users to provide command line overrides to any part of the config. There are three cases to consider -\n",
        "\n",
        " - Override existing value in config\n",
        " - Add new value in config\n",
        " - Remove old value in config"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "96CKbvn6Yc7f"
      },
      "source": [
        "### Overriding existing values in config\n",
        "\n",
        "Let's take the case where we want to change the optimizer from `novograd` to `adam`. Let's also change the beta values to default adam values.\n",
        "\n",
        "Hydra overrides are based on the `.` syntax - each `.` representing a level in the config itself.\n",
        "\n",
        "```sh\n",
        "$ python <script>.py \\\n",
        "  --config-path=\"dir to config\" \\\n",
        "  --config-name=\"name of config\" \\\n",
        "  model.optim.name=\"adam\" \\\n",
        "  model.optim.betas=[0.9,0.999]\n",
        "```\n",
        "\n",
        "It is to be noted, if lists are passed, there cannot be any spaces between items.\n",
        "\n",
        "------\n",
        "\n",
        "We can also support multi validation datasets with the above list syntax, but it depends on the model level support.\n",
        "\n",
        "For ASR collection, the following syntax is widely supported in ASR, ASR-BPE and classification models. Let's take an example of a model being trained on LibriSpeech -\n",
        "\n",
        "```sh\n",
        "$ python <script>.py \\\n",
        "  --config-path=\"dir to config\" \\\n",
        "  --config-name=\"name of config\" \\\n",
        "  model.validation_ds.manifest_filepath=[\"path to dev clean\",\"path to dev other\"] \\\n",
        "  model.test_ds.manifest_filepath=[\"path to test clean\",\"path to test other\"]\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wj7oMkepYc17"
      },
      "source": [
        "### Add new values in config\n",
        "----------\n",
        "\n",
        "Hydra allows us to inject additional parameters inside the config using the `+` syntax.\n",
        "\n",
        "Let's take an example of adding `amsgrad` fix for the `novograd` optimizer above.\n",
        "\n",
        "```sh\n",
        "$ python <script>.py \\\n",
        "  --config-path=\"dir to config\" \\\n",
        "  --config-name=\"name of config\" \\\n",
        "  +model.optim.amsgrad=true\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p23327hsYc0Z"
      },
      "source": [
        "### Remove old value in config\n",
        "---------\n",
        "\n",
        "Hydra allows us to remove parameters inside the config using the `~` syntax.\n",
        "\n",
        "Let's take an example of removing `weight_decay` inside the Novograd optimizer\n",
        "\n",
        "```sh\n",
        "$ python <script>.py \\\n",
        "  --config-path=\"dir to config\" \\\n",
        "  --config-name=\"name of config\" \\\n",
        "  ~model.optim.weight_decay\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8VSWIbzjYzDi"
      },
      "source": [
        "## Setting a value to `None` from the command line\n",
        "\n",
        "We may sometimes choose to disable a feature by setting the value to `None`.\n",
        "\n",
        "We can accomplish this by using the keyword `null` inside the command line.\n",
        "\n",
        "Let's take an example of disabling the validation data loader inside an ASR model's config -\n",
        "\n",
        "\n",
        "```sh\n",
        "$ python <script>.py \\\n",
        "  --config-path=\"dir to config\" \\\n",
        "  --config-name=\"name of config\" \\\n",
        "  model.test_ds.manifest_filepath=null\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ah8rgrvvsw5R"
      },
      "source": [
        "# NeMo Examples\n",
        "\n",
        "NeMo supports various pre-built models for ASR, NLP and TTS tasks. One example we see in this notebook is the ASR model for Speech to Text - by using the Citrinet model.\n",
        "\n",
        "The NeMo repository has a dedicated `examples` directory with scripts to train and evaluate models for various tasks - ranging from ASR speech to text, NLP question answering and TTS text to speech using models such as `FastPitch` and `HiFiGAN`.\n",
        "\n",
        "NeMo constantly adds new models and new tasks to these examples, such that these examples serve as the basis to train and evaluate models from scratch with the provided config files.\n",
        "\n",
        "NeMo Examples directory can be found here - https://github.com/NVIDIA/NeMo/tree/main/examples"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "999KAomdtWlu"
      },
      "source": [
        "## Structure of NeMo Examples\n",
        "-------\n",
        "\n",
        "The NeMo Examples directory is structured by domain, as well as sub-task. Similar to how we partition the collections supported by NeMo, the examples themselves are separated initially by domain, and then by sub-tasks of that domain.\n",
        "\n",
        "All these example scripts are bound to at least one default config file. These config files contain all of the information of the model, as well as the PyTorch Lightning Trainer configuration and Experiment Manager configuration.\n",
        "\n",
        "In general, once the model is trained and saved to a PyTorch Lightning checkpoint, or to a .nemo tarfile, it will no longer contain the training configuration - no configuration information for the Trainer or Experiment Manager.\n",
        "\n",
        "**These config files have good defaults pre-set to run an experiment with NeMo, so it is advised to base your own training configuration on these configs.**\n",
        "\n",
        "\n",
        "Let's take a deeper look at some of the examples inside each domain.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8Fk2grx0uSBQ"
      },
      "source": [
        "## ASR Examples\n",
        "-------\n",
        "\n",
        "NeMo supports multiple Speech Recognition models such as Jasper, QuartzNet, Citrinet, Conformer and more, all of which can be trained on various datasets. We also provide pretrained checkpoints for these models trained on standard datasets so that they can be used immediately. These scripts are made available in `speech_to_text_ctc.py`.\n",
        "\n",
        "ASR examples also supports sub-tasks such as speech classification - MatchboxNet trained on the Google Speech Commands Dataset is available in `speech_to_label.py`. Voice Activity Detection is also supported with the same script, by simply changing the config file passed to the script!\n",
        "\n",
        "NeMo also supports training Speech Recognition models with Byte Pair/Word Piece encoding of the corpus, via the `speech_to_text_ctc_bpe.py` example. Since these models are still under development, their configs fall under the `experimental/configs` directory.\n",
        "\n",
        "Finally, in order to simply perform inference on some dataset using these models, prefer to use the `speech_to_text_eval.py` example, which provides a look at how to compute WER over a dataset provided by the user."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HhtzYATsuSJV"
      },
      "source": [
        "## NLP Examples\n",
        "---------\n",
        "\n",
        "NeMo supports a wide variety of tasks in NLP - from text classification and language modelling all the way to glue benchmarking!\n",
        "\n",
        "All NLP models require text tokenization as data preprocessing steps. The list of tokenizers can be found in nemo.collections.common.tokenizers, and include WordPiece tokenizer, SentencePiece tokenizer or simple tokenizers like Word tokenizer.\n",
        "\n",
        "A non-exhaustive list of tasks that NeMo currently supports in NLP is -\n",
        "\n",
        " - Language Modelling - Assigns a probability distribution over a sequence of words. Can be either generative e.g. vanilla left-right-transformer or BERT with a masked language model loss.\n",
        " - Text Classification - Classifies an entire text based on its content into predefined categories, e.g. news, finance, science etc. These models are BERT-based and can be used for applications such as sentiment analysis, relationship extraction\n",
        " - Token Classification - Classifies each input token separately. Models are based on BERT. Applications include named entity recognition, punctuation and capitalization, etc.\n",
        " - Intent Slot Classification - used for joint recognition of Intents and Slots (Entities) for building conversational assistants.\n",
        " - Question Answering - Currently only SQuAD is supported. This takes in a question and a passage as input and predicts a span in the passage, from which the answer is extracted.\n",
        " - Glue Benchmarks - A benchmark of nine sentence- or sentence-pair language understanding tasks\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F2m4BT2AuSM_"
      },
      "source": [
        "## TTS Examples\n",
        "---------\n",
        "\n",
        "NeMo supports Text To Speech (TTS, aka Speech Synthesis) via a two step inference procedure. First, a model is used to generate a mel spectrogram from text. Second, a model is used to generate audio from a mel spectrogram.\n",
        "\n",
        "Supported Models:\n",
        "\n",
        "Mel Spectrogram Generators:\n",
        "*   Tacotron2\n",
        "*   FastPitch\n",
        "*   Talknet\n",
        "*   And more...\n",
        "\n",
        "Audio Generators (Vocoders):\n",
        "*   WaveGlow\n",
        "*   HiFiGAN\n",
        "*   And more..."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XKJPRgUns2On"
      },
      "source": [
        "# NeMo Tutorials\n",
        "\n",
        "Alongside the example scripts provided above, NeMo provides in depth tutorials for usage of these models for each of the above domains inside the `tutorials` directory found in the NeMo repository.\n",
        "\n",
        "Tutorials are meant to be more in-depth explanation of the workflow in the discussed task - usually involving a small amount of data to train a small model on a task, along with some explanation of the task itself.\n",
        "\n",
        "While the tutorials are a great example of the simplicity of NeMo, please note for the best performance when training on real datasets, we advice the use of the example scripts instead of the tutorial notebooks.\n",
        "\n",
        "NeMo Tutorials directory can be found here - https://github.com/NVIDIA/NeMo/tree/main/tutorials"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "00_NeMo_Primer.ipynb",
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}